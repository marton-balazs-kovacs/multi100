---
title: "Investigating the analytical robustness of the social and behavioural sciences"
format: docx
editor: source
editor_options: 
  chunk_output_type: console
bibliography: references.bib
csl: nature.csl
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
# Load packages
library(tidyverse)
library(ggrain)
library(maps)
library(mapdata)
library(countrycode)
library(raster)
library(gt)
library(grid)
library(patchwork)

# Load custom functions
script_files <- list.files(here::here("R"), pattern = "\\.R$", full.names = TRUE)
walk(script_files, source)

# Read processed data files
processed <- readr::read_csv(here::here("data/processed/multi100_processed_data.csv"))

peer_eval <- readr::read_csv(here::here("data/processed/multi100_peer-eval_processed_data.csv"))

peer_eval_review <- readr::read_csv(here::here("data/processed/multi100_peer-eval-review_processed_data.csv"))

## Transform datafiles for analysis 
# Add number of evaluations per analysis
peer_eval <-
  peer_eval |>
  dplyr::group_by(paper_id, analyst_id) |>
  dplyr::mutate(n_peer_evals = dplyr::n()) |>
  dplyr::ungroup()

all_people <- readr::read_csv(here::here("data/processed/multi100_all-people_processed_data.csv"))
```

```{r checks, include=FALSE}
# Check if any person is associated with multiple analyst IDs
processed |>
  dplyr::distinct(first_name, last_name, analyst_id) |>
  dplyr::group_by(first_name, last_name) |>
  dplyr::mutate(n_analyst_id = dplyr::n()) |>
  dplyr::filter(n_analyst_id > 1)

# Check if the analyst_id's are always unique to one person
processed |>
  dplyr::distinct(analyst_id, first_name, last_name) |>
  dplyr::group_by(analyst_id) |>
  dplyr::mutate(n_names = dplyr::n_distinct(paste(first_name, last_name))) |>
  dplyr::filter(n_names > 1)
```

# Introduction

Over the past decade, social and behavioural scientists have been striving to enhance the robustness, objectivity, and replicability of their findings through systemic reforms in the conduct and communication of empirical research. Practices such as preregistration[@nosek_preregistration_2018], registered reports[@chambers_whats_2019], multisite replications[@coles_build_2022], analytical reproducibility checks[@brodeur_replication_2023, @brodeur_mass_2024], and automated result validation techniques[@nuijten__2020] have been investigated and recommended to produce robust and replicable findings. An important aspect of robustness has yet to be systematically charted across these sciences: the contingency of the results on researchers’ analytical choices.

In a typical research pipeline, the collected empirical data are analysed by a single analyst or team and the published report presents a conclusion based on one analytical path, occasionally accompanied by a few robustness tests. The peer review process aims to ensure that the analysis approach meets the statistical and field-specific standards. However, this procedure does not systematically ascertain whether justifiable alternative analytical choices could have led to different results.

Theories and empirical designs rarely constrain analysts to a single analytical path. Many degrees of freedom exist in how researchers operationalise their variables, process their data, construct their statistical model, select algorithms and software for model estimation, and define their inference criteria; whether they follow frequentist, Bayesian, or likelihoodist analytical approaches; use machine learning, or conduct computational modelling to answer the same research question[@silberzahn_crowdsourced_2015, @wagenmakers_one_2022]. This inherent freedom of the analyst constitutes the so-called _analytical variability_ contained within empirical projects, a key component in the robustness of statistical results. In practical terms, it is the manifested variation among the choices independent scientists consider justified. Fig. 1 lists some sources of analytical variability that can manifest themselves in analysts’ statistical results and the conclusions drawn from the results.

One way to explore analytical variability is to employ a _multiverse_ methodology[@patel_assessment_2015, @steegen_increasing_2016] in which the analyst conducts all combinations of analytic choices they are able to generate across a wide range of reasonable scenarios. Alternatively, in the _multi-analyst approach_, multiple analysts analyse the data following their best judgement. The latter approach requires more organisation, but it takes advantage of alternative expert perspectives without the combinatory expansion of the number of results. A multi-analyst approach also examines naturally occurring variation, empirically answering the counterfactual question of what might have happened if another investigator had considered the same research question using the same data.

Multi-analyst projects[@botvinik-nezer_variability_2020, @bastiaansen_time_2020, @hoogeveen_many-analysts_2023, @salganik_measuring_2020, @schweinsberg_same_2021, @silberzahn_many_2018, @starns_assessing_2019, @veronese_reproducibility_2021, @breznau_observing_2022, @huntingtonklein_influence_2021, @trubutschek_eegmanypipelines_2024, @menkveld_nonstandard_2024, @sarstedt_same_2024, @schilling_tractography_2021] have provided some evidence of the extent to which analysts’ individual choices influence the results and conclusions. From economics to neuroscience, these explorations demonstrated that the robustness of empirical findings can be compromised by researcher degrees of freedom[@gelman_garden_2013]. The estimates of previous multi-analyst studies suggest that the variability in effect-size estimates attributable to analytical heterogeneity can exceed the variability one would expect due to sampling error[@holzmeister_heterogeneity_2024].

Do we know how robust published findings are to analytical choices across the social and behavioural sciences? One could argue that multi-analyst projects so far have been purposefully conducted in research areas with little consensus on the best analytical approach or were motivated to demonstrate the potential effect of analytical choices and, therefore, may represent rare cases where alternative analyses produce important differences in results. For example, perhaps the datasets selected afforded greater researcher degrees of freedom than is typical, raising issues of the generalizability of the findings to scientific research more broadly. Differences between academic methodologies and fields also seem plausible - for example, the relatively simple experiments sometimes used in social psychology and behavioural economics may contain fewer analytic decisions than the complex longitudinal observational datasets used in macroeconomics and finance, and thus be more analytically robust in general[@menkveld_nonstandard_2024]. To the extent that this is the case, the findings from the existing multi-analyst projects could be biased towards worst-case scenarios and the traditional analytical practice and review system may not require fundamental adjustments. If, on the other hand, observed results are contingent on the analyst's choices across fields, methodologies, and types of datasets, then the scientific literature could be less robust than is often assumed. If so, the general practices of how we conduct, report, and review empirical analyses should be reformed to address this source of uncertainty.

```{r number of analysis who failed peer evaluation, include=FALSE}
n_failed_peer <-
  processed |>
  dplyr::filter(!peer_eval_pass | !incomplete_response_pass) |>
  nrow()
```

```{r excluding analyst who failed peer evaluation, include=FALSE}
# Excluding analyst who failed the peer evaluation from the rest of the analysis
processed <-
  processed |> 
  dplyr::filter(peer_eval_pass & incomplete_response_pass)
```

```{r number of analysis, include=FALSE}
n_analysis <-
  processed |> 
  nrow()
```

```{r number of analyst, include=FALSE}
final_n_analyst <-
  processed |> 
  dplyr::distinct(analyst_id) |> 
  nrow()
```

After conducting `r n_analysis` re-analyses with the involvement of `r final_n_analyst` independent re-analysts on a stratified random sample of 100 social and behavioural studies, we conducted strictly exploratory analyses in order to describe the patterns in the findings. Inspecting the results across different research characteristics and study designs gives rise to a number of hypotheses for future research on how to maximize transparency and address this often-neglected component of scientific uncertainty.

## Variability of the results

To explore the robustness of published claims, we selected a key claim from each of our 100 studies, in which the authors provided evidence for a (directional) effect. We presented each empirical claim to at least five analysts along with the original data and asked them to analyse the data to examine the claim, following their best judgement and report only their main result. The analysts were encouraged to analyse those studies where they saw the greatest relevance of their expertise. Therefore, in this study, analytical variability, as a key component of robustness, is defined as the variation among the analytical results when different analysts are provided with the same research questions and the same data.

```{r number of different test family and values, include=FALSE}
same_test_family <-
  processed |>
  dplyr::select(
    simplified_paper_id,
    analyst_id,
    reanalysis_type_of_statistic,
    reanalysis_statistic_report
  ) |>
  dplyr::filter(!is.na(reanalysis_statistic_report) &
                  !is.na(reanalysis_type_of_statistic)) |> 
  dplyr::mutate(
    reanalysis_statistic_report = round(reanalysis_statistic_report, 2),
    reanalysis_statistic = interaction(reanalysis_type_of_statistic, reanalysis_statistic_report)
  ) |>
  dplyr::group_by(simplified_paper_id, reanalysis_statistic) |>
  dplyr::summarise(
    pair_count = n(), .groups = 'drop'
  ) |>
  dplyr::mutate(
    is_unique = (pair_count == 1)
  ) |> 
  dplyr::group_by(simplified_paper_id, is_unique) |>
  dplyr::summarise(
    total = sum(pair_count), .groups = 'drop'
  ) |> 
   tidyr::complete(
    simplified_paper_id, is_unique,
    fill = list(total = 0)
  )

proportion_unique_analysis <-
  same_test_family |>
  dplyr::group_by(is_unique) |>
  dplyr::summarise(grand_total = sum(total), .groups = 'drop') |>
  dplyr::mutate(total_count = sum(grand_total),
                percentage = round(grand_total / total_count * 100))

proportion_unique_paper <-
  same_test_family |> 
  dplyr::group_by(simplified_paper_id) |>
  dplyr::summarise(
    has_non_unique = sum(total[!is_unique] > 0), .groups = 'drop'
  ) |> 
  dplyr::summarise(
    percentage_all_unique = round(mean(has_non_unique == 0) * 100, 0)
  )
```

First, we explored the degree to which the re-analysts produced the same statistics in the re-analysis of each study. We found that in `r pull(proportion_unique_paper, percentage_all_unique)`% of the studies, the corresponding analysts reported different statistics regarding statistical test families (such as _t_-test, F-test, χ²) and their values (after rounding them to two decimal places).

A challenge in any multi-analyst project is to find a common metric that allows the results of the different analyses to be compared. A practical solution is to transform the reported point estimates into a standard effect size measure. Although these transformations have limitations and their calculation relies on assumptions that may not hold in all considered analysis settings[@gelman_garden_2013, @holzmeister_heterogeneity_2024, @coretta_multidimensional_2023, @van_assen_end_2023], for the sake of comparability we decided to compute Cohen’s _d_ for each re-analysis, wherever it was feasible. (For an alternative approach, see Supplementary information.) Fig. 1) The methods, materials, analysis plan, peer evaluation, and data management strategy of the project were preregistered on the OSF repository of the project (Deviations from the registered plan are reported and explained in the ‘Deviations from preregistration’ supplementary document). In our preregistration, we defined that we consider two results qualitatively the same when their effect sizes are within the tolerance region of ± 0.05 Cohen’s _d_. However, below, we also report analyses with alternative tolerance regions. Our results revealed how far the new estimates were from the original ones (Fig. 2a) and how often the effect sizes of the re-analyses fell within this tolerance region (Fig. 2i).

```{r data processing for tolerance_region_main_plot, include=FALSE}
# Number of papers where the original effect size is missing
missing_original_n <-
  processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(is.na(original_cohens_d)) |> 
  dplyr::filter(`is.na(original_cohens_d)`) |> 
  dplyr::pull(n)

# Check if there are cases where there is missing original effect size on the paper level but not for every analysis
# Would mean that there is a mistake in the merging code
processed |> 
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |> 
  dplyr::group_by(paper_id) |> 
  dplyr::mutate(
    number_of_analysis = dplyr::n() 
  ) |> 
  dplyr::filter(is.na(original_cohens_d)) |>
  mutate(
    number_of_analysis_after_filtering = dplyr::n()
  ) |> 
  dplyr::ungroup() |> 
  dplyr::mutate(
    match = dplyr::if_else(number_of_analysis == number_of_analysis_after_filtering, TRUE, FALSE)
  ) |> 
  dplyr::filter(!match)

# Number of cases where the reanalysed effect size is missing
missing_reanalysis_n <-
  processed |> 
  dplyr::count(is.na(reanalysis_cohens_d)) |> 
  dplyr::filter(`is.na(reanalysis_cohens_d)`) |> 
  dplyr::pull(n)

# Number of cases where reanalyzed effect size is comparable to original effect size
processed |>
  dplyr::select(original_cohens_d, reanalysis_cohens_d) |> 
  dplyr::summarise(
    both_missing = sum(is.na(original_cohens_d) & is.na(reanalysis_cohens_d)),
    both_present = sum(!is.na(original_cohens_d) & !is.na(reanalysis_cohens_d)),
    one_missing = sum(is.na(original_cohens_d) != is.na(reanalysis_cohens_d))
  )
  
# Preparing the data for the plot
reanalysis_data <-
  processed |>
  dplyr::select(simplified_paper_id,
                analyst_id,
                reanalysis_cohens_d,
                original_cohens_d) |>
  dplyr::group_by(simplified_paper_id) |>
  dplyr::rename(effect_size = reanalysis_cohens_d) |>
  dplyr::mutate(effect_size_type = paste0("re-analysis_0", row_number()),) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    simplified_paper_id = as.factor(simplified_paper_id),
    # Put missing values at the end
    original_cohens_d = dplyr::if_else(is.na(original_cohens_d), -Inf, original_cohens_d),
    simplified_paper_id = forcats::fct_reorder(.f = simplified_paper_id,
                                               .x = original_cohens_d,
                                               .fun = function(x) median(x),
                                               .na_rm = FALSE)
  )

original_data <-
  processed |>
  dplyr::distinct(simplified_paper_id, original_cohens_d) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    simplified_paper_id = as.factor(simplified_paper_id),
    simplified_paper_id = forcats::fct_reorder(simplified_paper_id,
                                               original_cohens_d,
                                               .na_rm = FALSE)
  ) |> 
  dplyr::filter(!is.na(original_cohens_d))

excluded_es <-
  dplyr::filter(reanalysis_data, effect_size > 4.5 | effect_size < -2) |> 
  mutate(excluded = glue::glue("study {simplified_paper_id}: {effect_size}")) |> 
  summarise(excluded_list = glue::glue_collapse(excluded, sep = "; "))
```

```{r generating tolerance_region_main_plot, include=FALSE}
tolerance_region_main_plot <-
  reanalysis_data |> 
  dplyr::filter(effect_size <= 4.5 & effect_size >= -2) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    y = simplified_paper_id,
    x = effect_size,
    # color = effect_size_type
  ) +
  ggplot2::geom_point(
    shape = 16,
    color = viridis::viridis(5)[[3]],
    # size = 4
    ) +
  # ggplot2::scale_color_manual(values = color_vector) +
  ggplot2::geom_pointrange(
    data = original_data,
    ggplot2::aes(
      x = original_cohens_d,
      xmin = tolarence_region_lower,
      xmax = tolarence_region_upper,
      alpha = 0.8
      ),
    show.legend = FALSE,
    color = "black",
    shape = 15,
    # size = 1
    ) +
  labs(
    x = expression("Effect size in Cohen's " * italic("d")),
    y = "Study number"
  ) +
  ggplot2::guides(color = "none") +
  ggplot2::scale_x_continuous(breaks = seq(-2.0, 4.5, by = 0.5)) +
  ggplot2::theme(
    axis.ticks = ggplot2::element_blank(),
    legend.title = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(),
    plot.margin = ggplot2::margin(t = 10, r = 20, b = 10, l = 10, "pt"),
    panel.grid = ggplot2::element_line(color = "lightgray"),
    # panel.grid.major.x = ggplot2::element_blank(),
    panel.grid.minor.x = ggplot2::element_blank(),
    panel.background = ggplot2::element_blank(),
    axis.title = ggplot2::element_text(size = 15),
    axis.text.y = ggplot2::element_text(size = 10, color = "black"),
    # axis.text.x = ggplot2::element_text(angle = 90, size = 5)
    axis.text.x = ggplot2::element_text(size = 10, color = "black")
    # axis.text.y=element_text(margin = margin(1, unit = "cm"), vjust =1.5)
    )

ggplot2::ggsave(here::here("figures/tolerance_region_main_plot.jpg"), plot = tolerance_region_main_plot,
       width = 12, height = 13,
       dpi = 300)
```

```{r calculating analysis level tolerance regions, include=FALSE}
# On the individual analysis level
# +-0.05 threshold for tolerance region
analysis_level_tolerance_region <-
  processed |>
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |>
  calculate_tolerance_region(threshold = 0.05) |> 
  # Exclude missing re-analysis effect sizes
  dplyr::filter(is_within_region != "Missing") |>
  calculate_percentage(is_within_region)

# Calculate value for abstract
analysis_level_within_tolerance_region_5 <- dplyr::filter(analysis_level_tolerance_region, is_within_region == "Within tolerance region") |> dplyr::pull(percentage) |>  round(0)

# +-0.2 threshold for tolerance region
analysis_level_tolerance_region_20 <-
  processed |>
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |>
  calculate_tolerance_region(threshold = 0.2) |> 
  # Exclude missing re-analysis effect sizes
  dplyr::filter(is_within_region != "Missing") |> 
  calculate_percentage(is_within_region)

# Calculate value for abstract
analysis_level_within_tolerance_region_20 <- dplyr::filter(analysis_level_tolerance_region_20, is_within_region == "Within tolerance region") |> dplyr::pull(percentage) |> round(0)
```

```{r data processing for tolerance_region_all_plot, include=FALSE}
# On the paper level
# +-0.05 threshold for tolerance region
# Calculate the percentage of re-analysis effect sizes within the tolerance region by study
tolerance_region_all_data <-
  processed |> 
  summarize_tolerance_region(grouping_var = simplified_paper_id, drop_missing = T, threshold = 0.05) |> 
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(is_within_region == "Within tolerance region", percentage, NA), .desc = TRUE, .na_rm = TRUE)
  )
```

```{r calculating paper level inferential robustness based on tolerance regions, include = FALSE}
# Calculate the robustness of the re-analysis effect size on the paper level by the tolerance region
paper_level_inferential_robustness <-
  tolerance_region_all_data |> 
  dplyr::group_by(simplified_paper_id) |> 
  dplyr::summarise(
    robust = if_else(any(is_within_region == "Within tolerance region" & freq == 1), "Inferentially robust", "Inferentially not Robust")
    ) |>
  dplyr::ungroup() |> 
  calculate_percentage(robust)

# +-0.2 threshold for tolerance region
tolerance_region_all_20_data <-
  processed |> 
  summarize_tolerance_region(grouping_var = simplified_paper_id, drop_missing = T, threshold = 0.2) |> 
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(is_within_region == "Within tolerance region", percentage, NA), .desc = TRUE, .na_rm = TRUE)
  )

# Calculate the robustness of the re-analysis effect size on the paper level by the tolerance region
paper_level_inferential_robustness_20 <-
  tolerance_region_all_20_data |> 
  dplyr::group_by(simplified_paper_id) |> 
  dplyr::summarise(
    robust = if_else(any(is_within_region == "Within tolerance region" & freq == 1), "Inferentially robust", "Inferentially not Robust")
    ) |>
  dplyr::ungroup() |> 
  calculate_percentage(robust)
```

```{r generating tolerance_region_all_plot, include=FALSE}
tolerance_region_all_plot <- plot_tolarence_region(data = tolerance_region_all_data, grouping_var = simplified_paper_id, y_lab = "Study number", x_lab = "Percentage of reanalysis results") +
  ggplot2::theme(
    axis.title = ggplot2::element_text(size = 15),
    legend.text = ggplot2::element_text(size = 10),
    axis.text.x = ggplot2::element_text(size = 13, color = "black")
  )

ggplot2::ggsave(here::here("figures/tolerance_region_all_plot.jpg"), tolerance_region_all_plot, width = 8.27, height = 11.69, dpi = 300)
```

```{r data processing for tolerance_region_discipline_plot, include=FALSE}
processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(paper_discipline) |> 
  dplyr::arrange(n)

tolerance_region_discipline_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |>
  dplyr::mutate(paper_discipline = stringr::str_to_title(paper_discipline)) |> 
  # Exclude missing values
  summarize_tolerance_region(
    grouping_var = paper_discipline,
    drop_missing = TRUE,
    threshold = 0.05
    )
```

```{r generating tolerance_region_discipline_plot, include=FALSE}
tolerance_region_discipline_plot <-
  plot_percentage(
    data = tolerance_region_discipline_data,
    grouping_var = paper_discipline,
    categorization_var = is_within_region,
    y_lab = "Percentage of re-analysis results",
    x_lab = "",
    with_labels = TRUE,
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    axis.title.y = element_text(hjust = 1, size = 13),
    axis.text = element_text(size = 10, color = "black")
  )

ggplot2::ggsave(here::here("figures/tolerance_region_discipline_plot.jpg"), tolerance_region_discipline_plot, dpi = 300)
```

```{r data processing for estimate_range_discipline_plot, include=FALSE}
estimate_range_discipline_data <-
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = stringr::str_to_title(paper_discipline)) |> 
  calculate_estimate_range(grouping_var = paper_discipline)
```

```{r generating estimate_range_discipline_plot, include=FALSE}
estimate_range_discipline_plot <-
  plot_rain(data = estimate_range_discipline_data,
            grouping_var = paper_discipline,
            response_var = estimate_range,
            x_lab = "",
            y_lab = "Effect size estimate range in Cohen's d",
            trans = "log10",
            breaks = c(0.01, 0.1, 0.5, 1, 5, 10, 30, 60)) +
  ggplot2::theme(
    axis.title.y = element_text(hjust = 1, size = 13, color = "black"),
    axis.text = element_text(size = 10, color = "black")
  )

ggplot2::ggsave(here::here("figures/estimate_range_discipline_plot.jpg"), estimate_range_discipline_plot, dpi = 300)
```

```{r data processing for tolerance_region_studytype_plot, include=FALSE}
tolerance_region_studytype_data <- 
  processed |> 
  dplyr::mutate(experimental_or_observational = stringr::str_to_title(experimental_or_observational)) |> 
  summarize_tolerance_region(grouping_var = experimental_or_observational, drop_missing = TRUE, threshold = 0.05)
```

```{r generating tolerance_region_studytype_plot, include=FALSE}
tolerance_region_studytype_plot <-
  plot_percentage(
    data = tolerance_region_studytype_data,
    grouping_var = experimental_or_observational,
    categorization_var = is_within_region,
    y_lab = "Percentage of re-analysis results",
    x_lab = "",
    with_labels = TRUE,
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    axis.title.y = element_text(hjust = 1, size = 13),
    axis.text = element_text(size = 10, color = "black")
  )

ggplot2::ggsave(here::here("figures/tolerance_region_studytype_plot.jpg"), tolerance_region_studytype_plot, dpi = 300)
```

```{r data processing for estimate_range_studytype_plot, include=FALSE}
estimate_range_studytype_data <-
  processed |> 
  dplyr::mutate(experimental_or_observational = stringr::str_to_title(experimental_or_observational)) |> 
  calculate_estimate_range(grouping_var = experimental_or_observational)
```

```{r generating estimate_range_studytype_plot, include=FALSE}
estimate_range_studytype_plot <- plot_rain(
  data = estimate_range_studytype_data,
  grouping_var = experimental_or_observational,
  response_var = estimate_range,
  x_lab = "",
  y_lab = "Effect size estimate range in Cohen's d",
  trans = "log10",
  breaks = c(0.01, 0.1, 0.5, 1, 5, 10, 30, 60)
) +
  # ggplot2::coord_flip() +
  ggplot2::theme(
    axis.title = element_text(size = 13),
    axis.text = element_text(size = 10, color = "black")
  )

ggplot2::ggsave(here::here("figures/estimate_range_studytype_plot.jpg"), estimate_range_studytype_plot, dpi = 300)
```

```{r data processing for tolerance_region_expertise_plot, include=FALSE}
processed |> 
  dplyr::distinct(expertise_self_rating)

tolerance_region_expertise_data <-
  processed |>
  dplyr::mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  ) |>
  summarize_tolerance_region(grouping_var = expertise_self_rating,
                             drop_missing = TRUE,
                             threshold = 0.05)
```

```{r generating tolerance_region_expertise_plot, include=FALSE}
tolerance_region_expertise_plot <-
  plot_height(
    data = tolerance_region_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = is_within_region,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    rev_limits = FALSE,
    with_sum = FALSE
  ) +
  ggplot2::theme(
    axis.title = element_text(size = 13)
  )

ggplot2::ggsave(here::here("figures/tolerance_region_expertise_plot.jpg"), tolerance_region_expertise_plot, dpi = 300,
                # width = 8, height = 6
                )
```

```{r data processing for tolerance_region_familiarity_plot, include=FALSE}
tolerance_region_familiarity_data <-
  summarize_tolerance_region(
    data = processed,
    grouping_var = familiar_with_paper,
    drop_missing = TRUE,
    threshold = 0.05
  )
```

```{r generating tolerance_region_familiarity_plot, include=FALSE}
tolerance_region_familiarity_plot <-
  plot_percentage(
    data = tolerance_region_familiarity_data,
    grouping_var = familiar_with_paper,
    categorization_var = is_within_region,
    y_lab = "Percentage of re-analysis\nresults",
    x_lab = "Familiar with the paper",
    with_labels = TRUE
  ) +
  ggplot2::theme(
    plot.margin = margin(
    t = 20,
    r = 10,
    b = 10,
    l = 10,
    unit = "pt"
  ),
  axis.title = element_text(size = 13),
  axis.text = element_text(size = 10)
  )

ggplot2::ggsave(here::here("figures/tolerance_region_familiarity_plot.jpg"), tolerance_region_familiarity_plot, dpi = 300)
```

```{r data processing for tolerance_region_samplesize_plot, include=FALSE}
tolerance_region_samplesize_data <-
  processed |>
  dplyr::select(
    paper_id,
    analyst_id,
    reanalysis_model_sample_size,
    original_cohens_d,
    reanalysis_cohens_d
  ) |>
  calculate_tolerance_region(threshold = 0.05) |> 
  dplyr::filter(
    !is.na(reanalysis_model_sample_size),
    is_within_region != "Missing") |> 
  dplyr::mutate(
    is_within_region = dplyr::case_when(
      is_within_region == "Within tolerance region" ~ "Yes",
      is_within_region == "Outside of tolerance region" ~ "No",
    ),
    is_within_region = factor(is_within_region, levels = c("Yes", "No"))
  )
```

```{r generating tolerance_region_samplesize_plot, include=FALSE}
tolerance_region_samplesize_plot <-
  plot_rain(
    data = tolerance_region_samplesize_data,
    grouping_var = is_within_region,
    response_var = reanalysis_model_sample_size,
    y_lab = "Sample size",
    x_lab = "Is the re-analysis effect size\nwithin the tolerance region?",
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000)
  ) +
  ggplot2::coord_flip() +
  ggplot2::theme(
    axis.title.x = element_text(size = 13, color = "black"),
    axis.title.y = element_text(size = 13, color = "black")
  )

ggplot2::ggsave(here::here("figures/tolerance_region_samplesize_plot.jpg"), tolerance_region_samplesize_plot, dpi = 300)
```

```{r generating estimate_panel, echo=FALSE}
#| fig-width: 30
#| fig-height: 14
#| out-width: 100%
#| fig-dpi: 300

# Define the remaining plots section
small_plot_section <-
  (tolerance_region_discipline_plot | estimate_range_discipline_plot) /
  (tolerance_region_studytype_plot | estimate_range_studytype_plot) /
  (tolerance_region_expertise_plot | tolerance_region_familiarity_plot | tolerance_region_samplesize_plot) +
  plot_layout(ncol = 1, nrow = 3)

# Combine the main section and the remaining plots section
estimate_panel <-
  (tolerance_region_main_plot | small_plot_section | tolerance_region_all_plot) + plot_layout(widths = c(1, 2, 1)) + plot_annotation(tag_levels = 'a')

estimate_panel

ggsave(here::here("figures/estimate_panel.png"), estimate_panel, width = 30, height = 14, dpi = 300)
```

We found that in `r dplyr::filter(paper_level_inferential_robustness, robust == "Inferentially robust") |> dplyr::pull(percentage)`% (`r dplyr::filter(paper_level_inferential_robustness, robust == "Inferentially robust") |> dplyr::pull(n)` out of `r dplyr::filter(paper_level_inferential_robustness, robust == "Inferentially not Robust") |> dplyr::pull(N)`) of the studies for which we could obtain the original effect size all re-analysis effect sizes were inside the tolerance region (+/- 0.05 Cohen’s _d_) of the result of the original study (Fig. 2a).  Out of the `r dplyr::distinct(analysis_level_tolerance_region, N) |> dplyr::pull(N)` available re-analysis effect sizes, `r dplyr::filter(analysis_level_tolerance_region, is_within_region == "Within tolerance region") |> dplyr::pull(percentage)`% were inside the tolerance region. As a robustness test of our analysis, we explored the degree to which we would observe different results with different tolerance regions. With a four times broader tolerance region (+/- 0.20 Cohen’s _d_), in `r dplyr::filter(paper_level_inferential_robustness_20, robust == "Inferentially robust") |> dplyr::pull(percentage)`% of the studies, all corresponding re-analysis result inside the tolerance region. Further, out of the available `r dplyr::filter(analysis_level_tolerance_region_20, is_within_region == "Within tolerance region") |> dplyr::pull(N)` reanalysis effect sizes, `r dplyr::filter(analysis_level_tolerance_region_20, is_within_region == "Within tolerance region") |> dplyr::pull(percentage)`% (`r dplyr::filter(analysis_level_tolerance_region_20, is_within_region == "Within tolerance region") |> dplyr::pull(n)`) were inside of this region (Extended Data Fig. 1a).

Alternatively, we could define the tolerance region as the percentage of the given effect size. As an additional robustness test, we varied the tolerance region between +/-5% and +/-20% but it barely made any difference regarding the percentage of robust studies (Extended Data Fig. 1b).

We next considered whether these robustness results vary by the disciplines of the studies, the study designs, the expertise of the analysts, their prior familiarity with the data, and the sample size in the data. Fig. 2b and Fig. 2c show the results for the major disciplines in our sample (>=10 studies). For Fig. 2c, we created an effect-size estimate range for each study as the numerical difference between the highest and lowest estimate of re-analysis effect sizes. In our reading, the listed disciplines do not yield large differences in the robustness of the results. Still, it is reasonable to think that the level of analytical robustness in different disciplines can be influenced by the type of studies that are commonly conducted there. For example, one could conjecture that empirical claims based on observational data show lower robustness of the conclusions since they likely involve more researcher degrees of freedom in terms of viable analysis paths than experimental research settings. Fig. 2d and Fig. 2e explore this question and indicate that the results of studies with observational study designs have lower analytical robustness in our sample, relative to experimental designs (also see Tables S5 and S6).

Considering the analytical variability found in the statistical results of the re-analyses, one immediate concern is that it could be an artefact of a lack of analytical expertise among some re-analysts. Therefore, we explored whether our robustness results exhibit a different pattern when examined in relation to the self-reported statistical expertise of the re-analysts. Visual inspection of Fig. 2f shows no support for this proposition, as a higher level of expertise corresponds with no increase or decrease in the ratio of the reported results being different from the original ones. It is noteworthy, however, that the level of self-perceived expertise was clustered in the higher end of the scale. 

```{r familiarity with paper, include=FALSE}
familiar_with_paper_data <-
  calculate_percentage(processed, familiar_with_paper)
```

Re-analyzing published studies entails a potential risk of bias if the re-analysts’ familiarity with a given study influences their choice of analysis. Re-analysts reported that they were familiar with the original study in only `r dplyr::filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> dplyr::pull(percentage)`% of cases. Moreover, there was no more than `r dplyr::filter(tolerance_region_familiarity_data, is_within_region == "Within tolerance region") |> summarize(diff = ceiling(percentage[1] - percentage[2])) |> dplyr::pull(diff)`% difference in robustness between those who were and those who were not familiar with the original study (Fig. 2g). For both groups, around two-thirds of the estimates fell outside our tolerance region. Finally, we were interested to see whether these robustness results would show a different pattern when considering sample size, as one could assume that studies with larger sample sizes could offer more robust results. Fig. 2h does not support this assumption as the density distributions of the sample sizes for results that are within and outside of the tolerance region are virtually the same. Therefore, studies with large sample sizes are not immune to analytical variability.

```{r data processing for effec_size_corr_plot, include=FALSE}
effec_size_corr_plot_data <-
  reanalysis_data |>
  dplyr::filter(!is.na(effect_size)) |>
  dplyr::filter(effect_size <= 5 & effect_size >= -5) |>
  dplyr::filter(original_cohens_d != -Inf)

# Median
round(median(effec_size_corr_plot_data$effect_size), 0)
round(median(effec_size_corr_plot_data$original_cohens_d), 0)

# Mean
round(mean(effec_size_corr_plot_data$effect_size), 0)
round(mean(effec_size_corr_plot_data$original_cohens_d), 0)
```

```{r generating effec_size_corr_plot, include=FALSE}
effec_size_corr_plot <- plot_effect_size_correlation(effec_size_corr_plot_data, x_limits = c(0, 7), y_limits = c(-5, 5), annotate_positions = c(4.7, 4))

# Save plot
ggplot2::ggsave(
  here::here("figures/effec_size_corr_plot.jpg"),
  plot = effec_size_corr_plot,
  width = 13,
  height = 10,
  dpi = 300
)

effec_size_corr_plot
```

```{r data processing for effec_size_corr_plot_zoomed, include=FALSE}
effec_size_corr_plot_data_zoomed <-
  effec_size_corr_plot_data |> 
  dplyr::filter(effect_size <= 1 & effect_size >= -1) |>
  dplyr::filter(original_cohens_d <= 1 & original_cohens_d >= -1)
```

```{r generating effec_size_corr_plot_zoomed, include=FALSE}
effec_size_corr_plot_zoomed <- plot_effect_size_correlation(effec_size_corr_plot_data_zoomed, x_limits = c(0, 1), y_limits = c(-1, 1), annotate_positions = c(0.97, 0.8))

# Save plot
ggplot2::ggsave(
  here::here("figures/effec_size_corr_plot_zoomed.jpg"),
  plot = effec_size_corr_plot_zoomed,
  width = 13,
  height = 10,
  dpi = 300
)

effec_size_corr_plot_zoomed
```

We next asked whether the re-analyses show a trend or shift in effect sizes compared to the results of the original studies. If the re-analysis effect sizes randomly vary around the original effect size, we would expect that they are larger or smaller than the original ones with an equal chance. Fig. 3a (re-analysis data trimmed at Cohen’s _d_ =< 5) and 3b (Cohen’s _d_ =< 1) indicate that re-analysis effect sizes show a tendency to be smaller than the original effect sizes as reflected in their best-fitting (least squares) line. The distribution of original and re-analysis effect sizes also supports this, as the peak of the density distribution of the latter is markedly lower. The mean effect size of the original results is `r summarize(effec_size_corr_plot_data, mean = mean(original_cohens_d)) |> pull(mean) |> round(0)` (Median = `r summarize(effec_size_corr_plot_data, median = median(original_cohens_d)) |> pull(median) |> round(0)`), whereas for the re-analysis it is `r summarize(effec_size_corr_plot_data, mean = mean(effect_size)) |> pull(mean) |>  round(0)` (Median = `r summarize(effec_size_corr_plot_data, median = median(effect_size)) |> pull(median) |> round(0)`) Cohen’s _d_, computed on *d*s =< 5. This result is consistent with the possibilities that original authors were biased to report larger effects than re-analysts, that re-analysts were biased to report smaller effects than original analysts, or both.

```{r generating effec_size_corr_merged_plot, echo=FALSE, message=FALSE, warning=FALSE}
#| fig-width: 22
#| fig-height: 30
#| out-width: 100%
#| fig-dpi: 300

label_a <- textGrob("a", x = unit(0.02, "npc"), y = unit(0.98, "npc"), 
                    just = c("left", "top"), gp = gpar(fontsize = 26, fontface = "bold"))
label_b <- textGrob("b", x = unit(0.02, "npc"), y = unit(0.98, "npc"), 
                    just = c("left", "top"), gp = gpar(fontsize = 26, fontface = "bold"))

# Combine labels with the plots using grobTree
plot1 <- grobTree(effec_size_corr_plot, label_a)
plot2 <- grobTree(effec_size_corr_plot_zoomed, label_b)

# Arrange the plots with labels
effect_size_comparison_plot <-
  gridExtra::grid.arrange(
  plot1,
  plot2,
  ncol = 1,
  heights = c(1, 1) # Adjust relative heights if needed
)

save_grid_plot <- function(filename, plot, width, height, dpi = 300) {
  g <- gridExtra::grid.arrange(grobs = list(plot1, plot2), ncol = 1, heights = c(1, 1))
  ggsave(filename, plot = g, width = width, height = height, dpi = dpi, device = "jpeg")
}

# Save the plot
save_grid_plot(here::here("figures/effect_size_comparison_plot.jpg"), effect_size_comparison_plot, width = 22, height = 30, dpi = 300)
```

## Variability of the conclusions

```{r include=FALSE}
# Distinct values of task1_categorisation
distinct(processed, task1_categorisation)
distinct(processed, task1_categorisation_plotting)

conclusions_main_data <- 
  processed |> 
  calculate_conclusion(grouping_var = simplified_paper_id) |>
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(categorisation == "Same conclusion", percentage, NA), .desc = FALSE, .na_rm = TRUE)
  )

# When threshold is 100% same conclusion for robust result
conclusions_main_robustness_data <- summarize_conclusion_robustness(data = processed, threshold = 100, operator = "==")

# Calculate value for abstract
conclusion_robust_100_percentage <- filter(conclusions_main_robustness_data, robust == "Inferentially robust") |> pull(percentage)

# When threshold is 80% same conclusion for robust result
conclusions_main_robustness_80_data <- summarize_conclusion_robustness(data = processed, threshold = 80, operator = ">")

# Calculate value for abstract
conclusion_robust_80_percentage <- filter(conclusions_main_robustness_80_data, robust == "Inferentially robust") |> pull(percentage)

# When threshold is 60% same conclusion for robust result
conclusions_main_robustness_60_data <- summarize_conclusion_robustness(data = processed, threshold = 60, operator = ">")

### Inference for  >50% threshold
conclusions_main_robustness_50_data <- summarize_conclusion_robustness(data = processed, threshold = 50, operator = ">")
```

```{r include=FALSE}
conclusions_analysis_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |> 
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  calculate_percentage(categorisation)
```

Another focal question of our study was whether the re-analysts reached the same qualitative conclusions as the original study analysts. To answer this question, we asked the re-analysts to implement any statistical re-analysis they deemed most appropriate to test the original claim using the original data, with the goal of arriving at a single conclusion. Across all individual re-analyses (*n* = `r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(N)`), `r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(percentage)`% of analyses were reported to arrive at the same conclusion as in the original investigation;  `r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(percentage)`% to no effects/inconclusive result, and `r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(percentage)`% to an effect in the opposite direction as in the original investigation (Fig. 4a).

Out of `r distinct(conclusions_main_robustness_data, N) |> pull(N)` re-analysed claims, `r filter(conclusions_main_robustness_data, robust == "Inferentially robust") |> pull(percentage)`% were robust to independent re-analysis, such that all re-analysts reported that they found evidence for the originally reported claim. It is important to note, however, that this result is contingent on the level of agreement we use to define analytically robust findings. With a more liberal definition of analytical robustness, this value was `r filter(conclusions_main_robustness_80_data, robust == "Inferentially robust") |> pull(percentage)`% analytical robustness was defined as \>80% re-analysis agreement with the original conclusion, and it was `r  filter(conclusions_main_robustness_50_data, robust == "Inferentially robust") |> pull(percentage)`% when this definition was >50% (the results with alternative levels of agreement are displayed on Fig. 4j).

```{r generating conclusion_main_plot, include=FALSE}
conclusion_main_plot <-
  plot_percentage(
    data = conclusions_main_data,
    grouping_var = simplified_paper_id,
    categorization_var = categorisation,
    x_lab = "Study number",
    y_lab = "Percentage of re-analyses",
    with_sum = FALSE,
    reverse = TRUE,
    rev_limits = FALSE,
    coord_flip = TRUE
  ) +
  theme(
    axis.text.y = element_text(size = 10, color = "black"),
    axis.text.x = element_text(size = 10, color = "black"),
    axis.title.x = element_text(size = 13),
    legend.justification = "left",
    legend.box = "horizontal",
    legend.position = "bottom",
    legend.text = element_text(size = 10)
  )

ggsave(here::here("figures/conclusion_main_plot.jpg"), conclusion_main_plot, width = 9, height = 11.69, dpi = 300)
```

```{r data processing for conclusions_discipline_robustness_plot, include=FALSE}
conclusions_discipline_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = str_to_title(paper_discipline)) |> 
  calculate_conclusion(grouping_var = paper_discipline)

conclusions_discipline_robustness_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = str_to_title(paper_discipline)) |> 
  summarize_conclusion_robustness(grouping_var = paper_discipline, threshold = 100, operator = "==")
```

```{r generating conclusions_discipline_robustness_plot, include=FALSE}
conclusions_discipline_robustness_plot <-
  plot_percentage(
    data = conclusions_discipline_robustness_data,
    categorization_var = robust,
    grouping_var = paper_discipline,
    with_labels = TRUE,
    y_lab = "Percentage of studies",
    x_lab = "",
    colors = c("#440154FF", "#FDE725FF"),
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    axis.title.y = element_text(hjust = 1, size = 13)
  )

ggsave(here::here("figures/conclusions_discipline_robustness_plot.jpg"), conclusions_discipline_robustness_plot, dpi = 300)
```

```{r generating conclusions_discipline_plot, include=FALSE}
conclusions_discipline_plot <-
  plot_percentage(
    data = conclusions_discipline_data,
    grouping_var = paper_discipline,
    categorization_var = categorisation,
    with_labels = TRUE,
    y_lab = "Percentage of re-analyses",
    x_lab = "",
    rev_limits = FALSE
  ) + 
  theme(
    plot.margin = margin(t = 20, r = 10, b = 10, l = 10, unit = "pt"),
    legend.text = element_text(size = 8),
    axis.title.y = element_text(hjust = 1, size = 13)
    )

ggplot2::ggsave(here::here("figures/conclusions_discipline_plot.jpg"), conclusions_discipline_plot, dpi = 300)
```

```{r data processing for conclusions_studytype_robustness_plot, include=FALSE}
conclusions_studytype_data <-
  processed |>
  calculate_conclusion(grouping_var = experimental_or_observational) |>
  dplyr::mutate(experimental_or_observational = str_to_title(experimental_or_observational))

conclusions_studytype_robustness_data <-
  processed |>
  dplyr::mutate(experimental_or_observational = str_to_title(experimental_or_observational)) |>
  summarize_conclusion_robustness(grouping_var = experimental_or_observational, threshold = 100, operator = "==") |>
  dplyr::ungroup()
```

```{r generating conclusions_studytype_robustness_plot, include=FALSE}
conclusions_studytype_robustness_plot <-
  plot_percentage(
    data = conclusions_studytype_robustness_data,
    categorization_var = robust,
    grouping_var = experimental_or_observational,
    y_lab = "Percentage of studies",
    x_lab = "",
    with_labels = TRUE,
     colors = c("#440154FF", "#FDE725FF"),
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    axis.title.y = element_text(hjust = 1, size = 13)
  )

ggplot2::ggsave(here::here("figures/conclusions_studytype_robustness_plot.jpg"), conclusions_studytype_robustness_plot, dpi = 300)
```

```{r conclusions_studytype_plot, include=FALSE}
conclusions_studytype_plot <-
  plot_percentage(
    data = conclusions_studytype_data,
    grouping_var = experimental_or_observational,
    categorization_var = categorisation,
    x_lab = "",
    y_lab = "Percentage of re-analyses",
    with_labels = TRUE,
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    plot.margin = margin(
      t = 20,
      r = 10,
      b = 10,
      l = 10,
      unit = "pt"
    ),
    legend.text = element_text(size = 8)
  ) +
    ggplot2::theme(
    axis.title.y = element_text(hjust = 1, size = 13)
  )

ggplot2::ggsave(here::here("figures/conclusions_studytype_plot.jpg"), conclusions_studytype_plot, dpi = 300)
```

```{r data processing for conclusions_expertise_plot, include=FALSE}
conclusions_expertise_data <-
  processed |>
  calculate_conclusion(grouping_var = expertise_self_rating) |> 
  dplyr::mutate(
    expertise_self_rating = dplyr::case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  )
```

```{r generating conclusions_expertise_plot, include=FALSE}
conclusions_expertise_plot <-
  plot_height(
    data = conclusions_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = categorisation,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    with_sum = FALSE,
    rev_limits = FALSE
  ) +
  theme(
    legend.text = element_text(size = 8),
    axis.title = element_text(size = 13)
    )

ggsave(here::here("figures/conclusions_expertise_plot.jpg"), conclusions_expertise_plot, 
       height = 6, width = 8,
       dpi = 300)
```

```{r data processing for peer_eval_subset_task1_plot, include=FALSE}
peer_eval_subset_task1_data <- 
  peer_eval |>  
  dplyr::filter(n_peer_evals > 1) |>  
  dplyr::select(paper_id, analyst_id, task1_pipeline_acceptable) |>  
  dplyr::group_by(paper_id, analyst_id) |>  
  dplyr::summarise(
    common_task1_acceptable = ifelse(dplyr::n_distinct(task1_pipeline_acceptable) == 1, first(task1_pipeline_acceptable), NA),
    .groups = 'drop'
  ) |> 
  dplyr::filter(!is.na(common_task1_acceptable))

count(peer_eval_subset_task1_data, common_task1_acceptable)
  
peer_eval_subset_task1_data <-
  peer_eval_subset_task1_data |>
  dplyr::filter(common_task1_acceptable != "(2) Acceptable but low quality") |>
  left_join(
    dplyr::select(
      processed,
      paper_id,
      analyst_id,
      task1_categorisation_plotting
    ),
    by = c("paper_id", "analyst_id")
  ) |>
  dplyr::mutate(
    common_task1_acceptable = dplyr::case_when(
      common_task1_acceptable == "(3) Acceptable and medium quality" ~ "Acceptable and medium quality",
      common_task1_acceptable == "(4) Acceptable and high quality" ~ "Acceptable and high quality"
    )
  ) |>
  calculate_conclusion(grouping_var = common_task1_acceptable)
```

```{r generating peer_eval_subset_task1_plot, include=FALSE}
peer_eval_subset_task1_plot <-
  plot_percentage(
    data = peer_eval_subset_task1_data,
    categorization_var = categorisation,
    grouping_var = common_task1_acceptable,
    with_labels = TRUE,
    y_lab = "Percentage of studies",
    x_lab = "Peer evaluation rating of\nTask 1 analysis pipeline"
  ) +
  ggplot2::theme(
    legend.text = element_text(size = 8),
    axis.title.y = element_text(hjust = 1, size = 13),
    axis.title.x = element_text(size = 13)
  )

ggsave(here::here("figures/peer_eval_subset_task1_plot.jpg"), peer_eval_subset_task1_plot, dpi = 300)
```

```{r data processing for conclusions_familiarity_plot, include=FALSE}
conclusions_familiarity_data <- 
  processed |> 
  mutate(
    familiar_with_paper = as.factor(familiar_with_paper)
    ) |> 
  calculate_conclusion(grouping_var = familiar_with_paper)
```

```{r generating conclusions_familiarity_plot, include=FALSE}
conclusions_familiarity_plot <- 
  plot_percentage(
    data = conclusions_familiarity_data,
    grouping_var = familiar_with_paper,
    categorization_var = categorisation,
    with_labels = TRUE,
    y_lab = "Percentage of re-analyses",
    x_lab = "Analyst familiarity with the paper",
    bar_outline = NA) + 
  theme(
    plot.margin = margin(t = 20, r = 10, b = 10, l = 10, unit = "pt"),
    legend.text = element_text(size = 8),
    axis.title.y = element_text(hjust = 1, size = 13),
    axis.title.x = element_text(size = 13)
    )

ggsave(here::here("figures/conclusions_familiarity_plot.jpg"), conclusions_familiarity_plot, dpi = 300)
```

```{r generating conclusions_samplesize_plot, include=FALSE}
conclusions_samplesize_plot <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::select(paper_id,
                analyst_id,
                categorisation,
                reanalysis_model_sample_size) |>
  dplyr::filter(!is.na(reanalysis_model_sample_size)) |>
  dplyr::mutate(
    categorisation = dplyr::case_when(
      categorisation == "No effect/inconclusive" ~ "No effect\ninconclusive",
      TRUE ~ categorisation
    ),
    categorisation = forcats::fct_relevel(
      categorisation,
      c("Same conclusion", "No effect\ninconclusive", "Opposite effect")
    )
  ) |> 
  plot_rain(
    grouping_var = categorisation,
    response_var = reanalysis_model_sample_size,
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000),
    x_lab = "Direction of the re-analysis conclusion\ncompared to the original effect",
    y_lab = "Sample size",
    color_by_group = TRUE
  ) +
  ggplot2::coord_flip() +
  ggplot2::theme(
    axis.title.x = element_text(size = 13, color = "black"),
    axis.title.y = element_text(size = 13, color = "black")
  )

ggplot2::ggsave(here::here("figures/conclusions_samplesize_plot.jpg"), conclusions_samplesize_plot, dpi = 300)
```

```{r data processing for inferential_threshold_robustness_plot, include=FALSE, message=FALSE, warning=FALSE}
# Estimate inferential robustness with alternative accordance thresholds
# We learned that even with a 4 times broader tolerance region  (+/- 0.5 Cohen’s d), we would find that around 80% of the studies still show results outside of this region and around half of the individual reanalysis effect sizes are outside of this region.
thresholds <- seq(10, 100, by = 1)

inferential_threshold_robustness_plot_data <-
  tibble(
    threshold = thresholds,
    robustness = purrr::map_dbl(
      threshold,
      ~ summarize_conclusion_robustness(
        processed,
        threshold = .x,
        operator = dplyr::if_else(.x == 100, "==", ">")
      ) |> 
        dplyr::filter(robust == "Inferentially robust") |> 
        dplyr::pull(percentage)
    )
  )
```

```{r generating inferential_threshold_robustness_plot plot, include=FALSE}
inferential_threshold_robustness_plot <-
  inferential_threshold_robustness_plot_data |>
  ggplot() +
  aes(x = threshold,
      y = robustness) +
  geom_line() +
  scale_x_continuous(
    breaks = seq(10, 100, 10), 
    labels = scales::percent_format(scale = 1),
    limits = c(0, 100)) +
  scale_y_continuous(
    labels = scales::percent_format(scale = 1),
    breaks = seq(0, 100, 10),
    limits = c(0, 100)
  ) +
  labs(x = "Threshold of proportion of re-analyses\narriving at the same conclusion as the original study",
       y = "Percentage of studies") +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    legend.position = c(0.8, 0.8),
    axis.title = element_text(size = 12)
  )

ggplot2::ggsave(here::here("figures/inferential_threshold_robustness_plot.jpg"), inferential_threshold_robustness_plot, height = 4, dpi = 300)
```

```{r generating inferential_panel, echo=FALSE, message=FALSE, warning=FALSE}
#| fig-width: 22
#| fig-height: 14
#| out-width: 100%
#| fig-dpi: 300

inferential_panel <- (
  conclusion_main_plot +
  (
    conclusions_discipline_robustness_plot +
    conclusions_discipline_plot +
    conclusions_studytype_robustness_plot +
    conclusions_studytype_plot +
    conclusions_expertise_plot +
    peer_eval_subset_task1_plot +
    conclusions_familiarity_plot +
    conclusions_samplesize_plot +
    inferential_threshold_robustness_plot
  )
) +
  plot_layout(widths = c(1, 3)) + 
  plot_annotation(tag_levels = 'a')


inferential_panel

ggsave(here::here("figures/inferential_panel.png"), inferential_panel, width = 22, height = 14, dpi = 300)
```

We examined whether these results show a different pattern when inspecting them along the earlier-mentioned aspects of the analyses. Fig. 4b and Fig. 4c present the proportions of conclusions that were robust in each of the listed disciplines. Just as in the case of analyses of robustness of the statistical results, the listed disciplines do not manifest large differences in robustness of the conclusions, whereas their robustness may be influenced by the study designs most common in a given field or subfield. Fig. 4d supports this notion as it indicates that nearly half of the conclusions from experimental studies remained robust upon independent re-analysis, whereas less than one-third of observational studies yielded robust conclusions. Moreover, Fig. 4e indicates that, although the majority of re-analyses for both study designs reached the same conclusions as the original study, the figure was `r dplyr::filter(conclusions_studytype_data, categorisation == "Same conclusion") |> summarize(diff = ceiling(percentage[1] - percentage[2])) |> dplyr::pull(diff)`% higher for experimental studies than for observational studies. Just as for the robustness of the results, we can ask whether the deviation from the originally reported claim in terms of conclusions is explained by the re-analysts’ lack of analytical expertise. Fig. 4f shows no support for this conjecture when evaluating the pattern of results as a function of self-reported statistical expertise. The same conjecture can be assessed by considering the quality of the submitted statistical analyses that were evaluated by peer evaluators on a subset of the analyses (see Methods). Fig. 4g shows that the proportion of inferentially robust conclusions is numerically larger for analyses that were rated as medium-quality by peer evaluators than for analyses that were rated as high-quality. Whether this pattern was a result of noise or whether more sophisticated analyses are characterized by greater heterogeneity in approaches and results should be the topic of future metascientific projects.

Just as for the analyses of the robustness of the statistical results, we were interested to see whether these results show a different pattern when inspecting them as a function of analysts’ prior familiarity with the dataset. Although those familiar with the original study did report the same conclusion in a higher proportion than those who were not familiar, `r dplyr::filter(conclusions_familiarity_data, familiar_with_paper == "Yes", categorisation == "No effect/inconclusive") |> dplyr::pull(percentage)`% of their re-analyses still indicated a conclusion different from the original one (Fig. 4h).

Again, we aimed to explore whether these robustness results would show a different pattern when considering sample size. As presented in Fig. 4i, the density distribution corresponding to the analyses with the different conclusion types shows a comparable spread, suggesting that the conclusions of studies with smaller and larger sample sizes appear to be similarly contingent on analytical choices.

For descriptive information about the re-analysts, peer evaluators, and additional robustness analyses, see Extended Data Figs. 2, 3, 4, and Supplementary Information’s General descriptives, Demographics of the re-analysts, Peer evaluation, and Robustness analyses sections.

# Limitations

This study has a number of limitations. First, our collection of 100 articles represents only a tiny fraction of all the empirical work in the social and behavioural disciplines. Despite our efforts to select a representative sample of published articles across disciplines from the investigated time period, we could not include studies when the underlying data were not obtainable, and we excluded studies when our screening attempt to analytically reproduce the original results following the published procedures failed. We cannot exclude the possibility that these prerequisites, in addition to the self-selection of the analysts, led to sampling bias.

Although we conducted more than 500 analyses, our project included only five independent analyses for most datasets, therefore, we do not know to what degree these analyses capture the full variability of analyses and results for the given research question and dataset. Also, since we re-analysed already published studies and the re-analysts were provided with these studies, the original analysis pipeline could have anchored some of the choices of the re-analysts. On the other hand, some analysts could have been motivated to produce alternative results, as it is a basic incentive of scientists to say something new.

While Cohen’s _d_ has the advantage of being easy to compute and comparable across different analyses, Kümpel and Hoffmann[@kumpel_formal_2022] recently proposed the concept of generalised marginal effects (gMEs), an effect size metric that is both formally applicable and comparable across different statistical models. We had not originally planned to calculate standardised gMEs, and, accordingly, did not collect all required analysis outputs to compute them across the board. Still, we calculated gMEs for a sample of our studies to showcase their potential for future multi-analyst studies (Supplementary Fig. 1).

We presented some exploratory analyses, but there are many other factors to explore that could contribute to analytical variability (e.g., topical expertise). Finally, despite our best efforts to conduct quality checks on the re-analyses to ensure the soundness of the analytic strategies[@silberzahn_many_2018], it is possible that some of the discrepancies between the original and the new results are due to weaknesses in the re-analysts’ approach rather than equally justifiable alternative analysis decisions. It is likewise possible that there are weaknesses in the original analysts’ approaches. It is unknown whether the quality control processes for the re-analysts resulted in better, worse, or similar overall quality of analysis decisions as compared with the quality control processes for original analysts' decisions. The declared statistical expertise of the re-analysts makes us believe that the observed heterogeneity in analyses and outcomes is a good representation of variation in informed analysis decision-making in social-behavioural research.

# Discussion

Are published results in the social and behavioural sciences robust to independent re-analyses? The present exploration shows considerable variability due to researcher degrees of freedom in statistical choices. Overall, when independent researchers analysed the same research question on the original data, `r filter(conclusions_main_robustness_data, robust == "Inferentially robust") |> pull(percentage)`% of studies remained robust to independent re-analysis in the sense that all re-analysts arrived at the same conclusion as the original analyst or analyst team. Notably, the new conclusions converged with the original ones in `r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> summarize(total = sum(percentage)) |>  round(0)`% of the individual re-analyses. Our descriptive results suggest a number of hypotheses concerning the circumstances in which we could expect greater analytical variability.

## Why can there be multiple answers?

Faced with the variability in the analysts’ effect-size estimates and conclusions, one intuitive hypothesis is that the variation must be due to researcher characteristics, such as statistical or field-specific knowledge. Previous multi-analyst studies found little to no effect of researcher-specific characteristics, such as experience in the field or statistical expertise[@silberzahn_many_2018, @breznau_observing_2022, @coretta_multidimensional_2023]. Instead, they suggest that analytic results are dependent on the particular choices that the analysts make among similarly acceptable data processing and analysis choices[@coretta_multidimensional_2023]. For example, when 46 independent analyst teams analysed the same speech dataset to answer the same research question, the authors concluded that “depending on the choice of how the speech signal is operationalised, researchers might find evidence for or against a theoretically relevant prediction” (p. 21)[@coretta_multidimensional_2023].

In line with previous findings, our results showed no strikingly different patterns across self-reported statistical expertise and experience in a matching field (see Fig. 2f, Extended Data Fig. 4,  Supplementary Table 5, 6, 7, 8, and 9). At the same time, the few analysts who reported that they were familiar with the original article produced alternative results and conclusions at a comparable rate. More importantly, our peer evaluation process did not indicate that the analytical variability of the re-analyses was due to inadequate statistical practices. These results are in line with Menkveld et al.[@menkveld_nonstandard_2024] in which the quality assessment of the proposed analysis pipelines did not statistically explain the results.

Another line of thought would suggest that the lack of robustness in the original published results reflects some conceptual ambiguity in the theories or methodology[@kumpel_formal_2022]. Research hypotheses are often short verbal expressions that do not force the specifications of the analyses. The underspecification of claims31 could represent a major source of ambiguity in analytic decisions. We could not test the role of hypothesis ambiguity in a controlled manner, but it is a plausible contributor considering that social science theories often make general claims across many variables, creating theory-laden choice points regarding how constructs are operationalised, and how hypotheses are tested[@scheel_why_2022]. 

Regarding methods, we explored our results by separating them by experimental and observational study designs, and observed that the proportions of results and conclusions that were analytically robust were 15-20% higher for the experimental studies. The estimated range of effect sizes was also apparently wider for observational studies compared to experimental ones. This exploratory finding motivates the hypothesis that the increased control over data collection circumstances and the reduced number of variables in experimental versus observational research translate to more limited analytic flexibility. Notably, however, there was still substantial statistical variability among findings from experimental studies.

## Why do these findings matter?

Where multiple acceptable analytical paths exist, researchers can use this freedom opportunistically[@oberauer_addressing_2019, @brodeur_methods_2020] and bias the results towards desired findings ("myside bias"[@simmons_false-positive_2011]). The much-discussed credibility challenges in the social and behavioural sciences stem partly from the suspicion that the prevailing incentive systems for publication encourage researchers to report and interpret empirical data to best serve non-epistemic goals such as storytelling[@stanovich_bias_2021]. Reform initiatives, such as the preregistration of research and analysis plans, aim to decrease researcher degrees of freedom to tweak the analytic method or the research question to the observed data. Would results in these fields become markedly more credible if every study was preregistered? Since preregistration is a protection against overfitting, we hypothesize that it would reduce or eliminate the showed finding that original analyses observed stronger evidence for positive results than re-analyses. However, we also hypothesize that preregistration would have little impact on the observed heterogeneity across alternative analysis strategies since registering and following a single analytic path constrains the analysts only from choosing opportunistically from the alternative analytical paths. Still, it does not confer any unique statistical or epistemic status to the pre-selected analytic path[@holzmeister_heterogeneity_2024]. Unexplored but alternative justifiable analyses applied to the same data could still lead to very different results. The present exploration is clear about the presence of this variability in approaches, results, and inferences in the social and behavioural sciences. Without exploring this variability, authors cannot guarantee consumers of their research that the reported conclusions hold a privileged status over alternative conclusions.

## What can we do?

The outcomes of this project suggest that the empirical answers to research questions in the social and behavioural sciences depend on the analytic paths taken to pursue them. Therefore, we advocate for the broader adoption of approaches that explore, recognise, and address the uncertainty created by analytical variability.

Two main types of solutions are (1) multi-analyst studies, such as our own, where multiple investigators independently follow their own approach, and (2) the multiverse[@patel_assessment_2015, @steegen_increasing_2016, @gernsbacher_rewarding_2018] approach, where one investigator or team performs numerous analyses across the set of reasonable pipelines. Conducting exploratory studies to identify analytical uncertainties and holding out samples are further advisable practices to tackle analytical variability.  

Project leaders aiming to conduct multi-analyst studies can consult various tutorial papers and guidelines. Aczel et al.[@aczel_consensus-based_2021] provide an expert consensus guideline on the entire life-cycle of multi-analyst projects from recruiting suitable analysts, through conducting the project to the reporting of the outcomes. Kümpel & Hoffmann[@kumpel_formal_2022] offer a framework for synthesising objective outcome metrics. The Subjective Evidence Evaluation Survey[@aczel_consensus-based_2021] is a tool for systematically exploring and quantifying subjective measures of evidence in multi-analyst studies allowing analysis teams to subjectively reflect on various aspects of evidence, such as coherence, robustness, and relevance, as well as the quality of the research design and data.

Multiverse analysis is also useful, especially when the dataset cannot be shared with other research groups due to confidentiality reasons or when there are insufficient human resources to recruit several independent analysts. Several guideline papers help researchers conduct and interpret such analyses[@patel_assessment_2015, @simonsohn_specification_2020, @del_giudice_travelers_2021, @liu_boba_2020, @olsson-collentine_meta-analyzing_2023].

Recently, many scholars have called for a stronger focus on replication in science[@zwaan_making_2018]. Similar to preregistration, however, replications are unlikely to help address the robustness of results to multiple analysis strategies as they intentionally repeat the same (or at least a very similar) analysis path. In this sense, replications can help detect bodies of work in which authors may have leveraged their researcher degrees of freedom to generate results that are in line with their own or the journal’s expectations. All other things being equal, a severely *p*-hacked literature should contain fewer replicable findings. And yet, replicability does not eliminate analytical variability itself. Nevertheless, having multiple studies, creates an opportunity to observe if analytical variability is, itself, replicable. For example, imagine that Study A provides evidence for a claim with Analysis 1 but not with Analysis 2. If several replications also find evidence for the claim with Analysis 1 but not with Analysis 2, then the analytic choices are directly implicated in how evidence for the phenomenon is observed. However, if it is random across replications whether Analysis 1 or Analysis 2 provides evidence for the claim, then the implications of the analytic variability are very different. The combination of replications and robustness investigations will facilitate the advancement of stronger theoretical underpinnings of the topics of study, and could reduce analytical variability in the long run by creating a more direct mapping between theory and measurement[@auspurg_has_2021, @schaller_empirical_2016, @steegen_increasing_2016].

All in all, we argue that the scholarly communication system could foster more engagement with systematic and transparent robustness testing. As a starting point, the research data shared openly alongside codebooks and analysis scripts is a prerequisite for any assessment of analytical robustness. Research findings of particular scientific or societal importance could be accompanied by robustness reports[@bartos_introducing_2025] that summarise the results of alternative theory-motivated analytic choices by independent analysts. This publication format already provides a platform for analysts to scrutinise the fragility of the findings before they have a major impact on scholarship and policy( see <https://scipost-staging.org/JRobustRep>). 

## What did we learn about the robustness of research?

Our results support the view that the results in social and behavioural science studies are contingent on the analyst’s choices and if an analyst reports a single result from a single analytical path, they have not exhausted the possible answers that the dataset can provide. This finding aligns with the conclusions drawn by Wagenmakers, Sarafoglou, and Aczel[@wagenmakers_one_2022], that the belief that "for any dataset, there exists a single, uniquely appropriate analysis procedure" and "multiple plausible analyses would reliably yield similar conclusions"(p. 424) are no more than statistical myths. Without multi-analyst and multiverse approaches, the fragility of empirical findings remains.

Nonetheless, we emphasise that an optimistic or pessimistic interpretation is a matter of perspective and greatly depends on what evidential support we expect from a given study. Therefore, whether a result is satisfyingly robust will always depend on our epistemic needs and the precision we expect from our results. We caution against using blanket rules in aggregating or interpreting results across different analytical approaches within the same investigation.

Objectivity is a fundamental ideal of science, implying that claims about the world should not be contingent on the predispositions of the claimant. What our results reveal is not that we must distrust or reject the results of the past, including the studies we analysed. Instead, they suggest that we should adopt greater caution about the evidence that single analytical paths can offer to support social and behavioural science claims. We believe that the limitations of "single-shot" analyses cut across numerous scientific disciplines. Methodological innovations, such as multi-lab collaborations, multi-analyst approaches, or multiverse methods, could increase the robustness of the social and behavioural sciences, and perhaps more broadly, in other empirical fields.

## Figure legends

__Fig. 1 | Major sources of analytical variability.__

Analytical variability can arise from the ambiguity of the research question, the alternative operationalisations of the concepts, variations in data preprocessing options, or model and method choices, as well as from undiscovered statistical or data processing errors.

__Fig. 2 | Analytical robustness of the statistical results.__ 

__a,__ Effect size of the original analysis (gray square; all represented as positive values) and the effect sizes of the re-analyses (red dot) for each study. The figure displays `r processed |>  filter(reanalysis_cohens_d <= 4.5 & reanalysis_cohens_d >= -2) |> filter(!is.na(reanalysis_cohens_d)) |> nrow()` re-analysis effect-size estimates that were convertible to Cohen’s d and excludes effect sizes outside the [-2, 4.5] range. For the `r missing_original_n` studies listed at the bottom of the figure, we could not determine the original effect size due to missing information. Study numbers correspond to studies listed in <https://osf.io/mkwhn>. The studies are ordered by the size of the original effect size. __b,__ Percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by major disciplines. The figure displays the count of re-analyses next to each discipline name. __c,__ Distributions of effect-size estimate ranges calculated per study for each major discipline. __d,__ Proportion of re-analysis results falling within or outside of the tolerance region of the original results of the studies by study type. The figure displays the count of re-analyses next to each discipline name. __e,__ Distribution of effect-size estimate ranges calculated per study for observational and experimental studies. __f,__ Percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by self-rated expertise (1= beginner, 10 = expert). __g,__ Percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by declared familiarity with the study. h, Distribution of sample sizes separately for re-analysis effect sizes falling within or outside of the tolerance region of the original results. __i,__ Proportion of effect sizes falling within the preset tolerance range (+/- 0.05 Cohen’s d) for each study. 

__Fig. 3 | Original study effect size versus re-analysis effect size.__ 

The thin diagonal line represents an ideal case when the re-analysis effect sizes are equal to original effect size, the thick line shows the best-fitting (least squares) line of the displayed dots. Density plots of original (*n* = `r dplyr::filter(paper_level_inferential_robustness, robust == "Inferentially not Robust") |> dplyr::pull(N)`) and re-analysis (*n* = `r nrow(processed)`) effect sizes are parallel to their respective axis. β refers to the regression slope. Figure __a__ shows effect sizes Cohen’s *d* =< 5, Figure __b__ displays the same for effect sizes Cohen’s *d* =< 1. 

__Fig. 4 | Analytical robustness of the conclusions.__

__a,__ Proportion of same conclusion, no effect/inconclusive results, and opposite direction conclusions for each study. Study numbers correspond to studies listed in <https://osf.io/mkwhn>. __b,__ Proportion of inferentially robust results (i.e., all re-analyses arrived at the same conclusion for the given study) by major disciplines (more than 10 studies in our collection: Economics, Political Science, and Psychology). __c,__ Proportion of same effect, no effect/inconclusive results, and conclusions in the opposite direction of the original studies by major discipline. The number of re-analyses is displayed below each discipline. __d,__ Proportion of inferentially robust results by study design (experimental vs. observational). The number of re-analyses is given below each study design. __e,__ Proportion of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by study type (experimental, observational). __f,__ Proportion of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by self-rated expertise (on a scale of 1 (Beginner) to 10 (Expert)). __g,__ Proportion of inferentially robust studies by the acceptability of the analysis pipelines according to the peer evaluators. For this figure, we included only studies with more than one peer evaluation and where the peer evaluators agreed on their rating. The figure shows only the rating options with 5 or more re-analyses in that category. __h,__ Proportion of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by declared familiarity with the study. __i,__ Distribution of the sample size of the re-analyses resulting in the same conclusion, no effect/inconclusive, and opposite effects. Sample size values were available for `r dplyr::filter(processed, !is.na(reanalysis_model_sample_size)) |> nrow()`) re-analyses. __j,__ Percentage of studies with robust conclusions above different levels of re-analysis consensus. Re-analysis consensus refers to the agreement among the conclusions drawn by the original study and the independent re-analyses.

# Methods

All methods and procedures in this study were vetted by a panel of experts with prior experience in multi-analyst studies or who are specialists in the relevant methodology (see Additional Details of Method within Supplementary Information).

## Preregistration

The methods, materials, analysis plan, peer evaluation, and data management strategy of the project were preregistered on the OSF. Deviations from the registered plan are reported and explained in the “Deviations from preregistration” supplementary document.

## Ethical considerations

The datasets resulting from this project were not considered human subject research and are covered under an umbrella ethics protocol that was managed by the Center for Open Science (COS) (BRANY SBER IRB protocol #21-056-749), with concurrence from the United States Naval Information Warfare Center Pacific, HRPO. The institutional ethics board of the Faculty of Education and Psychology at Eötvös Loránd University, Budapest, Hungary, determined that the re-analysts are not considered research participants and that the project raises no ethical concerns.

# Materials
## Selection of studies

The selection of studies was completed in two stages. In the first stage, the SCORE team created an initial study and claim collection. From this collection, we selected our sample using additional criteria.

In the SCORE project, a stratified random sample of 600 articles was identified from a larger pool of randomly stratified ~30,000 articles from 62 journals, published between 2009 and 2018. The journals covered the main branches of social and behavioural sciences (criminology, economics, education, health-related, marketing/organisational behaviour, management, political science, psychology, public administration, sociology). To obtain the original studies, the following steps were taken: First, the paper was reviewed. If data and/or code were available, they were downloaded and saved into a project on OSF. If data and/or code were not available, the SCORE team attempted to contact the corresponding author to request that they share the data and code used for the original publication. Studies were excluded from the sample if they did not contain at least one inferential test using non-simulated, human data, where human data are defined at any level of human organisation (e.g., the individual person, family, political entity, firm, economic unit). The majority of the studies were tested for analytic reproducibility using the original specification, which is to be distinguished from robustness to alternative specifications. Analytic reproducibility was tested in cases when both original data and code were available (*n* = 63), or when the original data were available but the original code had to be adapted by the SCORE team in order to successfully reproduce the result (*n* = 7). If data were available but the original code was not, SCORE sourced a collaborating lab to generate new analytic code for the reproduction (*n* = 10). If data and code were not available, the collaborating lab used the secondary source data, which were shared upon request (acquired by SCORE), alongside newly generated analytical code for the reproduction (*n* = 11). Some reproductions were never attempted (*n* = 9). If the analytic reproduction failed, the paper was removed from the pool. Therefore, the present project focused solely on robustness to alternative specifications and did not conduct direct reproducibility checks using the original specification, as these had already been carried out by SCORE. Further details of the SCORE methodology (list of journals, selection process, etc.) are available in the original report[@alipourfard_systematizing_2021]. 

In the present work, a further requirement of the selected studies was to contain a single inferential statistical test result that corresponded to the claim with our instructions. Thus, we ensured that given the claim and the instructions, no other statistical result could correspond to the claim in the original article. If all potential claims from the study were too ambiguous and, therefore, could not be linked with a single inferential test statistic with the specification instructions, the study was excluded from our sample. The above-described study selection process was continued until we reached our target number of 100 studies, corresponding claims, and datasets.

The selected studies and all available corresponding data and materials were made available to the re-analysts so that they could fully understand the selected claim and approach. There are trade-offs for how much information to give to the re-analysts to conduct re-analyses. Complete blinding of the original analysis strategy would ensure an entirely independent decision-making process about how to analyse the data. However, in much scientific writing, there is insufficient clarity in the description of the theoretical background, rationale, and specification of the conceptual model to be tested. In some papers, there is a clean break between these and clear hypotheses to test. In other papers, the narrative intermixes theoretical statements and analysis decisions and may not clearly state hypotheses or how they correspond with observed results. As a consequence, attempts to blind papers inevitably lead to variation in what is blinded across papers and many subjective decisions about what should be blinded (because it provides information about analysis strategy) and what can remain unblinded (because it provides information about theory and rationale). A major risk of those blinding decisions is that important information could be removed, which would weaken the re-analysts’ ability to conduct a fair re-analysis of the original claim. As such, we opted for complete transparency of the original article so that no potentially important information was missing for the re-analysts, and we instructed re-analysts that they should create an analysis plan based on their own decisions for how best to assess the study’s claim. On balance, this increases the risk of dependent decision-making but reduces the risk of misspecification of the hypothesis and rationale of the original research. In this context, we judged the latter to be a more important precondition for conducting an informative study.

## Claim selection

Claim selection was built on Phase 1 of the SCORE project effort. The claims identified for Phase 1 of SCORE were executed according to a “single trace” approach, where only a single claim trace was extracted from the article, which corresponded to one statistically significant inferential test result (see <https://docs.google.com/document/d/1yKvjMFaIcwCLq1k-02m_Y1G937yMIc__1JK4OfMay40/edit?usp=sharing>). Within the current project, first, the lead team ensured that the extractions (i) are understandable, (ii) contain only one claim, (iii) indicate the direction of the effect, (iv) there is a statistical hypothesis test-based result provided in the article that corresponds to the claim; and (v) the claim was phrased on a conceptual and not statistical level. If not, then they extracted the part of the claim that is relevant, or if this could not be achieved, they selected another more suitable sentence from the abstract, or if this could not be achieved, they searched for another suitable sentence from other parts of the article that could satisfy all of our criteria. When none of these steps presented a claim that satisfied the expectations, then the given article was not used in our study (for their list and explanation of dropping, see our data table). Where an expression of a claim has been judged by the lead team as ambiguous or rhetorical, they substituted the expression with an ellipsis mark (e.g., “dramatically increased” to “ … increased”) while preserving the original wording and the meaning of the claim. Only in cases where, due to the selection, the wording of the claim became complicated, ungrammatical, or contained an ambiguous definition or an unexplained abbreviation, did the core team make necessary (and marked) adjustments in the grammar or wording of the claim while preserving the original meaning of the extraction. For example, the following selection “Three factors increase the salience of the proliferation threat: (1) prior violent militarized conflict…” was changed to “[prior violent militarized conflict] increase[s] the salience of the proliferation threat …”. The list of claims can be found at <https://osf.io/mkwhn>.

## Analysis instructions

For the re-analysts’ second task, instructions were needed in cases where the original paper contained more than one statistical analysis corresponding to the high-level claim, in order to be able to compare the new result to the one in the original paper. For this, the lead team prepared certain instructions (e.g., data selection, exclusions) that single out only one statistical result in the original paper. The instructions always remained circumstantial (e.g., data selection, exclusions, choice of measurement) and never gave direct instructions to the choice of statistical approach or full specification of the model. 

## Procedures
## Re-analyst recruitment

Our preregistered aim was to have at least five independent re-analyses carried out for each of the 100 selected studies (Extended Data Fig. 5). Our choice of 5 analyses per study was led by practical considerations, as we judged that recruiting 500 analysts for a project is the limit of our capacity.

Participation in the project was advertised on social media, at conferences, in mailing lists (e.g., SCORE collaborator list), via personal networks, and in research newsletters. As a response to our recruitment call, 1141 researchers signed up to participate in our study. Out of these volunteers, 459 signed up to analyse at least one dataset and submitted their work by the deadline or an extended deadline. From all the eligible volunteers, we selected re-analysts and peer evaluators on a first-come, first-served basis. The expectation of participation in the study was experience with conducting statistical analyses, and this was communicated to the volunteers from the start of the recruitment. Re-analysts were informed that they would qualify as authors on the publication of this study if (1) they completed their analyses and submitted all required materials and the post-analysis survey on time; (2) their analyses passed the peer evaluation, and (3) they reviewed and approved the manuscript in time.

Re-analysts received a flat fee of 100 USD for each of their completed re-analyses (including both Task 1 and Task 2) if they submitted their work before March 2023, the deadline of the grant budget, unless they were from an embargoed country, in which case we were unable to transfer any payment. Peer evaluators received a flat fee of 10 USD per peer evaluation. Any further volunteers were informed that this payment did not apply to them.

Upon joining the project, the volunteers for re-analysis were required to accept the project requirements. They were informed about (a) their tasks and responsibilities; (b) the project confidentiality agreements; (c) the plans for publishing the research report and presenting the data, analyses, and conclusion; (d) the conditions for an analysis to be included or excluded from the study; (e) that their names will be publicly linked to the analyses; (f) the re-analysts’ rights to update or revise their analyses; (g) the project time schedule; and (h) the nature and criteria of compensation. Re-analysts were informed that, whereas they could consult other researchers during their analyses, they could not work in teams in this project. Before discussing the details of the analyses with others, the re-analysts were asked to ascertain that the person was not another re-analyst on that dataset. All communication materials of this study are openly available on the public repository of the project at <https://osf.io/nvy8a>.

## Assignment of analyses and tasks

The following procedure was first piloted with two analysts to learn about the practical challenges and time demands of the following tasks. As the results of those analyses were not of central interest, we kept no records of them.

First, each re-analyst was asked to assign themselves to one study, but at later rounds of recruitment, we allowed re-analysts to complete analysis on another paper, other than the one they completed earlier. They were asked to choose those studies where they saw the greatest relevance of their expertise. The authors of the original study could not be the re-analysts of that study.

For several practical reasons, the re-analyses were not started at the same time for each study and each analyst. Firstly, it took us several rounds of recruitment to gather the target number of analyses for each study, mainly due to dropouts, delays, unplanned personal difficulties, and a shortage of staff. Secondly, our analysts found it difficult to retrieve, open, or interpret some of the datasets. In some cases, we had to reach out to the original authors, causing further delays in the project.

The task of the re-analysts was to reflect on the corresponding claim (see claim selection) by re-analyzing the corresponding data. The re-analysts were provided with access to the datasets, extracted claims, the original articles, and all the corresponding materials. They were informed that their analyses should be conducted preferably with scripts that could reproduce all their results (including data preprocessing, extraction of test statistics and p-values/Bayes Factors, computing effect-size measures, etc.), but they could use the statistical software of their choice to produce an analysis script. Re-analysts were asked to write and structure their code such that others could understand their analysis scripts (e.g., by annotating the different analysis steps), and they were also informed that the analysis scripts from all analysts would be made publicly available with their names linked to the analyses. 

Re-analysts received two main tasks for each study, where Task 2 was given after the completion of Task 1. Once Task 1 was submitted, the analysts could not change the submission of Task 1 unless they were asked by the lead team to provide some missing information from their analysis.

_Task 1_ The re-analysts were asked to reflect on the selected claim by re-analyzing the corresponding data. They could conduct and report as many analyses as they wished, but they had to draw a single conclusion from their analysis. They were asked to report their analyses and indicate whether their results provided evidence for the relationship/effect as claimed by the original study.

_Task 2_ For this task, the re-analysts had to produce only one statistical result corresponding to the claim they studied in Task 1, which would be compared to a statistical result in the original paper. The lead team provided certain instructions (e.g., data selection, exclusions) for this analysis to be able to compare the new result to one result in the original paper (see Analysis instructions section). Re-analysts were asked to report their results in terms of statistical families of r, z-, t-, F-, or χ² tests (or their non-parametric versions). In addition, they were asked to report sample sizes (e.g., per group) and the corresponding degrees of freedom. By this means, most results could be translated into standardized coefficients by the coordinators.

The reason for requiring two analyses from the re-analysts was that they served two different aims. The results of Task 1 aimed to answer our first preregistered project question: “Do different analysts arrive at the same conclusions as the analyst of the original study?”, whereas the results of Task 2 aimed to answer our second preregistered project question: “Do different analysts arrive at the same effect estimates as the analyst of the original study?” We found that asking only one of the tasks would not have been sufficient to fully address both questions. In Task 1, researchers were not constrained to one analysis, so they could have produced more than one statistical result in order to draw a conclusion from the dataset. Therefore, in Task 1, it was not guaranteed that we would be able to select a single effect size from each analyst in order to answer our second project question. Another challenge to finding an answer to our second question was that in some of the original articles, one claim could have had more than one corresponding statistical result listed. In these cases, we prepared instructions for Task 2 in order to single out only one statistical result in the original paper. For example, if the original study contained two corresponding regression models, one with some exclusions and one with no exclusions, then we chose one of them (e.g., the latter), and instructed the re-analysts not to apply any exclusions to the analysed data. In all other regards, re-analysts were free to conduct their calculations according to their best judgment. 

After completing the analysis and writing up the methods, results, and conclusion, re-analysts were expected to upload their analysis code (if available) to the corresponding OSF folder. Their reported methods, results, and conclusions were collected via an online form (see <https://osf.io/fjnhz/>). When uploading the materials, they were also asked to fill out a post-analysis survey. All major communications between the core project team and re-analysts from the study are openly available on the public repository of the project.

## Peer evaluations

The goal of peer evaluation in this project was to assess whether the applied analytical choices are acceptable and whether the reported conclusion follows from the statistical results. By acceptable, we mean that peer evaluators agree that the analysis pipeline is within the variations that could be considered appropriate by the scientific community in addressing the given analytical task. 

The peer evaluation phase did not address potential errors in translating the description of the analytic methodology into analysis scripts. To mitigate potential gross errors in the analysis, peer evaluators were provided with a thorough and standardised description of the results and conclusions obtained using the described analysis, including sample sizes, the effect size, the test statistic, and degrees of freedom. From the description of the dataset, the description of the analysis, and the reported results and conclusions, peer evaluators were able to identify potential flaws in the implementation of the analysis that could stem from errors and/or mismatches.

## Assignment of the analyses

When assigning the volunteer peer evaluators to analyses, the initial rule was that they should not evaluate any re-analyses conducted on datasets they had re-analysed as a re-analyst. In practice, for logistical reasons, this rule was applied in all but six cases (i.e., 99% of peer evaluations were carried out on a dataset that was different from the dataset they analysed themselves). They were asked to choose to evaluate those analyses where they see the greatest relevance of their expertise. If, after choosing a study to evaluate, a peer evaluator did not feel sufficiently skilled/experienced to judge whether the proposed analysis was acceptable, he/she was told not to fill out our template and should return the re-analysis to the pool and choose a new one. 

## Peer Evaluation Procedure

For details, see the corresponding section in the Supplementary Information.

## Analysis methods

This exploratory study contains no inferential statistics. Besides the frequency- and proportion-based summary statistics, we calculated only the effect sizes of the results from the original articles and the re-analyses. 

## Cohen’s *d* effect sizes

Following our preregistration, we converted all results into Cohen’s *d*s wherever possible. For a number of cases, we could not achieve this due to missing information in the original studies or reported statistics that cannot be converted into Cohen’s *d* (e.g., logistic regression). All the conversions are listed in the R scripts and the data documentation. All the original effect sizes are listed as positive values, and the re-analysis effect sizes are negative only when they showed an opposite effect compared to the original study.

For further information on methods, see Supplementary Information.

## Reporting summary

Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.

## Data availability

Study data and materials are available on the project OSF (<https://osf.io/q5h2c/>) and GitHub repositories (<https://github.com/marton-balazs-kovacs/multi100/>). Archived data include the original datasets or a description of how to gain access to them. Our shared materials include all the survey questions and the general communication texts and instructions that we sent to the re-analysts and peer-evaluators. We excluded from our data files the email addresses of the re-analysts, as well as the records of those analysts who did not comply with the instructions and did not submit all the required analyses by the deadline. For further details about our exclusion criteria and procedure, see our Supplementary Information document.

## Code availability

All analysis codes serving this project are available at <https://github.com/marton-balazs-kovacs/multi100>.

# References {-}

::: {#refs}
:::

# Acknowledgements

We thank Laura Caron who contributed to the re-analyses but did not wish to be listed as an author. We also thank Dan Sun who contributed to the re-analyses. We are grateful to Peter Czingraber, Julianna Harangozo, and Marton Kolozsvari for their assistance with data review and preparation.We are grateful to Meike Latz for helping with our graphic designs. We say thanks to our reviewers whose recommendations and insights greatly improved this work.

# Author information

D.v.R. was supported by 016.Vidi.188.001; M. Adamkovic was supported by APVV-22-0458, PRIMUS/24/SSH/017; A. Sarafoglou was supported by Amsterdam Brain and Cognition (ABC) project grant (ABC PG 22), 2024 Ammodo Science Award; G. Navarrete was supported by Comisión Nacional de Investigación Científica y Tecnológica (CONICYT/FONDECYT Regular 1211373); D. Lacko was supported by Czech Science Foundation (no. GA24-11974S); B.A., B. Szaszi, L.K., T.M.E., and B.A.N. were supported by Defense Advanced Reseach Projects Agency (DARPA): cooperative agreements No. HR00112020015 and N660011924015; C.B.-R. was supported by Deutsche Forschungsgemeinschaft (DFG) through the CRC TRR 190 “RationalityandCompetition.”; H.C. was supported by Deutsche Forschungsgemeinschaft, Grant/Award Number: 402170461 [TRR 265]; S.D.-S. was supported by Deutsche Forschungsgemeinschaft, grant no. 440923825; R.A.Z. was supported by Deutscher Akademischer Austauschdienst (DAAD), ref. no. 91803023; M. Rusconi and R.S. were supported by Dipartimento di Eccellenza 2023-2027, DIECO_DDE_2023-2027; C.S. was supported by ESRC grant ES/T015357/1; D.R.S. was supported by ESRC grant ES/Y002482/1; I.M. was supported by European Union - Next Generation EU, Mission 4 Component 1 CUP J53D23007960006 and J53D23017140001; D.W. was supported by European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement no. 665501, Flemish Science Foundation (FWO, FWO.KAN.2019.0023.01), Special Research Fund of Ghent University; T.N. was supported by Excellence Fund of ELTE Eötvös Loránd University and the János Bolyai research fellowship of the Hungarian Academy of Sciences.; Y.L.M. was supported by F.R.S. - FNRS (Fonds National pour la Recherche Scientifique); J.V.A. was supported by FWO.3E0.2021.0085.01; F.R. was supported by German Research Foundation (CRC TR224, Project A04) and Joachim Herz Stiftung; X.S. was supported by German Research Foundation, grant number SCHM 3460/3-1; G. Niso was supported by Grants RYC2021-033763-I and PID2023-150034OA-I00 from the Spanish Government; D. Grigoryev was supported by HSE University Basic Research Program; E.L.U. was supported by INSEAD R&D committee; E.C. was supported by Insight Grant $435-2024-0336, Social Sciences and Humanities Research Council (SSHRC) of Canada; S. Alzahawi was supported by Institute for Museum and Library Services #LG-250130-OLS-21; R.M.R. was supported by John Templeton Foundation (grant ID: 62631); R.D. was supported by Keynes Fund [JHVH], CRASSH Grant (climaTRACES lab) and Cambridge Humanities Research Grants; A.D. was supported by Knut and Alice Wallenberg Foundation, Marianne and Marcus Wallenberg Foundation, Jan Wallander and Tom Hedelius Foundation; M.M.E. was supported by Leverhulme Trust Early Career Fellow, RM56G0344; H.D. was supported by MOE (Ministry of Education in China) Project of Humanities and Social Sciences, 24YJA190001; C.M.G. was supported by MSCA-PF; J. Hogeveen was supported by NIAAA (R01AA030283); NSF (BCS 2237795); I.R.J. was supported by NIHR Manchester Biomedical Research Centre (NIHR203308); P. McKee was supported by NSF GRFP DGE 2139754; E.M.M. was supported by NWO (Nederlandse Organisatie voor Wetenschappelijk Onderzoek) Domain Social Sciences and Humanities (SSH) Starter Grant.; P. Papale was supported by NWO Veni VI.Veni.222.217; N.N.N.v.D. was supported by NWO Vici grant no.181.029; A. Gasiorowska was supported by National Science Center, 2021/41/B/HS6/03104; R.L.-B. was supported by Next Generation EU; S. Massoni was supported by Program FUTURE LEADER of Lorraine Université d’Excellence within the program Investissements Avenir (ANR-15-IDEX-04-LUE) operated by the French National Research Agency; F.D. and J. Lukavský were supported by RVO 68081740; J. Pfänder was supported by SCALUP ANR grant ANR-21-CE28-0016-01; M. Schaerer was supported by Singapore Ministry of Education Tier 1 Grant (MSS22B011) and Tier 2 Grant (MM22B03); I. Ropovik was supported by Slovak Research and Development Agency under the Contract no. APVV-23-0421; NPO Systemic Risk Institute (LX22NPO510); S.T. Schwartz was supported by Stanford Center for Mind, Brain, Computation and Technology; M.M.A. was supported by Stone Program in Wealth Distribution, Inequality, and Social Policy at Harvard University; P. Pärnamets was supported by Swedish Research Council 2020-02584; A.N.M. was supported by Swiss National Science Foundation grant PZ00P1_201956; V. Chirkov was supported by The Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC 2002/1 “Science of Intelligence” – project number 390523135; A. Boldt was supported by The Wellcome Trust 206480/Z/17/Z; B.P. was supported by The article was produced with financial support of the European Union under the REFRESH – Research Excellence For REgion Sustainability and High-tech Industries project number CZ.10.03.01/00/22_003/0000048 via the Operational Programme Just Transition.; F.M. was supported by UKRI BBSRC (BB/X008428/1) and NIHR Leeds Biomedical Research Centre (NIHR203331); G. Corlazzoli was supported by Université Libre de Bruxelles (Mini-ARC); Y.A.W. was supported by a Social Sciences and Humanities Research Council (SSHRC) Insight Development Grant (430-2022-00087); P. Präg was supported by a grant of the French National Research Agency ANR, ‘Investissements d’Avenir’ (LabEx Ecodec/ANR-11-LABX-0047).; S.R.S. was supported by an Interdisciplinary Exploratory Synergy grant from the Novo Nordisk foundation (NNF: NNF20OC0064869); J.F.G.M. was supported by an NSF SBE Postdoctoral Research Fellowship (award number 2104629); Q.S. was supported by grant 72371165, 71971199 and 71942004 from the National Natural Science Foundation of China.; M.R.S. was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) - GRK 2277 - project number 310365261; D.T. was supported by the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 101023805; R.R.-C. was supported by the European Union’s Horizon 2020 research and innovation program under the Marie Skłodowska-Curie Actions—Global Postdoctoral Fellowship (project Nº 101149355).; M. Kowal was supported by the Foundation for Polish Science (FNP) START scholarship; F.C. was supported by the National Natural Science Foundation of China [Grants 72173113 and 72322009].; A.M.A. was supported by the OTKA FK 146604 research grant; A. Antonietti was supported by the Project “EBRAINS-Italy (European Brain ReseArch INfrastructureS-Italy)” granted by European Union – NextGenerationEU adopted by the Italian Ministry of University and Research, CUP B51E22000150006.; A. Plachti was supported by the Region Hovedstadens Forskningsfond til Sundhedsforskning (grant agreement number A7118); H.F.S. was supported by the Research Council of Norway through its Centres of Excellence funding scheme (#262700); P. Kačmár was supported by the Slovak Research and Development Agency under the Contract no. APVV-22-0458 and APVV-23-0548; A. Arenas was supported by the project Proyectos de Generación de Conocimiento de la Agencia Estatal de Investigación PID2023-150898OB-I00; C. Koba was supported by the project of the Minister of Science and Higher Education "Support for the activity of Centers of Excellence established in Poland under Horizon 2020" on the basis of the contract number MEiN/2023/DIR/379, and the European Union’s Horizon 2020 research and innovation programme under grant agreement No 857533, and by Sano project carried out within the International Research Agendas programme of the Foundation for Polish Science, co-financed by the European Union under the European Regional Development Fund.

# Contributions

__Conceptualization:__ B.A., B. Szaszi, F. Holzmeister, G. Nilsonne, S. Hoffmann, L.K., Z.A.T., E.-J.W., T.E., and B.A.N.

__Data curation:__ H.T.C., M. Kovacs, M.A.V., and F. Venczel

__Formal analysis:__ B.A., B. Szaszi, H.T.C., M. Kovacs, H.S.-K., F. Holzmeister, G. Nilsonne, S. Hoffmann, Y.A., C.L.A., O.A.A., M. Adamkovic, T. Adamovich, K.A., A.M.A., A.S.A.-B., A.H.A.-H., C.J.A., P.J.A., T. Alsalti, M. Altman, S. Alzahawi, E. Ambrosini, S. Anafinova, R.A., M. Angerer, A.A.-B., A. Antonietti, J.A., A. Arenas, M.M.A., F.A., M. Bachl, B. Bago, Š.B., B.J.B., E. Balayan, C.L.B., B. Banai, K.B., F.B., E. Baskin, N. Bault, C.W.B., Q.H.B., M. Behnke, T.B., S.B., A. Bernard, U.B., P.A.B., A. Boldt, C.B.-R., A. Bouyamourn, O.B., L.B., J. Breuer, R.B., H.B., E. Buchanan, J. Buckenmaier, J. Buckley, J. Buczny, M. Burghart, B.H.B., N. Byrd, V. Cafarelli, P.C., T.C., K.C., A.M.C., G. Cepaluni, E.C., J.J.C., C.-c.C., X.C., S.S.C., F.C., H.C., V. Chirkov, D.C., B.C., S.G.C., C. Cohen, J. Collins, S.W.C., G. Corlazzoli, J. Cummins, C. Czymara, J.D.h., A.D.R., A.M.B.D., C.P.D., M.V.D., F.D.K., J.R.d.L., T.R.d.V., R.D., F.D., E.E.D., M.D., V.D., S.D.-S., S.D., L.D., J.D., A.R.D., H.D., J.E.E., A.E., E. Efendic, J.E., M.M.E., M.E., E. Estrada, L.E., T.R.E., A.F., E.M.F., L.F., F.M.F., J.L. Fiechter, M. Fišar, P.E.F.-K., M. Folwarczny, J.L. Fossum, V.R.F., R.F., D.F., J.F., A.C.F., J.D.G., L.C.G., C.M.G., B.G., S.M.G., A. Gasiorowska, B.G.P., R.G., A. Geminiani, D. Geraldes, C. Giani, E.G., V.G., T.G., A. Godefroidt, B.G.-B., A. Goreis, M.K.G., L.G.-V., M.G., D. Grigoryev, S.G., D.J.G., J.F.G.M., C. Guichet, L.G., H.H., A.C.H., S. Hafenbrädl, C. Häffner, F. Hagemeister, M. Haigh, N. Hajdu, N. Hajimoladarvish, J.D.H., M. Hamjediers, R.M.H., M. Harma, N.R.H., Á.D.H., R.H.H., A.H., Ø.H., D.H., T. Heyman, J. Hicks, J. Hogeveen, J. Höpler, S.D.H., C. Huber, C. Hughes, T. Hummler, K.H., M.I., T.I., O.I., K.I., I.R.J., A. Jahn, M. Jain, A. Jakubow, D.J., J.J., M. Jekel, F.J., W.J.-L., R.J., A. Jones, S.J., P. Kačmár, C. Kaiser, Y. Kalaycı, J. Kantorowicz, A. Karabulut, J. Karch, H.K.-R., J.A.K., A. Kažemekaitytė, A. Kazlou, Z.K., J. Kim, B.K.-D., K.N.K., J.W.K., C. Koba, M. Kołczyńska, P. Kolias, M. Korbmacher, Ž.K., M. Kowal, A. Kretzschmar, V.K., A.-M.K., M. Kubsch, Y. Kunisato, D. Lacko, J.R.L., M. Lange, H.L., D. Lee, S. Lee, E.P.L.J., D. Lempert, A. Leo, E.L., J.M.L., P. Li, J. Lin, L.L., D. Lisovoj, M. Liu, S. Liu, T.L., S.L.I., P. Lodder, R.L.-B., R.L.-N., K.L., N.M.L., A. Lovakov, J.G.L., J. Ludwig, F.L., J. Lukavský, C.L., X.L., E.M., M. Máčel, M.L.M., C.R.M., A. Mädebach, J.M.-K., D.J.M., I.M., T.M., M.M. Marini, D.M.F., M. Martínez, M. Martinoli, A. Masiliunas, S. Massoni, K.C.M., S. Mayer, D. Mayer, M. Mayer, E.M.M., I.M.M., A.L.M., M.M. McIntyre, P. McKee, A.N.M., P.F.M., H.M., C. Merkle, R.M., M.P.M., P. Michaelsen, G.M., W.M., P. Millroth, K.G.M., M. Misiak, Y.L.M., D. Moreau, C. Moreh, C. Morvinski, F.M., T.N., C.N., E.N., G. Navarrete, S.N., R.N., M.N., E.N.-S., Y.A.N., G. Niso, B.N., M.O., K. Ong, A.I.O., C.O., K. Otten, S. Pandey, M. Pantazi, P. Papale, P. Pärnamets, S. Pauer, Y.G.P., S. Pawel, J.E.P., H.K.P., A. Peez, F.P., B.D.P., B.P., J. Petter, J. Pfänder, G.P., J. Phillips, M.T.P., A. Pirrone, I.L.P., A. Plachti, I.S.P., M. Ploner, M.M.H.P., S. Porcher, P. Präg, A.A.Y.P., J. Pugel, R.P., M. Püski, S. Radkani, L. Raes, I. Rafaï, K. Raiber, S. Rathje, M. Reshetnikov, C.J.R., J.P.R., K. Rigaud, C.R., S. Rivera, O.R., R.R.-C., I. Ropovik, L. Röseler, R.M.R., A.R., F.F.R., F.R., M. Rusconi, I. Russo, A.H.J.S., J. Salamon, M. Samahita, A. Sanaei, A. Sangchooli, M. Scandola, H.S., M. Schaerer, E. Schares, H.T.S., X.S., K.S., M.R.S., J.M.S., A.-L.S., B. Schuetze, D.H.S., L. Schulze, S.T. Schwartz, N.S., B. Scoggins, Y.S., R.S., S.T. Shaw, J. Shaw, Q.S., C.S., M. Sladekova, A. Somo, A. Sondhi, B. Sonmez, L. Spantig, M. Speekenbrink, A. Stamos, L. Stasielowicz, L.C.S., S.R.S., A.H.S., C.N.H.S., J.W.S., H.F.S., J. Sundquist, V.S., S.D.S., P.S., R.D.S.-C., E. Szumowska, A. Tacconelli, E. Talbert, J.P.T., J.N.T., M.T., E. Toffalini, A. Tomašević, S.T., L. Torkkeli, L. Tozzi, J.T., A. Trinidad, D.T., K.T., M.U., K.U., J.V.A., K.v.L., D.v.R., R.v.V., L.A.V., M.V., P.V., A.V., E.V., F. Votta, A. Waldendorf, M.J.W., M.B.W., H.W., K.W., I.W., Y.A.W., M. Weinmann, M. Weiss, C.W., A. Wichman, B.J.W., D.W., T.K.A.W., M. Woźniak, J.D.W., W.Y., J.N.W., T.Y., S.K.Y., K.S.L.Y., M.Z., R.A.Z., X.Z., Z.Z., S.Z., C. Ziller, D.Z., C. Zogmaister, and R.i.Z.

__Funding acquisition:__ B.A., B. Szaszi, T.E., and B.A.N.

__Investigation:__ L.A.V.

__Methodology:__ B.A., B. Szaszi, H.T.C., M. Kovacs, H.S.-K., F. Holzmeister, G. Nilsonne, S. Hoffmann, L.K., Z.A.T., C.J.A., J.A.B., R.B.-N., A.D., M.A.G., M.H.K., R.A.P., A. Sarafoglou, T.S., D.R.S., E.L.U., M.A.L.M.v.A., N.N.N.v.D., D.v.R., E.-J.W., T.E., and B.A.N.

__Project administration:__ B.A., B. Szaszi, M. Kovacs, L.K., Z.A.T., N.F., and T.E.

__Resources:__ B.A., B. Szaszi, H.T.C., F. Holzmeister, L.K., L.D., N.F., T.E., and B.A.N.

__Software:__ M. Kovacs

__Supervision:__ B.A., B. Szaszi, H.T.C., F. Holzmeister, G. Nilsonne, S. Hoffmann, C.J.A., J.A.B., R.B.-N., A.D., M.A.G., M.H.K., A.N., R.A.P., A. Sarafoglou, T.S., D.R.S., E.L.U., M.A.L.M.v.A., N.N.N.v.D., D.v.R., E.-J.W., T.E., and B.A.N.

__Validation:__ S. Anafinova, X.C., and D.D.

__Visualization:__ B.A., B. Szaszi, and M. Kovacs

__Writing - original draft:__ B.A., B. Szaszi, H.T.C., M. Kovacs, and L.K.

__Writing - review & editing:__ B.A., B. Szaszi, H.T.C., M. Kovacs, H.S.-K., F. Holzmeister, G. Nilsonne, S. Hoffmann, L.K., Z.A.T., Y.A., C.L.A., O.A.A., E. Acem, M. Adamkovic, T. Adamovich, K.A., L.A., A.M.A., A.S.A.-B., A.H.A.-H., C.J.A., P.J.A., T. Alsalti, M. Altman, S. Alzahawi, E. Ambrosini, S. Anafinova, R.A., M. Angerer, A.A.-B., A. Antonietti, J.A., A. Arenas, M.M.A., F.A., M. Bachl, B. Bago, Š.B., B.J.B., E. Balayan, C.L.B., B. Banai, K.B., F.B., E. Baskin, J.A.B., N. Bault, C.W.B., Q.H.B., M. Behnke, T.B., S.B., A. Bernard, U.B., P.A.B., A. Boldt, C.B.-R., R.B.-N., A. Bouyamourn, O.B., L.B., J. Breuer, R.B., H.B., E. Buchanan, J. Buckenmaier, J. Buckley, J. Buczny, M. Burghart, B.H.B., N. Byrd, V. Cafarelli, P.C., T.C., K.C., A.M.C., G. Cepaluni, E.C., J.J.C., C.-c.C., X.C., S.S.C., F.C., H.C., V. Chirkov, D.C., B.C., S.G.C., C. Cohen, J. Collins, S.W.C., G. Corlazzoli, J. Cummins, C. Czymara, J.D.h., A.D.R., A.M.B.D., C.P.D., M.V.D., F.D.K., J.R.d.L., T.R.d.V., R.D., F.D., E.E.D., M.D., D.D., V.D., S.D.-S., S.D., L.D., J.D., A.R.D., A.D., H.D., J.E.E., A.E., E. Efendic, J.E., M.M.E., M.E., E. Estrada, L.E., T.R.E., A.F., E.M.F., L.F., F.M.F., J.L. Fiechter, M. Fišar, P.E.F.-K., M. Folwarczny, J.L. Fossum, V.R.F., R.F., D.F., J.F., A.C.F., J.D.G., L.C.G., C.M.G., B.G., S.M.G., A. Gasiorowska, B.G.P., R.G., A. Geminiani, D. Geraldes, M.A.G., C. Giani, E.G., V.G., T.G., A. Godefroidt, B.G.-B., A. Goreis, M.K.G., L.G.-V., M.G., D. Grigoryev, S.G., D.J.G., J.F.G.M., C. Guichet, L.G., H.H., A.C.H., S. Hafenbrädl, C. Häffner, F. Hagemeister, M. Haigh, N. Hajdu, N. Hajimoladarvish, J.D.H., M. Hamjediers, R.M.H., M. Harma, N.R.H., Á.D.H., R.H.H., A.H., Ø.H., D.H., T. Heyman, J. Hicks, J. Hogeveen, J. Höpler, S.D.H., C. Huber, C. Hughes, T. Hummler, K.H., M.I., T.I., O.I., K.I., I.R.J., A. Jahn, M. Jain, A. Jakubow, D.J., J.J., M. Jekel, F.J., W.J.-L., R.J., A. Jones, S.J., P. Kačmár, C. Kaiser, Y. Kalaycı, J. Kantorowicz, A. Karabulut, J. Karch, H.K.-R., J.A.K., A. Kažemekaitytė, A. Kazlou, Z.K., J. Kim, M.H.K., B.K.-D., K.N.K., J.W.K., C. Koba, M. Kołczyńska, P. Kolias, M. Korbmacher, Ž.K., M. Kowal, A. Kretzschmar, V.K., A.-M.K., M. Kubsch, Y. Kunisato, D. Lacko, J.R.L., M. Lange, H.L., D. Lee, S. Lee, E.P.L.J., D. Lempert, A. Leo, E.L., J.M.L., P. Li, J. Lin, L.L., D. Lisovoj, M. Liu, S. Liu, T.L., S.L.I., P. Lodder, R.L.-B., R.L.-N., K.L., N.M.L., A. Lovakov, J.G.L., J. Ludwig, F.L., J. Lukavský, C.L., X.L., E.M., M. Máčel, M.L.M., C.R.M., A. Mädebach, J.M.-K., D.J.M., I.M., T.M., M.M. Marini, D.M.F., M. Martínez, M. Martinoli, A. Masiliunas, S. Massoni, K.C.M., S. Mayer, D. Mayer, M. Mayer, E.M.M., I.M.M., A.L.M., M.M. McIntyre, P. McKee, A.N.M., P.F.M., H.M., C. Merkle, R.M., M.P.M., P. Michaelsen, G.M., W.M., P. Millroth, K.G.M., M. Misiak, Y.L.M., D. Moreau, C. Moreh, C. Morvinski, F.M., T.N., C.N., E.N., G. Navarrete, S.N., A.N., R.N., M.N., E.N.-S., Y.A.N., G. Niso, B.N., M.O., K. Ong, A.I.O., C.O., K. Otten, S. Pandey, M. Pantazi, P. Papale, P. Pärnamets, S. Pauer, Y.G.P., S. Pawel, J.E.P., H.K.P., A. Peez, F.P., B.D.P., B.P., J. Petter, J. Pfänder, G.P., J. Phillips, M.T.P., A. Pirrone, I.L.P., A. Plachti, I.S.P., M. Ploner, R.A.P., M.M.H.P., S. Porcher, P. Präg, A.A.Y.P., J. Pugel, R.P., M. Püski, S. Radkani, L. Raes, I. Rafaï, K. Raiber, S. Rathje, M. Reshetnikov, C.J.R., J.P.R., K. Rigaud, C.R., S. Rivera, O.R., R.R.-C., I. Ropovik, L. Röseler, R.M.R., A.R., F.F.R., F.R., M. Rusconi, I. Russo, A.H.J.S., J. Salamon, M. Samahita, A. Sanaei, A. Sangchooli, A. Sarafoglou, M. Scandola, H.S., M. Schaerer, E. Schares, H.T.S., X.S., K.S., T.S., M.R.S., J.M.S., A.-L.S., B. Schuetze, D.H.S., L. Schulze, S.T. Schwartz, N.S., B. Scoggins, Y.S., R.S., D.R.S., S.T. Shaw, J. Shaw, Q.S., C.S., M. Sladekova, A. Somo, A. Sondhi, B. Sonmez, L. Spantig, M. Speekenbrink, A. Stamos, L. Stasielowicz, L.C.S., S.R.S., A.H.S., C.N.H.S., J.W.S., H.F.S., J. Sundquist, V.S., S.D.S., P.S., R.D.S.-C., E. Szumowska, A. Tacconelli, E. Talbert, J.P.T., J.N.T., M.T., E. Toffalini, A. Tomašević, S.T., L. Torkkeli, L. Tozzi, J.T., A. Trinidad, D.T., K.T., M.U., E.L.U., K.U., J.V.A., M.A.L.M.v.A., N.N.N.v.D., K.v.L., D.v.R., R.v.V., M.A.V., L.A.V., F. Venczel, M.V., P.V., A.V., E.V., F. Votta, E.-J.W., A. Waldendorf, M.J.W., M.B.W., H.W., K.W., I.W., Y.A.W., M. Weinmann, M. Weiss, C.W., A. Wichman, B.J.W., D.W., T.K.A.W., M. Woźniak, J.D.W., W.Y., J.N.W., T.Y., S.K.Y., K.S.L.Y., M.Z., R.A.Z., X.Z., Z.Z., S.Z., C. Ziller, D.Z., C. Zogmaister, R.i.Z., N.F., T.E., and B.A.N.

# Corresponding Authors

Correspondence and requests for materials should be addressed to Balazs Aczel and Barnabas Szaszi; E-mail: balazs.aczel@gmail.com and szaszi.barnabas@gmail.com.

# Ethics declarations
## Competing interests

The authors declare no competing interests.

# SUPPLEMENTARY INFORMATION

```{r, echo=FALSE}
tribble(
  ~ Content,
  ~ Location,
  "Project preregistrations",
  "https://osf.io/4ev9t, https://osf.io/y4evp",
  "Deviations from preregistration",
  "https://osf.io/ys9uz",
  "Supplementary information",
  "https://osf.io/2h4zb",
  "Project OSF Repository",
  "https://osf.io/q5h2c/",
  "Project GitHub Repository",
  "https://github.com/marton-balazs-kovacs/multi100"
) |> 
  gt() |> 
  cols_label(Content = "Content", Location = "Location") |> 
  fmt_markdown(columns = Location)
```

# Extended data figures

__Extended Data Fig. 1 | Additional statistical results requested by the reviewers.__

__a,__ Proportion of same effect, no effect/inconclusive results, and conclusions in the opposite direction of the original studies, by matches and nonmatches between the discipline of the re-analyst and the original study. __b,__ The distribution of the heterogeneity ratios calculated between the effect size variability over the re-analyses and the sampling variability of the original study effect-size estimates.

__Extended Data Fig. 2 | Descriptive statistics of the peer evaluators.__

__a,__ The peer evaluators’ years of experience with data analysis. When a peer evaluator submitted more than one evaluation and a year passed between the responses, we kept only their first response. __b,__ The regularity with which peer evaluators perform data analysis. __c,__ The peer evaluators’ self-rated level of expertise in data analysis. When a peer evaluator submitted more than one re-analysis, we kept only their first response.

__Extended Data Fig. 3 | Descriptive statistics of the analysts and the analyses.__

__a,__ The distribution of the analysts’ age. When an analyst submitted additional re-analyses with a higher reported age, we kept only their age at the time of their first submission. Moreover, one analyst is not represented in the figure because they did not disclose their age. __b,__ The analysts’ country of residence. When an analyst submitted more than one re-analysis, and they moved between the submissions, we only kept their first response. __c,__ The analysts’ years of experience with data analysis.  We only kept their first response when an analyst submitted additional re-analyses with a higher reported age. __d,__ The regularity of the analysts' data analysis. __e,__ The analysts’ self-rated level of expertise in data analysis.  When an analyst submitted more than one re-analysis, we only kept their first response. __f,__ The software the analysts used for their re-analysis tasks. In case an analyst completed multiple re-analyses or reported using multiple software applications, we kept all their responses for this figure. The figure displays only software applications used by more than 1% of the analysts.

__Extended Data Fig. 4 | Robustness of the statistical results.__ 

__a,__ Robustness of the statistical results with different widths (+/-[0.01-0.20] Cohen’s d) of the tolerance region. __b,__ Robustness of the statistical results with different percentages (5-20%) of Cohen’s d as a tolerance region. Calculations on the study and re-analysis levels are shown in different lines.

__Extended Data Fig. 5 | Overview of the project procedures__

The figure depicts the procedural workflow of the selection of the studies (1); the selection of the re-analysts (2); and the re-analysis process for each study (3).

# Abstract
The same dataset can be analysed in different justifiable ways to answer the same research question, potentially challenging the robustness of empirical science1–3. In this crowd initiative, we investigated the degree to which research findings in the social and behavioural sciences are contingent on analysts’ choices. We examined a stratified random sample of 100 studies published between 2009 and 2018, where for one claim per study, at least five re-analysts independently re-analysed the original data. The statistical appropriateness of the re-analyses was assessed in peer evaluations, and the robustness indicators were inspected along a range of research characteristics and study designs. We found that `r analysis_level_within_tolerance_region_5`of the independent re-analyses yielded the same result (within a tolerance region of +/- 0.05 Cohen’s d) as the original report; with a four times broader tolerance region, this indicator rose to `r analysis_level_within_tolerance_region_20`%. Regarding the conclusions drawn, `r conclusion_robust_100_percentage`% of analyses were reported to arrive at the same conclusion as in the original investigation; 24% to no effects/inconclusive result, and 2% to the opposite effect as in the original investigation. This exploratory study suggests that the common single-path analyses in social and behavioural research should not simply be assumed to be robust to alternative analyses[@wagenmakers_one_2022]. Therefore, we recommend the development and use of practices to explore and communicate this neglected source of uncertainty. 

__Keywords__ analytical variability, crowdsourcing science, data analysis, research credibility, robustness, scientific transparency