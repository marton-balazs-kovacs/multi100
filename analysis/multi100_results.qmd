---
title: "Results"
format: docx
editor: source
editor_options: 
  chunk_output_type: console
---

```{r include=FALSE, message=FALSE, warning=FALSE}
# Load packages
library(tidyverse)
library(ggrain)
library(maps)
library(mapdata)
library(countrycode)
library(raster)
library(gt)
library(patchwork)

# Load custom functions
source(here::here("R/utils.R"))

# Read processed data files
processed <- readr::read_csv(here::here("data/processed/multi100_processed_data.csv"))

peer_eval <- readr::read_csv(here::here("data/processed/multi100_peer-eval_processed_data.csv"))

peer_eval_not_reviewed <- readr::read_csv(here::here("data/processed/multi100_peer-eval-not-reviewed_processed_data.csv"))

peer_eval_review <- readr::read_csv(here::here("data/processed/multi100_peer-eval-review_processed_data.csv"))

## Transform datafiles for analysis 
# Add number of evaluations per analysis
peer_eval <-
  peer_eval |>
  dplyr::group_by(paper_id, analyst_id) |>
  dplyr::mutate(n_peer_evals = dplyr::n()) |>
  dplyr::ungroup()

all_people <- readr::read_csv(here::here("data/processed/multi100_all-people_processed_data.csv"))

# Transforming the timestamp to date type from character
processed <-
  processed |>
  dplyr::mutate(
    task1_timestamp = lubridate::ymd_hms(task1_timestamp))

# Table of long paper ids and simplified paper ids
processed |> 
  dplyr::distinct(simplified_paper_id, paper_id) |> 
  dplyr::arrange(simplified_paper_id)
```

```{r include=FALSE}
# Check if the analyst_id's are always unique to one person
processed |>
  dplyr::distinct(first_name, last_name, analyst_id) |>
  dplyr::group_by(first_name, last_name) |>
  dplyr::mutate(n_analyst_id = dplyr::n()) |>
  dplyr::arrange(dplyr::desc(n_analyst_id))

# Check if there are peer evaluators who evaluated an analysis for the same paper they analyzed
peer_eval |> 
  dplyr::select(evaluator_id, paper_id) |> 
  rename(evaluator_paper_id = paper_id) |>
  left_join(distinct(processed, paper_id, analyst_id), by = c("evaluator_id" = "analyst_id"), relationship = "many-to-many") |> 
  mutate(match = case_when(
    paper_id == evaluator_paper_id ~ TRUE,
    paper_id != evaluator_paper_id ~ FALSE
  )) |> 
  filter(match)
```

## General descriptives

```{r include=FALSE}
analyst_signed_up <-
  all_people |>
  dplyr::mutate(first_name = tolower(first_name),
                last_name = tolower(last_name)) |>
  dplyr::distinct(first_name, last_name, .keep_all = T) |>
  dplyr::filter(disclosure_agreement == "I agree")

analyst_submitted <-
  processed |>
  dplyr::distinct(analyst_id) |>
  nrow()
```

As a response to our recruitment call, `r nrow(analyst_signed_up)` researchers signed up to participate in our study. Out of these volunteers, `r analyst_submitted` signed up to analyse at least one dataset and submitted their work by the deadline or an extended deadline.

```{r include=FALSE}
n_analysis <-
  processed |> 
  nrow()
```

Throughout the project, `r n_analysis` re-analyses have been submitted. This number is higher than the number of co-analysts as some co-analysts volunteered to analyse more than one dataset.

```{r include=FALSE}
n_failed_peer <-
  processed |>
  dplyr::filter(!peer_eval_pass | !incomplete_response_pass) |>
  nrow()
```

Out of the submitted analyses `r nrow(dplyr::filter(processed, !peer_eval_pass))` of them was omitted from the summary analysis as its analysis failed the peer evaluation and an additional `r nrow(dplyr::filter(processed, !incomplete_response_pass))` analyses were excluded due to incomplete responses.

```{r include=FALSE}
# Excluding analyst who failed the peer evaluation from the rest of the analysis
processed <-
  processed |> 
  dplyr::filter(peer_eval_pass & incomplete_response_pass)
```

```{r include=FALSE}
final_n_analyst <-
  processed |> 
  dplyr::distinct(analyst_id) |> 
  nrow()
```

As a result, we ended up with `r nrow(processed)` re-analyses, submitted by `r final_n_analyst` co-analysts.

Although we invited more than 5 co-analysts to each of the 100 studies, due to drop-outs and peer evaluation exclusions the final number of completed analyses ranged between `r min(dplyr::count(processed, paper_id)$n)` and `r max(dplyr::count(processed, paper_id)$n)`. Table 1 shows the distribution of of the number of analyses for individual studies.

**Table 1. The Distribution of the Number of Analyses for Studies**

```{r echo=FALSE, message=FALSE}
processed |>
  dplyr::count(paper_id) |> 
  dplyr::count(n, name = "N") |>
  dplyr::rename(`Number of Completed Analyses` = n,
                `Number of Studies` = N) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

## Basic demographics of the co-analysts

```{r include=FALSE}
# Checking if analysts consistently reported their current position
check_diff_response(processed, analyst_id, current_position_grouped)

position <-
  processed |>
  dplyr::select(analyst_id,
                paper_id,
                current_position_grouped,
                task1_timestamp) |>
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) |>
  count(current_position_grouped) |>
  rename(position = current_position_grouped) |>
  mutate(position = factor(
    position,
    levels = c(
      "Professor",
      "Associate Professor",
      "Assistant Professor",
      "Post-Doc Researcher",
      "Doctoral Student",
      "Other academic/research position"
    )
  ))
```

Out of all the co-analysts who submitted their work by the deadline, there were `r dplyr::filter(position, position == "Professor") |> dplyr::pull(n)` professors, `r dplyr::filter(position, position == "Associate Professor") |> dplyr::pull(n)` associate professors, `r filter(position, position == "Assistant Professor") |> dplyr::pull(n)` assistant professors, `r dplyr::filter(position, position == "Post-Doc Researcher") |> dplyr::pull(n)` post-doctoral researchers, `r dplyr::filter(position, position == "Doctoral Student") |> dplyr::pull(n)` doctoral students, `r dplyr::filter(position, position == "Other academic/research position") |> dplyr::pull(n)` from other academic/research positions.

```{r include=FALSE}
check_diff_response(processed, analyst_id, gender)

gender <-
  processed |>
  dplyr::select(analyst_id, paper_id, gender, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(gender)
```

The gender distribution of the co-analysts is as follows: `r dplyr::filter(gender, gender == "Female") |> dplyr::pull(n)` female, `r dplyr::filter(gender, gender == "Male") |> dplyr::pull(n)` male, `r dplyr::filter(gender, gender == "Non-binary") |> dplyr::pull(n)` other, and `r dplyr::filter(gender, gender == "Prefer not to say") |> dplyr::pull(n)` didn't want to respond to this question.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = age)

age <-
  processed |>
  dplyr::select(analyst_id, paper_id, age, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  # Filter erroneous data
  dplyr::filter(age != "00") |> 
  dplyr::mutate(age = as.numeric(age))
  
age_group <-
  age |> 
  dplyr::mutate(
    age_group = dplyr::case_when(
      age <= 39 ~ "young",
      age >= 40 | age <= 59 ~ "middle",
      age >= 60 ~ "old",
      TRUE ~ NA_character_
    ),
    age_group = factor(age_group, levels = c("young", "middle", "old"))
    ) |> 
  dplyr::count(age_group) |> 
  tidyr::complete(age_group, fill = list(n = 0))
```

The age distribution of the co-analysts is depicted in @fig-age-plot. `r dplyr::filter(age_group, age_group == "young") |> dplyr::pull(n)` young adults (-39 years); `r dplyr::filter(age_group, age_group == "middle") |> dplyr::pull(n)` middle-aged adults (40-59 years); and no old adults (60- years).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-age-plot
#| fig-cap: "The figure shows the distribution of the analysts' age. When an analyst submitted more than one re-analysis with more than a year apart, we kept only their age at the time of their first submission. Moreover, one analyst is excluded because they did not disclose their age."
age_plot <- 
  age |>
  ggplot2::ggplot() +
  ggplot2::aes(x = age) +
  ggplot2::geom_histogram(binwidth = 1) +
  ggplot2::scale_y_continuous(expand = c(0, 0), limits = c(0, 40)) +
  ggplot2::scale_x_continuous(
    # limits = c(20, 55),
    # breaks = c(20, 30, 40, 50, 60),
    # labels = c("20", "30", "40", "50", "60")
  ) +
  ggplot2::labs(x = "Age (years)",
       y = "Number of co-analysts") +
  ggplot2::theme(
    panel.grid = ggplot2::element_blank(),
    panel.background = ggplot2::element_blank(),
    axis.line = ggplot2::element_line()
  )

ggplot2::ggsave(here::here("figures/demographic_age_plot.jpg"), age_plot, dpi = 300)

age_plot
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = education_level)

education <-
  processed |>
  dplyr::select(analyst_id, paper_id, education_level, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(education_level) |> 
  dplyr::rename(education = education_level)
```

Regarding the highest level of education, `r dplyr::filter(education, education == "High-school diploma or equivalent") |> dplyr::pull(n)` co-analyst reported High-school diploma or equivalent, `r dplyr::filter(education, education == "Bachelor's degree or equivalent") |> dplyr::pull(n)` co-analysts had Bachelor's degree or equivalent, `r dplyr::filter(education, education == "Master's degree or equivalent") |> dplyr::pull(n)` Master's degree or equivalent, `r dplyr::filter(education, education == "Doctoral degree or equivalent") |> dplyr::pull(n)` had Doctoral degree or equivalent. In case the analysts completed more than one re-analysis and they advanced in their studies by the time of their second analysis, we kept only their first response for this comparison.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = country_of_residence)

country_data <- raster::ccodes()

country <- 
  processed |>
  dplyr::select(analyst_id, paper_id, country_of_residence, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(country_of_residence) |> 
  dplyr::rename(region = country_of_residence) |> 
  # Modify country names to fit the worldmap data
  dplyr::mutate(
    subregion = dplyr::case_when(
      region == "Hong Kong (China)" ~ "Hong Kong",
      TRUE ~ NA_character_
    ),
    region = dplyr::case_when(
      region == "Hong Kong (China)" ~ "China",
      region == "United States" ~ "USA",
      region == "United Kingdom" ~ "UK",
      TRUE ~ region
    ),
    continent = countrycode::countrycode(region, "country.name", "continent"),
    iso3_code = countrycode::countrycode(region, "country.name", "iso3c")
  ) |> 
  dplyr::left_join(dplyr::select(country_data, ISO3, UNREGION1), by = c("iso3_code" = "ISO3"))

continent <-
  country |> 
  dplyr::group_by(continent) |> 
  dplyr::summarise(N = sum(n))

region <-
  country |> 
  dplyr::group_by(UNREGION1) |> 
  dplyr::summarise(N = sum(n))
```

The country of residence of the co-analysts is shown on the map on @fig-country-plot. Regarding the continents, `r dplyr::filter(continent, continent == "Africa") |> dplyr::pull(N)` co-analyst was from Africa, `r dplyr::filter(continent, continent == "Asia") |> dplyr::pull(N)` were from Asia, `r dplyr::filter(continent, continent == "Oceania") |> dplyr::pull(N)` from Oceania, `r dplyr::filter(continent, continent == "Europe") |> dplyr::pull(N)` from Europe, `r dplyr::filter(region, UNREGION1 == "Northern America") |> dplyr::pull(N)` from North America, `r dplyr::filter(region, UNREGION1 %in% c("Central America", "South America")) |> dplyr::summarise(sum(N)) |> dplyr::pull("sum(N)")` from South America.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-country-plot
#| fig-cap: "The figure shows the analysts' country of residence. When an analyst submitted more than one re-analysis and they moved between the submissions, we only kept their first response."

world_map <- 
  ggplot2::map_data("world") |> 
  dplyr::mutate(
    subregion = dplyr::case_when(
      subregion == "Hong Kong" ~ subregion,
      TRUE ~ NA_character_
    )
  )

country_map <- dplyr::left_join(world_map, country, by = c("region", "subregion"))

country_map_plot <- 
  country_map |> 
  ggplot2::ggplot() +
  ggplot2::aes(x = long, y = lat, group = group, fill = n) +
  ggplot2::geom_polygon(color = "white", linewidth = 0.2) +
  ggplot2::scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Number of\nanalyst") +
  ggplot2::theme_void() +
  ggplot2::theme(
    legend.title = element_text(size = 6),
    plot.margin = margin(t = 10, r = 20, b = 10, l = 10, unit = "pt")
  )

ggplot2::ggsave(here::here("figures/demographic_country_plot.jpg"), country_map_plot, dpi = 300)

country_map_plot
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = primary_discipline)

analyst_discipline <- 
  processed |>
  dplyr::select(analyst_id, paper_id, primary_discipline, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  calculate_percentage(response_var = primary_discipline) |>
  dplyr::rename(discipline = primary_discipline) |> 
  dplyr::arrange(dplyr::desc(dplyr::if_else(discipline == "Other", -Inf, percentage)))
```

We asked the co-analysts which discipline is the closest to their research area. The following Table 2 summarizes the distribution of their disciplinary orientation. Co-analysts from `r dplyr::slice(analyst_discipline, 1) |> dplyr::pull(discipline)` and `r dplyr::slice(analyst_discipline, 2) |> dplyr::pull(discipline)` disciplines participated in the highest ratio in this study.

**Table 2. The Distribution of Co-analysts’ Disciplinary Orientation**

```{r, echo=FALSE}
# tbl-discipline
# "Distribution of the Analysts' Primary Discipline"

analyst_discipline |>
  dplyr::select(discipline, n, percentage) |>
  dplyr::rename(Discipline = discipline,
                Count = n,
                Percentage = percentage) |>
  gt::gt() |>
  tab_style(style = gt::cell_text(weight = "bold"),
            locations = gt::cells_column_labels()) |>
  gt::tab_footnote(
    "Note: Whenever the respondents provided more than one field we only kept their first responses."
  )
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = years_of_experience)

analyst_experience_years_data <-
  processed |>
  dplyr::select(analyst_id, paper_id, years_of_experience, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  # Dropped because of faulty response
  dplyr::filter(analyst_id != "RTX71")
```

The distribution of the years of experience with data analysis is depicted on @fig-experience-years-plot. The median time of experience with data analysis was `r median(analyst_experience_years_data$years_of_experience)` years among our co-analysts.

```{r echo=FALSE, message=FALSE}
#| label: fig-experience-years-plot
#| fig-cap: "The figure shows the analysts' years of experience with data analysis. When an analyst submitted more than one re-analysis and a year passed between the responses we only kept their first response."
analyst_experience_years_plot <-
  analyst_experience_years_data |> 
  ggplot2::ggplot() +
  ggplot2::aes(x = years_of_experience) +
  ggplot2::geom_histogram() +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(x = "Years of experience with data analysis",
                y = "Number of co-analysts") +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggplot2::ggsave(here::here("figures/demographic_experience_years_plot.jpg"), analyst_experience_years_plot, dpi = 300)

analyst_experience_years_plot
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = analysis_frequency)

analysis_frequency_count <-
  processed |> 
  dplyr::select(analyst_id, paper_id, analysis_frequency, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(analysis_frequency)
```

We asked our co-analysts how regularly they perform data analysis. @fig-analysis-frequency shows that the most frequent category was `r dplyr::filter(analysis_frequency_count, n == max(n)) |> dplyr::pull(analysis_frequency)`.

```{r echo=FALSE, message=FALSE}
#| label: fig-analysis-frequency
#| fig-cap: "The figure shows how regularly the analysts perform data analysis."

# For this question we report the responses by analysis and not analyst 
analysis_frequency_plot <-
  analysis_frequency_count |>
  dplyr::mutate(
    analysis_frequency = dplyr::case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = forcats::fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) |>
  ggplot2::ggplot() +
  ggplot2::aes(x = analysis_frequency, y = n) +
  ggplot2::geom_bar(stat = "identity") +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(x = "Frequency of doing data analysis",
                y = "Number of co-analysts") +
  ggplot2::theme(
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(color = "black"),
    axis.text.x = ggplot2::element_text(size = 7)
  )

ggplot2::ggsave(here::here("figures/demographic_analysis_frequency_plot.jpg"), analysis_frequency_plot, dpi = 300)

analysis_frequency_plot
```

```{r include=FALSE}
expertise_self_rating_data <-
  processed |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |>
  dplyr::count(expertise_self_rating)
```

We also asked them how they rated their level of expertise in data analysis between Beginner (1) and Expert (10). The distribution on @fig-self-rating-plot shows that the most prevalent answer was `r dplyr::filter(expertise_self_rating_data, n == max(n)) |> dplyr::pull(expertise_self_rating)` .

```{r echo=FALSE, message=FALSE}
#| label: fig-self-rating-plot
#| fig-cap: "The figure shows the analysts' self-rated level of expertise in data analysis. When an analyst submitted more than one re-analysis we only kept their first response."

expertise_self_rating_plot <-
  expertise_self_rating_data |>
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = as.factor(expertise_self_rating),
    expertise_self_rating = fct_relevel(
      expertise_self_rating,
      c("1\n(Beginner)",
        as.character(2:9),
        "10\n(Expert)")
    )
  ) |>
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-rated expertise of data analysis",
       y = "Number of co-analysts") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_expertise_self_rating_plot.jpg"), expertise_self_rating_plot, dpi = 300)

expertise_self_rating_plot
```

```{r include=FALSE}
familiar_with_paper_data <-
  calculate_percentage(processed, familiar_with_paper)
```

In `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(percentage)` % (`r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(n)` out of `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(N)`) of the cases, the co-analysts were familiar with the paper that the provided dataset belongs to before beginning their work on the project.

```{r include=FALSE}
processed |> 
  count(communication_check)
```

All co-analyst reported that they have not communicated about the details of their analysis with other co-analysts working with the same dataset.

```{r include=FALSE}
software_data <-
  processed |> 
  dplyr::reframe(
    software = c(task1_software, task2_software),
    software = tolower(software),
  ) |> 
  separate_rows(software, sep = ",\\s*") |> 
  mutate(
    software = case_when(
      software == "ms excel" ~ "excel",
      software == "r markdown" ~ "rmarkdown",
      software == "process v4.0 by hayes for r" ~ "process v4.0",
      software == "jamovi 1.6.23.0" ~ "jamovi",
      software == "jamovi 2.3.9" ~ "jamovi",
      software == "text editor to look at the stata code of the original paper" ~ "text editor",
      software == "text editor to read the stata code in the replication materials" ~ "text editor",
      TRUE ~ software
    )
  ) |> 
  calculate_percentage(software) |> 
  arrange(desc(n)) |> 
  mutate(
    software = case_when(
      software %in% c("r", "stata", "spss", "jasp") ~ toupper(software),
      TRUE ~ stringr::str_to_title(software)
    )
  )
```

We asked the co-analysts what programming language/software/tool they used in their data analysis during Task 1 and Task 2. The following figure indicates that `r slice(software_data, 1) |> pull(software)` (`r slice(software_data, 1) |> pull(percentage)`%), `r slice(software_data, 2) |> pull(software)` (`r slice(software_data, 2) |> pull(percentage)`%), and `r slice(software_data, 3) |> pull(software)` (`r slice(software_data, 3) |> pull(percentage)`%) were the most popular responses. @fig-software shows the distribution of these responses.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-software
#| fig-cap: "The figure shows which software the analysts used for their re-analysis tasks. In case an analyst completed multiple re-analyses or reported the use of multiple software we kept all their responses for this figure. The figure shows only software that was used by more than 1% of the analysts."

software_plot <-
  software_data |>
  dplyr::filter(percentage > 1) |>
  ggplot() +
  aes(x = reorder(software, -percentage),
      y = percentage) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0),
                     labels = scales::percent_format(scale = 1)) +
  labs(y = "Percentage of co-analysts",
       x = "Software") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_software_plot.jpg"), software_plot, dpi = 300)

software_plot
```

## Descriptives of the statistical analyses

A difference in Task 2 compared to Task 1 was that the co-analysts received some constraints for their analysis to make their result comparable to a single result in the original study (see Methods for more details).

```{r include=FALSE}
p_value_or_bayes_data <-
  calculate_percentage(processed, p_value_or_bayes)
```

In Task 2, when we asked the co-analysts to present one main statistical result, in `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(percentage)`% of the analyses (`r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(n)` out of `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(N)`), conclusion was based on the p-value. Bayes Factor was used in `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(percentage)`% of the cases (`r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(n)` out of `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(N)`).

```{r include=FALSE}
additional_calculations_data <-
  calculate_percentage(processed, additional_calculations)
```

For `r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(percentage)` % (`r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(N)`) of the analyses, the co-analysts reported having to make additional calculations in Task 2 compared to Task 1. In the remaining `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(percentage)`% (`r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(N)`) of the cases, the co-analysts indicated that despite the requirements of the instructions, they could conduct the same analyses as in Task 1.

```{r include=FALSE}
direction_of_result_data <- 
  calculate_percentage(processed, direction_of_result)
```

In Task 2, `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(percentage)`% of the results (`r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(n)` out of `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(N)`) were in the opposite direction as claimed by the original study, disregarding whether the effect was conclusive/significant.

```{r include=FALSE}
total_hours_data <-
  processed |> 
  filter(total_hours != 999)
```

The co-analysts were asked to estimate the time they spent performing Task 1 and Task 2 together. The median value of their response is `r median(total_hours_data$total_hours)` hours (@fig-total-hours).

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-total-hours
#| fig-cap: "The figure shows the reported total hours the analyst spent on Task 1 and Task 2 together. In case an analyst completed multiple re-analyses, we kept all their responses for this figure. One response was excluded due to being an outlier (999 hours)."

total_hours_plot <-
  total_hours_data |>
  ggplot() +
  aes(x = total_hours) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Total hours spent on the analysis",
       y = "Number of co-analysts") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_total_hours_plot.jpg"), total_hours_plot, dpi = 300)

total_hours_plot
```

```{r include=FALSE}
same_test_family <-
  processed |>
  dplyr::select(
    simplified_paper_id,
    analyst_id,
    reanalysis_type_of_statistic,
    reanalysis_statistic_report
  ) |>
  dplyr::filter(!is.na(reanalysis_statistic_report) &
                  !is.na(reanalysis_type_of_statistic)) |>
  dplyr::mutate(
    reanalysis_statistic_report = round(reanalysis_statistic_report, 2),
    reanalysis_statistic = interaction(reanalysis_type_of_statistic, reanalysis_statistic_report)
  ) |>
  dplyr::group_by(simplified_paper_id, reanalysis_statistic) |>
  dplyr::summarise(
    pair_count = n(), .groups = 'drop'
  ) |>
  dplyr::mutate(
    is_unique = (pair_count == 1)
  ) |> 
  dplyr::group_by(simplified_paper_id, is_unique) |>
  dplyr::summarise(
    total = sum(pair_count), .groups = 'drop'
  ) |> 
   tidyr::complete(
    simplified_paper_id, is_unique,
    fill = list(total = 0)
  )

proportion_unique_analysis <-
  same_test_family |>
  dplyr::group_by(is_unique) |>
  dplyr::summarise(grand_total = sum(total), .groups = 'drop') |>
  dplyr::mutate(total_count = sum(grand_total),
                percentage = round(grand_total / total_count * 100))

proportion_unique_paper <-
  same_test_family |> 
  dplyr::group_by(simplified_paper_id) |>
  dplyr::summarise(
    has_non_unique = sum(total[!is_unique] > 0), .groups = 'drop'
  ) |> 
  dplyr::summarise(
    percentage_all_unique = round(mean(has_non_unique == 0) * 100, 2)
  )
```

In Task 2 `r dplyr::filter(proportion_unique_analysis, is_unique) |> pull(percentage)`% of the co-analyst used unique analytical pipelines based on the statistical test family and the value of the test statistics they arrived at. In total `r pull(proportion_unique_paper, percentage_all_unique)`% of the papers had completely unique reanalysis attempts.

## Peer evaluation

### Peer evaluators

#### Basic demographics of the peer evaluators

```{r include=FALSE}
# Get peer evaluator demographic info from task1 and task2 survey results
peer_evaluator_data <-
  peer_eval |> 
  distinct(evaluator_id) |> 
  inner_join(processed, by = c("evaluator_id" = "analyst_id"))

# Check if an evaluator has more than one analysis submitted
peer_evaluator_data |> 
  count(evaluator_id) |> 
  arrange(desc(n))

# Check if a peer evaluator has more than one evaluation submitted
peer_eval |>
  count(evaluator_id) |>
  arrange(desc(n))
```

@fig-evaluator-years shows that most peer evaluators have many years of experience with conducting statistical analysis.

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, years_of_experience)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-evaluator-years
#| fig-cap: "The figure shows the peer evaluators’ years of experience with data analysis. When a peer evaluator submitted more than one evaluation and a year passed between the responses, we kept only their first response."

peer_analyst_experience_years_data <-
  peer_evaluator_data |>
  dplyr::select(evaluator_id, years_of_experience, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(evaluator_id, task1_timestamp)

peer_analyst_experience_years_plot <-
  peer_analyst_experience_years_data |>
  ggplot() +
  aes(x = years_of_experience) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Years of experience",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_experience_years_plot.jpg"), peer_analyst_experience_years_plot, dpi = 300)

peer_analyst_experience_years_plot
```

@fig-evaluator-analysis-frequency indicates that peer evaluators regularly perform data analysis.

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, analysis_frequency)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-evaluator-analysis-frequency
#| fig-cap: "The figure shows how regularly the peer evaluators perform data analysis."

peer_analysis_frequency_count <-
  peer_evaluator_data |> 
  dplyr::select(evaluator_id, paper_id, analysis_frequency, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = evaluator_id, time_var = task1_timestamp) |> 
  count(analysis_frequency)

# For this question we report the responses by analysis and not analyst 
peer_analysis_frequency_plot <-
  peer_analysis_frequency_count |>
  mutate(
    analysis_frequency = case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) |>
  ggplot() +
  aes(x = analysis_frequency, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Frequency of doing data analysis",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    axis.text.x = element_text(size = 6)
  )

ggsave(here::here("figures/demographic_evaluators_analysis_frequency_plot.jpg"), peer_analysis_frequency_plot, dpi = 300)

peer_analysis_frequency_plot
```

@fig-evaluator-expertise indicates that most peer evaluators rate themselves close to expert level in data analysis.

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-evaluator-expertise
#| fig-cap: "The figure shows the peer evaluators’ self-rated level of expertise in data analysis. When a peer evaluator submitted more than one re-analysis, we kept only their first response."

peer_expertise_self_rating_data <-
  peer_evaluator_data |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = evaluator_id, time_var = task1_timestamp) |>
  count(expertise_self_rating) |> 
  complete(expertise_self_rating, fill = list(n = 0))

peer_expertise_self_rating_plot <-
  peer_expertise_self_rating_data |>
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  ) |>
  tidyr::complete(expertise_self_rating, fill = list(n = 0)) |> 
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-reported expertise rating",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/demographic_evaluators_expertise_self_rating_plot.jpg"), peer_expertise_self_rating_plot, dpi = 300)

peer_expertise_self_rating_plot
```

### Peer evaluations

#### Descriptives of peer evaluations

```{r include=FALSE}
# Number of peer evaluations
# One response was excluded in multi100_raw_processed because the evaluator did not provide the analyst_id
nrow(peer_eval_not_reviewed)
nrow(peer_eval)
```

In total, we received `r nrow(peer_eval_not_reviewed) + 1` peer evaluation reports. One peer evaluation was removed because the ID of the analyst was not provided, and as such, we could not verify with certainty which re-analysis was being evaluated leaving us with a total of `r nrow(peer_eval_not_reviewed)` peer evaluation reports on `r nrow(distinct(peer_eval_not_reviewed, paper_id))` different papers. After the panel member review of the peer evaluations (see Peer Evaluation: Review and Decisions’ supplement for all decisions and reasoning behind each case), the final result of the peer evaluation was the following.

```{r include=FALSE}
dplyr::count(peer_eval, task1_pipeline_acceptable)
dplyr::count(peer_eval, task1_conclusion_follows_results)
dplyr::count(peer_eval, task2_pipeline_acceptable)
```

At the end of the peer evaluation process, one analysis was deemed to contain an unacceptable analysis pipeline. Therefore, we removed this single analysis from our results. For the remaining analyses, it was determined that all Task 1 and Task 2 analysis pipelines were acceptable. Furthermore, all remaining Task 1 conclusions were considered to accurately follow on from the results, and the analysts self-categorization of the results were considered adequate.

```{r include=FALSE}
reproducibility_checks_data <-
  peer_eval |> 
  calculate_percentage(any_code_mismatches)

reproducibility_checks_n <-
  reproducibility_checks_data |> 
  filter(any_code_mismatches %in% c("(3) I executed it and I found no mismatches", "(4) I executed it and I found mismatches")) |> 
  summarise(n_reproducibility_checks = sum(n))
```

`r reproducibility_checks_n` analytical reproducibility checks were successfully conducted which identified mismatches in `r filter(reproducibility_checks_data, any_code_mismatches == "(4) I executed it and I found mismatches") |> pull(n)` analyses. In all of these cases we verified that that the mismatches did not have a meaningful impact on the reported conclusion, categorization, or effect size.

## Inferential robustness: The robust of the conclusions to analytical choices published in social sciences

Do different analysts arrive at the same conclusions as the analysts of the original study?

### Task 1 Survey results

```{r include=FALSE}
# Distinct values of task1_categorisation
distinct(processed, task1_categorisation)
distinct(processed, task1_categorisation_plotting)

conclusions_main_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |> 
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  calculate_conclusion(grouping_var = simplified_paper_id, categorization_var = categorisation) |>
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(categorisation == "Same conclusion", percentage, NA), .desc = FALSE, .na_rm = TRUE)
  )

# When threshold is 100% same conclusion for robust result
conclusions_main_robustness_data <- calculate_conclusion_robustness(data = processed, categorization_var = task1_categorisation_plotting, threshold = 100)
# When threshold is 80% same conclusion for robust result
conclusions_main_robustness_80_data <- calculate_conclusion_robustness(data = processed, categorization_var = task1_categorisation_plotting, threshold = 80, operator = ">")
# When threshold is 60% same conclusion for robust result
conclusions_main_robustness_60_data <- calculate_conclusion_robustness(data = processed, categorization_var = task1_categorisation_plotting, threshold = 60, operator = ">")
```

In Task 1, the co-analysts were asked to conduct any statistical analysis to arrive at a single conclusion. Out of `r distinct(conclusions_main_robustness_data, N) |> pull(N)` re-analysed studies, the conclusions of `r filter(conclusions_main_robustness_data, robust == "Inferentially robust") |> pull(percentage)`% (`r filter(conclusions_main_robustness_data, robust == "Inferentially robust") |> pull(n)`) remained robust to independent re-analysis, so all assigned co-analysts arrived at the same conclusion as reported in the article of the original study (inferential robustness; see @fig-conclusions-main-robustness). With alternative definitions of analytical robustness, this value was `r filter(conclusions_main_robustness_80_data, robust == "Inferentially robust") |> pull(percentage)`% when 80% and `r filter(conclusions_main_robustness_60_data, robust == "Inferentially robust") |> pull(percentage)`% when 60% reanalysis agreement with the original conclusion defined analytical robustness. For one study (1%), all co-analysts reported no effect or an inconclusive conclusion.

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-main-robustness
#| fig-cap: "The figure shows the proportion of the inferentially robust and not robust studies."

conclusion_main_robustness_plot <- plot_conclusion_robustness(conclusions_main_robustness_data, robust)

ggsave(here::here("figures/conclusion_main_robustness_plot.jpg"), conclusion_main_robustness_plot, dpi = 300)

conclusion_main_robustness_plot
```

@fig-conclusions-main shows the histogram display of the different and identical conclusions resulting from the re-analysis of each of the studies.

```{r echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-conclusions-main
#| fig-cap: "The figure shows the percentage of identical, inconclusive, and different conclusions for each study. Study numbers correspond to studies listed in Table S1."

conclusion_main_plot <-
  plot_percentage(
    data = conclusions_main_data,
    grouping_var = simplified_paper_id,
    categorization_var = categorisation,
    x_lab = "Study number",
    y_lab = "Percentage of re-analyses",
    with_sum = FALSE,
    reverse = TRUE,
    rev_limits = FALSE,
    coord_flip = TRUE
  ) +
  theme(
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 10),
    axis.title.x = element_text(size = 15),
    legend.justification = "left",
    legend.box = "horizontal",
    legend.position = "bottom",
    legend.text = element_text(size = 10)
  )

# Using a hacky solution to move legend under the Y axis
# conclusion_main_plot_wo_legend <- conclusion_main_plot + theme(legend.position = "none")
# legend <- cowplot::get_legend(conclusion_main_plot)
# conclusion_main_plot_w_legend <- cowplot::plot_grid(conclusion_main_plot_wo_legend, legend, nrow = 2, rel_heights = c(1, 0.05), rel_widths = c(1, 5))

ggsave(here::here("figures/conclusion_main_plot.jpg"), conclusion_main_plot, width = 9, height = 11.69, dpi = 300)

conclusion_main_plot
```

```{r include=FALSE}
conclusions_analysis_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |> 
  mutate(
    categorisation = fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  count(categorisation) |> 
  ungroup() |> 
  mutate(
    N = sum(n),
    freq = n / N,
    percentage = round(freq * 100, 2)
  )
```

Across all the re-analyses, `r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(percentage)` % (`r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "Same conclusion") |> pull(N)`) of them arrived at the same conclusion; `r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(percentage)`% (`r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "No effect/inconclusive") |> pull(N)`) to no effects, and `r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(percentage)`% (`r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(n)` out of `r filter(conclusions_analysis_data, categorisation == "Opposite effect") |> pull(N)`) to opposite effect compared to the original conclusion.

#### Inferential robustness by discipline

We were interested to see whether the above results show a different pattern when inspecting them in different disciplines. @fig-discipline-robustness shows that for the fields with more than 10 studies in our collection (Economics, Political Science, and Psychology) the pattern was comparably similar. We found no outstanding differences between the discipline regarding the percentage of different and identical conclusions either (see @fig-conclusions-discipline).

```{r include=FALSE}
conclusions_discipline_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = str_to_title(paper_discipline)) |> 
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  calculate_conclusion(grouping_var = paper_discipline, categorization_var = categorisation)

conclusions_discipline_robustness_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = str_to_title(paper_discipline)) |> 
  calculate_conclusion_robustness(grouping_var = paper_discipline, categorization_var = task1_categorisation_plotting)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-discipline-robustness
#| fig-cap: "The figure shows the inferential robustness of the studies by major disciplines (more than 10 studies in our collection)."

conclusions_discipline_robustness_plot <-
  plot_percentage(
    data = conclusions_discipline_robustness_data,
    categorization_var = robust,
    grouping_var = paper_discipline,
    with_labels = TRUE,
    y_lab = "Percentage of studies",
    x_lab = "Disciplines",
    colors = c("#000004FF", "#FCFDBFFF"),
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    axis.title.y = element_text(hjust = 1)
  )

ggsave(here::here("figures/conclusions_discipline_robustness_plot.jpg"), conclusions_discipline_robustness_plot, dpi = 300)

conclusions_discipline_robustness_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-discipline
#| fig-cap: "The figure shows the percentage of identical, inconclusive, and different conclusions of the studies by major disciplines. The figure displays the count of re-analyses next to each discipline name."

conclusions_discipline_plot <-
  plot_percentage(
    data = conclusions_discipline_data,
    grouping_var = paper_discipline,
    categorization_var = categorisation,
    with_labels = TRUE,
    y_lab = "Percentage of re-analyses",
    x_lab = "Disciplines",
    rev_limits = FALSE
  ) + 
  theme(
    plot.margin = margin(t = 20, r = 10, b = 10, l = 10, unit = "pt"),
    legend.text = element_text(size = 8),
    axis.title.y = element_text(hjust = 1)
    )

ggplot2::ggsave(here::here("figures/conclusions_discipline_plot.jpg"), conclusions_discipline_plot, dpi = 300)

conclusions_discipline_plot
```

#### Inferential robustness by study type (observational, experimental)

Here, we were interested to see whether these results show a different pattern when separating them by study type. @fig-studytype-robustness illustrates that nearly half of the results from experimental studies remained robust upon independent re-analysis, whereas only one-third of observational studies yielded robust conclusions. Moreover, @fig-conclusions-studytype indicates that, for both study types, the majority of the re-analyses reached the same conclusions as the original study.

```{r include=FALSE}
conclusions_studytype_data <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(categorisation = forcats::fct_relevel(
    categorisation,
    c("Same conclusion", "No effect/inconclusive", "Opposite effect")
  )) |>
  calculate_conclusion(grouping_var = experimental_or_observational,
                       categorization_var = categorisation) |>
  dplyr::mutate(experimental_or_observational = str_to_title(experimental_or_observational))

conclusions_studytype_robustness_data <-
  processed |>
  dplyr::mutate(experimental_or_observational = str_to_title(experimental_or_observational)) |>
  calculate_conclusion_robustness(grouping_var = experimental_or_observational,
                                  categorization_var = task1_categorisation_plotting) |>
  dplyr::ungroup()
```

```{r echo=FALSE, message=FALSE}
#| label: fig-studytype-robustness
#| fig-cap: "The figure shows the inferential robustness of the studies by study type (experimental or observational). The figure displays the count of re-analyses next to each study type name."

conclusions_studytype_robustness_plot <-
  plot_percentage(
    data = conclusions_studytype_robustness_data,
    categorization_var = robust,
    grouping_var = experimental_or_observational,
    y_lab = "Percentage of studies",
    x_lab = "Study type",
    with_labels = TRUE,
     colors = c("#000004FF", "#FCFDBFFF"),
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    axis.title.y = element_text(hjust = 1)
  )

ggplot2::ggsave(here::here("figures/conclusions_studytype_robustness_plot.jpg"), conclusions_studytype_robustness_plot, dpi = 300)

conclusions_studytype_robustness_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-studytype
#| fig-cap: "The figure shows percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by study type (experimental, observational)."

conclusions_studytype_plot <-
  plot_percentage(
    data = conclusions_studytype_data,
    grouping_var = experimental_or_observational,
    categorization_var = categorisation,
    x_lab = "Study type",
    y_lab = "Percentage of re-analyses",
    with_labels = TRUE,
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    plot.margin = margin(
      t = 20,
      r = 10,
      b = 10,
      l = 10,
      unit = "pt"
    ),
    legend.text = element_text(size = 8)
  ) +
    ggplot2::theme(
    axis.title.y = element_text(hjust = 1)
  )

ggplot2::ggsave(here::here("figures/conclusions_studytype_plot.jpg"), conclusions_studytype_plot, dpi = 300)

conclusions_studytype_plot
```

#### Inferential robustness by expertise (self-reported expertise in data analysis)

Here, we were interested to see whether these results show a different pattern when inspecting them along the reported expertise of the co-analysts. @fig-conclusions-expertise shows these results.

```{r include=FALSE, message=FALSE}
conclusions_expertise_data <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  calculate_conclusion(grouping_var = expertise_self_rating, categorization_var = categorisation) |> 
  dplyr::mutate(
    categorisation = forcats::fct_relevel(
      categorisation,
      c("Same conclusion", "No effect/inconclusive", "Opposite effect")
    ),
    expertise_self_rating = dplyr::case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  )
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-conclusions-expertise
#| fig-cap: "The figure shows the percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by self-rated expertise (on a scale of 1 (Beginner) to 10 (Expert)). The figure does not display the bottom two categories where fewer than 3 responses were collected for each."

conclusions_expertise_plot <-
  plot_height(
    data = conclusions_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = categorisation,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    with_sum = FALSE,
    rev_limits = FALSE
  ) +
  theme(legend.text = element_text(size = 8))

ggsave(here::here("figures/conclusions_expertise_plot.jpg"), conclusions_expertise_plot, 
       height = 6, width = 8,
       dpi = 300)

conclusions_expertise_plot
```

#### Inferential robustness by peer evaluations

@fig-subset-task1-pipeline shows the inferential robustness of the studies by the acceptability of the analysis pipelines according to the peer evaluators.

```{r include=FALSE}
count(peer_eval_not_reviewed, task1_pipeline_acceptable)
count(peer_eval_not_reviewed, task1_conclusion_follows_results)
count(peer_eval_not_reviewed, task1_categorisation_is_accurate)
count(peer_eval_not_reviewed, task2_pipeline_acceptable)
```

```{r include=FALSE}
peer_eval_subset_task1_data <- 
  peer_eval |>  
  dplyr::filter(n_peer_evals > 1) |>  
  dplyr::select(paper_id, analyst_id, task1_pipeline_acceptable) |>  
  dplyr::group_by(paper_id, analyst_id) |>  
  dplyr::summarise(
    common_task1_acceptable = ifelse(dplyr::n_distinct(task1_pipeline_acceptable) == 1, first(task1_pipeline_acceptable), NA),
    .groups = 'drop'
  ) |> 
  dplyr::filter(!is.na(common_task1_acceptable))

count(peer_eval_subset_task1_data, common_task1_acceptable)
  
peer_eval_subset_task1_data <- 
  peer_eval_subset_task1_data |> 
  dplyr::filter(common_task1_acceptable != "(2) Acceptable but low quality") |> 
  left_join(dplyr::select(processed, paper_id, analyst_id, task1_categorisation_plotting), by = c("paper_id", "analyst_id")) |> 
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(
    common_task1_acceptable = dplyr::case_when(
      common_task1_acceptable == "(3) Acceptable and medium quality" ~ "Acceptable and medium quality",
      common_task1_acceptable == "(4) Acceptable and high quality" ~ "Acceptable and high quality"
    ),
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect"))
    ) |> 
  calculate_conclusion(grouping_var = common_task1_acceptable,
                       categorization_var = categorisation)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-subset-task1-pipeline
#| fig-cap: "The figure shows the inferential robustness of the studies by the acceptability of the analysis pipelines according to the peer evaluators. For this figure we only included studies with more than one peer evaluation and where the peer evaluators agreed on their rating. The figure shows only the studies with a medium and high quality of analysis pipelines."

peer_eval_subset_task1_plot <-
  plot_percentage(
    data = peer_eval_subset_task1_data,
    categorization_var = categorisation,
    grouping_var = common_task1_acceptable,
    with_labels = TRUE,
    y_lab = "Percentage of studies",
    x_lab = "Peer evaluation rating of\nTask 1 analysis pipeline"
  ) +
  ggplot2::theme(
    legend.text = element_text(size = 8),
    axis.title.y = element_text(hjust = 1)
  )

ggsave(here::here("figures/peer_eval_subset_task1_plot.jpg"), peer_eval_subset_task1_plot, dpi = 300)

peer_eval_subset_task1_plot
```

```{r include=FALSE}
peer_eval |> 
  dplyr::select(paper_id, analyst_id, evaluator_id, task1_pipeline_acceptable) |>
  count(paper_id, analyst_id) |> 
  arrange(n)

calculate_percentage(peer_eval, task1_pipeline_acceptable) |>
  dplyr::select(
    `Task 1 analysis pipeline evaluation rating` = task1_pipeline_acceptable,
    `Number of occurances` = n,
    `Number of all responses` = N,
    Percentage = percentage
  ) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

#### Inferential robustness by prior familiarity with the dataset

Here, we were interested to see whether these results show a different pattern when inspecting them along their prior familiarity with the dataset. @fig-conclusions-familiarity shows that for these results.

```{r include=FALSE}
count(processed, familiar_with_paper)

conclusions_familiarity_data <- 
  processed |> 
  rename(categorisation = task1_categorisation_plotting) |>
  mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect")),
    familiar_with_paper = as.factor(familiar_with_paper)
    ) |> 
  calculate_conclusion(grouping_var = familiar_with_paper, categorization_var = categorisation)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-familiarity
#| fig-cap: "The figure shows the percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by declared familiarity with the study."

conclusions_familiarity_plot <- 
  plot_percentage(
    data = conclusions_familiarity_data,
    grouping_var = familiar_with_paper,
    categorization_var = categorisation,
    with_labels = TRUE,
    y_lab = "Percentage of re-analyses",
    x_lab = "Analyst familiarity with the paper") + 
  theme(
    plot.margin = margin(t = 20, r = 10, b = 10, l = 10, unit = "pt"),
    legend.text = element_text(size = 8),
    axis.title.y = element_text(hjust = 1)
    )

ggsave(here::here("figures/conclusions_familiarity_plot.jpg"), conclusions_familiarity_plot, dpi = 300)

conclusions_familiarity_plot
```

#### Inferential robustness by the level of confidence with the suitability of the analysis

```{r include=FALSE}
conclusions_suitability_data <- 
  processed |> 
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(categorisation, c("Same conclusion", "No effect/inconclusive", "Opposite effect")),
    confidence_in_approach = dplyr::case_when(
      confidence_in_approach == 1 ~ "1\nNot confident at all",
      confidence_in_approach == 5 ~ "5\nVery confident",
      TRUE ~ as.character(confidence_in_approach)
      ),
    confidence_in_approach = as.factor(confidence_in_approach)
    ) |> 
  calculate_conclusion(grouping_var = confidence_in_approach, categorization_var = categorisation)
```

The following Table 3 shows the percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by the analyst's level of confidence with the suitability of the analysis.

**Table 3. Inferential Robustness by the Level of Confidence with the Suitability of the Analysis**

```{r echo=FALSE, message=FALSE}
conclusions_suitability_data |>
  dplyr::mutate(Count = paste(n, "/", N),
                percentage = paste0(percentage, "%")) |>
  dplyr::select(
    `Confidence rating` = confidence_in_approach,
    `Direction of the conclusion` = categorisation,
    Count,
    Percentage = percentage
  ) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

#### Inferential robustness by the sample size

Here, in @fig-conclusions-samplesize we were interested to see whether these results show a different pattern when considering sample size.

```{r include=FALSE}
processed |>
  ggplot2::ggplot() +
  # Converting to natlog because of extreme outlier
  ggplot2::aes(x = log(reanalysis_model_sample_size)) +
  ggplot2::geom_histogram()

processed |> 
  dplyr::count(is.na(reanalysis_model_sample_size))
```

```{r echo=FALSE, message=FALSE}
#| label: fig-conclusions-samplesize
#| fig-cap: "This raincloud figure shows the distribution of the sample sizes of the re-analyses resulting in same conclusion, no effect/inconclusive, and opposite effects."

conclusions_samplesize_plot <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::select(paper_id,
                analyst_id,
                categorisation,
                reanalysis_model_sample_size) |>
  # TODO: There are missing sample size values what to do with them?
  dplyr::filter(!is.na(reanalysis_model_sample_size)) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(
      categorisation,
      c("Same conclusion", "No effect/inconclusive", "Opposite effect")
    )
  ) |> 
  plot_rain(
    grouping_var = categorisation,
    response_var = reanalysis_model_sample_size,
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000),
    x_lab = "Direction of the re-analysis conclusion\ncompared to the original effect",
    y_lab = "Sample size"
  ) +
  ggplot2::coord_flip() +
  ggplot2::theme(
    axis.title = element_text(size = 9)
  )

ggplot2::ggsave(here::here("figures/conclusions_samplesize_plot.jpg"), conclusions_samplesize_plot, dpi = 300)

# Number of analysis where samplesize is present
nrow(filter(processed, !is.na(reanalysis_model_sample_size)))
# Number of analysis where df1 is present
nrow(filter(processed, !is.na(reanalysis_degrees_of_freedom_1) | !is.na(reanalysis_degrees_of_freedom_2)))
# dplyr::select(processed, reanalysis_model_sample_size, reanalysis_degrees_of_freedom_1, reanalysis_degrees_of_freedom_2)

conclusions_df_plot <-
  processed |>
  dplyr::rename(categorisation = task1_categorisation_plotting) |>
  dplyr::select(paper_id,
                analyst_id,
                categorisation,
                reanalysis_degrees_of_freedom_1,
                reanalysis_degrees_of_freedom_2) |>
  # It looks like of df2 is present than df2 belongs to the responses and df1 is for the parameters
  dplyr::mutate(
    reanalysis_degrees_of_freedom_1 = dplyr::case_when(
      !is.na(reanalysis_degrees_of_freedom_2) ~ reanalysis_degrees_of_freedom_2,
      is.na(reanalysis_degrees_of_freedom_2) ~ reanalysis_degrees_of_freedom_1
    )
  ) |> 
  dplyr::select(-reanalysis_degrees_of_freedom_2) |> 
  dplyr::filter(!is.na(reanalysis_degrees_of_freedom_1)) |>
  dplyr::mutate(
    categorisation = forcats::fct_relevel(
      categorisation,
      c("Same conclusion", "No effect/inconclusive", "Opposite effect")
    )
  ) |> 
  plot_rain(
    grouping_var = categorisation,
    response_var = reanalysis_degrees_of_freedom_1,
    x_lab = "Direction of the re-analysis conclusion\ncompared to the original effect",
    y_lab = "Log10 of degrees of freedom",
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000)
  ) +
  ggplot2::coord_flip() +
    ggplot2::theme(
    axis.title = element_text(size = 9)
  )

ggplot2::ggsave(here::here("figures/conclusions_df_plot.jpg"), conclusions_df_plot, dpi = 300)

conclusions_df_plot
```

#### Estimate robustness: robust of the statistical findings published in social sciences to analytical choices

```{r include=FALSE}
# Number of cases where the original effect size is missing
missing_original_n <-
  processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(is.na(original_cohens_d)) |> 
  dplyr::filter(`is.na(original_cohens_d)`) |> 
  dplyr::pull(n)

# Check if there are cases where there is missing original effect size on the paper level but not for every analysis
# Would mean that there is a mistake in the merging code
processed |> 
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |> 
  dplyr::group_by(paper_id) |> 
  dplyr::mutate(
    number_of_analysis = dplyr::n() 
  ) |> 
  dplyr::filter(is.na(original_cohens_d)) |> 
  mutate(
    number_of_analysis_after_filtering = dplyr::n()
  ) |> 
  dplyr::ungroup() |> 
  dplyr::mutate(
    match = dplyr::if_else(number_of_analysis == number_of_analysis_after_filtering, TRUE, FALSE)
  )

# Number of cases where the reanalysed effect size is missing
missing_reanalysis_n <-
  processed |> 
  dplyr::count(is.na(reanalysis_cohens_d)) |> 
  dplyr::filter(`is.na(reanalysis_cohens_d)`) |> 
  dplyr::pull(n)

# Are there any cases where the original effect size is present but the reanalyzed is not and vica versa?
processed |> 
  dplyr::select(original_cohens_d, reanalysis_cohens_d) |> 
  dplyr::mutate(
    both_present_or_missing = is.na(original_cohens_d) == is.na(reanalysis_cohens_d)
  ) |> 
  dplyr::count(both_present_or_missing)

# Number of cases where reanalyzed effect size is comparable to original effect size
processed |>
  dplyr::select(original_cohens_d, reanalysis_cohens_d) |>
  filter(!is.na(original_cohens_d)) |>
  filter(!is.na(reanalysis_cohens_d)) |> 
  nrow()

# Check if there are any cases where everything is missing
processed |> 
  dplyr::group_by(paper_id) |>
  dplyr::mutate(
    both_missing = all(is.na(original_cohens_d) & is.na(reanalysis_cohens_d))
  ) |>
  dplyr::ungroup() |> 
  dplyr::count(both_missing)
  
# Preparing the data for the plot
reanalysis_data <-
  processed |>
  dplyr::select(simplified_paper_id,
                analyst_id,
                reanalysis_cohens_d,
                original_cohens_d) |>
  dplyr::group_by(simplified_paper_id) |>
  dplyr::rename(effect_size = reanalysis_cohens_d) |>
  dplyr::mutate(effect_size_type = paste0("re-analysis_0", row_number()),) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    simplified_paper_id = as.factor(simplified_paper_id),
    # Put missing values at the end
    original_cohens_d = dplyr::if_else(is.na(original_cohens_d), -Inf, original_cohens_d),
    simplified_paper_id = forcats::fct_reorder(.f = simplified_paper_id,
                                               .x = original_cohens_d,
                                               .fun = function(x) median(x),
                                               .na_rm = FALSE)
  )

original_data <-
  processed |>
  dplyr::distinct(simplified_paper_id, original_cohens_d) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    simplified_paper_id = as.factor(simplified_paper_id),
    simplified_paper_id = forcats::fct_reorder(simplified_paper_id,
                                               original_cohens_d,
                                               .na_rm = FALSE)
  ) |> 
  dplyr::filter(!is.na(original_cohens_d))

excluded_es <-
  dplyr::filter(reanalysis_data, effect_size > 5 | effect_size < -5) |> 
  mutate(excluded = glue::glue("study {simplified_paper_id}: {effect_size}")) |> 
  summarise(excluded_list = glue::glue_collapse(excluded, sep = "; "))
```

A main question of our study was whether different analysts arrive at the same effect estimates (+/- 0.05 Cohen’s d) as the analyst of the original study? @fig-effect-main shows percentages of the effect sizes falling within the preset tolerance range (+/- 0.05 Cohen’s d) for each study. The figure displays `r nrow(dplyr::filter(reanalysis_data, !is.na(effect_size)))` re-analysis effect size estimates. For `r missing_reanalysis_n` re-analyses, the reported effect size was not convertible to Cohen’s d. The figure does not show `r dplyr::filter(reanalysis_data, effect_size > 5 | effect_size < -5) |> nrow()` re-analysed effect sizes which are higher than 5 or lower than -5 Cohen's d (`r dplyr::pull(excluded_es, excluded_list)`). For the `r missing_original_n` studies listed in the bottom of the graph, we could not determine the original effect size due to missing information.

```{r echo=FALSE, warning=FALSE}
#| label: fig-effect-main
#| fig-cap: "The figure shows the effect size of the original result (black square) and the effect sizes of the re-analyses (green dot) for each study after conversions to Cohen’s d. Study numbers correspond to studies listed in Table S1."

# Colors
# color_vector <- setNames(c("#F8766D", "#CD9600", "#7CAE00", "#00BE67", "#00BFC4", "#00A9FF", "#C77CFF"), paste0("re-analysis_", sprintf("%02d", 1:7)))

effect_main_plot <-
  reanalysis_data |> 
  dplyr::filter(effect_size <= 5 & effect_size >= -5) |> 
  ggplot2::ggplot() +
  ggplot2::aes(
    y = simplified_paper_id,
    x = effect_size,
    # color = effect_size_type
  ) +
  ggplot2::geom_point(
    shape = 16,
    color = viridis::viridis(5)[[3]],
    # size = 4
    ) +
  # ggplot2::scale_color_manual(values = color_vector) +
  ggplot2::geom_pointrange(
    data = original_data,
    ggplot2::aes(
      x = original_cohens_d,
      xmin = tolarence_region_lower,
      xmax = tolarence_region_upper,
      alpha = 0.8
      ),
    show.legend = FALSE,
    color = "black",
    shape = 15,
    # size = 1
    ) +
  labs(
    x = expression("Effect size in Cohen's " * italic("d")),
    y = "Study number"
  ) +
  ggplot2::guides(color = "none") +
  ggplot2::scale_x_continuous(breaks = seq(-5.0, 5.0, by = 0.5)) +
  ggplot2::theme(
    axis.ticks = ggplot2::element_blank(),
    legend.title = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(),
    plot.margin = ggplot2::margin(t = 10, r = 20, b = 10, l = 10, "pt"),
    panel.grid = ggplot2::element_line(color = "lightgray"),
    # panel.grid.major.x = ggplot2::element_blank(),
    panel.grid.minor.x = ggplot2::element_blank(),
    panel.background = ggplot2::element_blank(),
    axis.title = ggplot2::element_text(size = 20),
    axis.text.y = ggplot2::element_text(size = 10),
    # axis.text.x = ggplot2::element_text(angle = 90, size = 5)
    axis.text.x = ggplot2::element_text(size = 13)
    # axis.text.y=element_text(margin = margin(1, unit = "cm"), vjust =1.5)
    )

ggplot2::ggsave(here::here("figures/effect_main_plot.jpg"), plot = effect_main_plot,
       width = 12, height = 13,
       dpi = 300)

effect_main_plot
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Based on: https://github.com/CenterForOpenScience/rpp
colors <- c("Original" = "#440154FF", "Replication" = "#FDE725FF")

effec_size_corr_plot_data <-
  reanalysis_data |>
  dplyr::filter(!is.na(effect_size)) |>
  dplyr::filter(effect_size <= 5 & effect_size >= -5) |>
  dplyr::filter(original_cohens_d != -Inf)

# Median
round(median(effec_size_corr_plot_data$effect_size), 2)
round(median(effec_size_corr_plot_data$original_cohens_d), 2)

# Mean
round(mean(effec_size_corr_plot_data$effect_size), 2)
round(mean(effec_size_corr_plot_data$original_cohens_d), 2)

# Fit the linear model
lm_fit <- lm(effect_size ~ original_cohens_d, data = effec_size_corr_plot_data)

# Extract slope
beta <- coef(lm_fit)[2]

scatter <-
  effec_size_corr_plot_data |> 
  ggplot(aes(x = original_cohens_d, y = effect_size, color = as.factor(colors))) +
  geom_point(color = "Grey30",
             shape = 21,
             alpha = .8) +
  geom_smooth(
    method = "lm",
    se = FALSE,
    color = "black",
    alpha = 0.2
  ) +
  annotate("text", x = 4.7, y = 4, label = bquote(italic(beta) == .(round(beta, 2))), size = 4, color = "black") +
  geom_rug(
    aes(color = "Original"),
    size = 1,
    sides = "b",
    alpha = .6
  ) +
  geom_rug(
    aes(color = "Replication"),
    size = 1,
    sides = "l",
    alpha = .6
  ) +
  scale_color_manual(values = colors) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_abline(intercept = 0,
              slope = 1,
              color = "Grey60") +
  scale_x_continuous(
    limits = c(0, 5),
    breaks = c(0, .5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5)
  ) +
  scale_y_continuous(
    limits = c(-5, 5),
     breaks = c(-5, -4.5, -4, -3.5, -3, -2.5, -2, -1.5, -1, -0.5, 0, .5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5)
  ) +
  labs(
    x = expression("Original effect size in Cohen's " * italic("d")),
    y = expression("Re-analysis effect size in Cohen's " * italic("d"))
  ) +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    legend.position = "none",
    axis.title = element_text(size = 25),
    axis.text = element_text(size = 20)
    )

# Add reference lines for alignment checking
# x_dense <- x_dense + geom_vline(xintercept = c(0, 2.5, 5), linetype = "dashed", color = "red")
# y_dense <- y_dense + geom_vline(xintercept = c(-5, 0, 5), linetype = "dashed", color = "blue") + coord_flip()
# scatter <- scatter + geom_vline(xintercept = c(0, 2.5, 5), linetype = "dashed", color = "red") +
#                       geom_hline(yintercept = c(-5, 0, 5), linetype = "dashed", color = "blue")

effec_size_corr_plot <- ggExtra::ggMarginal(scatter, type = "density", margins = "both",  xparams = list(fill = colors["Original"], alpha = 0.5),  yparams = list(fill = colors["Replication"], alpha = 0.5))

effec_size_corr_plot_data_zoomed <-
  effec_size_corr_plot_data |> 
  dplyr::filter(effect_size <= 1 & effect_size >= -1) |>
  dplyr::filter(original_cohens_d <= 1 & original_cohens_d >= -1)

# Fit the linear model
lm_fit_zoomed <- lm(effect_size ~ original_cohens_d, data = effec_size_corr_plot_data_zoomed)

# Extract slope
beta_zoomed <- coef(lm_fit)[2]

scatter_zoomed <-
  effec_size_corr_plot_data_zoomed |> 
  ggplot(aes(x = original_cohens_d, y = effect_size, color = as.factor(colors))) +
  geom_point(color = "Grey30",
             shape = 21,
             alpha = .8) +
  geom_smooth(
    method = "lm",
    se = FALSE,
    color = "black",
    alpha = 0.2
  ) +
  annotate("text", x = 0.97, y = 0.8, label = bquote(italic(beta) == .(round(beta_zoomed, 2))), size = 4, color = "black") +
  geom_rug(
    aes(color = "Original"),
    size = 1,
    sides = "b",
    alpha = .6
  ) +
  geom_rug(
    aes(color = "Replication"),
    size = 1,
    sides = "l",
    alpha = .6
  ) +
  scale_color_manual(values = colors) + 
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_abline(intercept = 0,
              slope = 1,
              color = "Grey60") +
  scale_x_continuous(
    limits = c(0, 1),
    breaks = c(0, .5, 1)
  ) +
  scale_y_continuous(
    limits = c(-1, 1),
     breaks = c(-1, -0.5, 0, .5, 1)
  ) +
  labs(
    x = expression("Original effect size in Cohen's " * italic("d")),
    y = expression("Re-analysis effect size in Cohen's " * italic("d"))
  ) +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    legend.position = "none",
    axis.title = element_text(size = 25),
    axis.text = element_text(size = 20)
    )

effec_size_corr_plot_zoomed <- ggExtra::ggMarginal(scatter_zoomed, type = "density", margins = "both",  xparams = list(fill = colors["Original"], alpha = 0.5),  yparams = list(fill = colors["Replication"], alpha = 0.5))

# Save plot
ggplot2::ggsave(here::here("figures/effec_size_corr_plot.jpg"), plot = effec_size_corr_plot,
                width = 13, height = 10,
       dpi = 300)

ggplot2::ggsave(here::here("figures/effec_size_corr_plot_zoomed.jpg"), plot = effec_size_corr_plot_zoomed,
                width = 13, height = 10,
       dpi = 300)

effec_size_corr_plot

effec_size_corr_plot_zoomed
```

```{r include=FALSE}
# On the individual analysis level
# +-0.05 threshold for tolerance region
ind_within_tol_reg <-
  processed |>
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    is_within_region = dplyr::case_when(
      reanalysis_cohens_d >= tolarence_region_lower &
        reanalysis_cohens_d <= tolarence_region_upper ~ "Within tolerance region",
      reanalysis_cohens_d < tolarence_region_lower |
        reanalysis_cohens_d > tolarence_region_upper ~ "Outside of tolerance region",
      is.na(reanalysis_cohens_d) ~ "Missing"
    ),
    is_within_region = factor(
      is_within_region,
      levels = c("Within tolerance region", "Outside of tolerance region")
    )
  ) |> 
  # Exclude missing tolerance region decisions
  dplyr::filter(!is.na(is_within_region)) |> 
  calculate_percentage(is_within_region)

# +-0.2 threshold for tolerance region
ind_within_tol_reg_20 <-
  processed |>
  dplyr::select(paper_id, analyst_id, original_cohens_d, reanalysis_cohens_d) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.2,
    tolarence_region_upper = original_cohens_d + 0.2,
    is_within_region = dplyr::case_when(
      reanalysis_cohens_d >= tolarence_region_lower &
        reanalysis_cohens_d <= tolarence_region_upper ~ "Within tolerance region",
      reanalysis_cohens_d < tolarence_region_lower |
        reanalysis_cohens_d > tolarence_region_upper ~ "Outside of tolerance region",
      is.na(reanalysis_cohens_d) ~ "Missing"
    ),
    is_within_region = factor(
      is_within_region,
      levels = c("Within tolerance region", "Outside of tolerance region")
    )
  ) |> 
  # Exclude missing tolerance region decisions
  dplyr::filter(!is.na(is_within_region)) |> 
  calculate_percentage(is_within_region)

# On the paper level
effect_region_all_data <- 
  processed |> 
  calculate_tolerance_region(grouping_var = simplified_paper_id, drop_missing = T) |> 
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(is_within_region == "Within tolerance region", percentage, NA), .desc = TRUE, .na_rm = TRUE)
  )

within_tolerance_region <-
  effect_region_all_data |> 
  dplyr::group_by(simplified_paper_id) |> 
  dplyr::summarise(
    robust = if_else(any(is_within_region == "Within tolerance region" & relative_frequency == 1), "Inferentially robust", "Inferentially not Robust")
    ) |> 
  dplyr::ungroup() |> 
  dplyr::count(robust) |> 
  dplyr::mutate(
    N = sum(n),
    relative_frequency = n / N,
    percentage = round(relative_frequency * 100)
    )

effect_region_all_data_20 <- 
  processed |> 
  calculate_tolerance_region(grouping_var = simplified_paper_id, drop_missing = T, weight = 0.2) |> 
  mutate(
    simplified_paper_id = fct_reorder(simplified_paper_id, ifelse(is_within_region == "Within tolerance region", percentage, NA), .desc = TRUE, .na_rm = TRUE)
  )

within_tolerance_region_20 <-
  effect_region_all_data_20 |> 
  dplyr::group_by(simplified_paper_id) |> 
  dplyr::summarise(
    robust = if_else(any(is_within_region == "Within tolerance region" & relative_frequency == 1), "Inferentially robust", "Inferentially not Robust")
    ) |> 
  dplyr::ungroup() |> 
  dplyr::count(robust) |> 
  dplyr::mutate(
    N = sum(n),
    relative_frequency = n / N,
    percentage = round(relative_frequency * 100)
    )
```

We found that `r dplyr::filter(within_tolerance_region, robust == "Inferentially not Robust") |> dplyr::pull(percentage)`% (`r dplyr::filter(within_tolerance_region, robust == "Inferentially not Robust") |> dplyr::pull(n)` out of `r dplyr::filter(within_tolerance_region, robust == "Inferentially not Robust") |> dplyr::pull(N)`) of the studies contained at least one re-analysis result where the effect size was beyond the tolerance region (+/- 0.05 Cohen's d) of the result of the original study (@fig-effect-region-all). Out of the `r dplyr::distinct(ind_within_tol_reg, N) |> dplyr::pull(N)` available reanalysis effect sizes `r dplyr::filter(ind_within_tol_reg, is_within_region == "Outside of tolerance region") |> dplyr::pull(percentage)`% (`r dplyr::filter(ind_within_tol_reg, is_within_region == "Outside of tolerance region") |> dplyr::pull(n)`) were outside of the tolerance region. With a broader tolerance region (+/- 0.20 Cohen’s d) we found that from the available reanalysis effect sizes `r dplyr::filter(ind_within_tol_reg_20, is_within_region == "Outside of tolerance region") |> dplyr::pull(percentage)`% (`r dplyr::filter(ind_within_tol_reg_20, is_within_region == "Outside of tolerance region") |> dplyr::pull(n)`) were outside of the tolerance region.

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-all
#| fig-cap: "The figure shows percentages of the effect sizes falling within the preset tolerance range (+/- 0.05 Cohen’s d) for each study. Study numbers correspond to studies listed in Table S1."
  
# TODO: replace this function and delete it from utils
effect_region_all_plot <- plot_tolarence_region(data = effect_region_all_data, grouping_var = simplified_paper_id, y_lab = "Study number", x_lab = "Percentage of reanalysis results") +
  ggplot2::theme(
    axis.title = ggplot2::element_text(size = 15),
    legend.text = ggplot2::element_text(size = 10),
    axis.text.x = ggplot2::element_text(size = 13)
  )

ggplot2::ggsave(here::here("figures/effect_region_all_plot.jpg"), effect_region_all_plot, width = 8.27, height = 11.69, dpi = 300)

effect_region_all_plot
```

To investigate the robustness of our tolerance region threshold we calculated the proportion of reanalysis effect size estimates within the tolerance region for a range of thresholds between 0.05 and 0.1.

```{r include=FALSE, message=FALSE, warning=FALSE}
thresholds <- seq(0.01, 0.2, by = 0.001)

threshold_robustness_plot_data <- map_df(thresholds, ~ calculate_tolerance_region_proportions(processed, threshold = .x, weight = NULL))

threshold_robustness_plot_data <-
  threshold_robustness_plot_data |> 
  dplyr::mutate(
    tolerance_region = threshold * 2
  ) |> 
    pivot_longer(
    cols = c(analysis_percentage, paper_percentage),
    names_to = "type",
    values_to = "value",
    names_prefix = "_percentage"
  ) |> 
  dplyr::mutate(
    type =  stringr::str_replace(type, "_percentage", ""),
    type = dplyr::case_when(
      type == "analysis" ~ "Re-analysis effect sizes",
      type == "paper" ~ "Studies"
    )
  )
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
threshold_robustness_plot <-
  threshold_robustness_plot_data |> 
  ggplot() +
  aes(
    x = tolerance_region,
    y = value,
    linetype = type
  ) +
  geom_vline(xintercept = 0.1, color = "#440154FF", linetype = "dashed") +
  geom_line() +
  scale_x_continuous(breaks = c(0.01, seq(0.05, 0.2, by = 0.05)) * 2, expand = c(0.05, 0)) +
  scale_y_continuous(labels = scales::percent_format(scale = 1), breaks = seq(0, 100, 10), limits = c(0, 100)) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  labs(
    x = "Width of the Cohen's d tolerance region",
    y = "Percentage within the tolerance region",
    linetype = "Percentage of"
  ) +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    legend.position = c(0.8, 0.8)
  )

ggplot2::ggsave(here::here("figures/threshold_robustness_plot.jpg"), threshold_robustness_plot, height = 4, dpi = 300)

threshold_robustness_plot
```

```{r include=FALSE, message=FALSE, warning=FALSE}
weights <- seq(0.05, 0.2, by = 0.001)

threshold_weighted_robustness_plot_data <- map_df(weights, ~ calculate_tolerance_region_proportions(processed, threshold = NULL, weight = .x))

threshold_weighted_robustness_plot_data <-
  threshold_weighted_robustness_plot_data |> 
    pivot_longer(
    cols = c(analysis_percentage, paper_percentage),
    names_to = "type",
    values_to = "value",
    names_prefix = "_percentage"
  ) |> 
  dplyr::mutate(
    type =  stringr::str_replace(type, "_percentage", ""),
    type = dplyr::case_when(
      type == "analysis" ~ "Re-analysis effect sizes",
      type == "paper" ~ "Studies"
    )
  )
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
threshold_weighted_robustness_plot <-
  threshold_weighted_robustness_plot_data |> 
  ggplot() +
  aes(
    x = weight,
    y = value,
    linetype = type
  ) +
  geom_line() +
  scale_x_continuous(breaks = seq(0.05, 0.2, by = 0.05), labels = scales::percent_format(scale = 100), expand = c(0.05, 0)) +
  scale_y_continuous(labels = scales::percent_format(scale = 1), breaks = seq(0, 100, 10), limits = c(0, 100)) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  labs(
    x = "Percentage of original study's Cohen's d used\nto calculate tolerance region",
    y = "Percentage within the tolerance region",
    linetype = "Percentage of"
  ) +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    legend.position = c(0.8, 0.8)
  )

ggplot2::ggsave(here::here("figures/threshold_weighted_robustness_plot.jpg"), threshold_weighted_robustness_plot, height = 4, dpi = 300)

threshold_weighted_robustness_plot
```

##### Estimate robustness by discipline

We were interested to see whether these robustness results show a different pattern when inspecting separately by the disciplines of the studies. @fig-effect-region-discipline and @fig-effect-robustness-discipline show that for the major disciplines (\>=10 studies).

```{r include=FALSE, warning=FALSE}
processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(paper_discipline) |> 
  dplyr::arrange(n)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-discipline
#| fig-cap: "The figure shows the percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by major disciplines. The figure displays the count of re-analyses next to each discipline name."

effect_region_discipline_data <- 
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |>
  dplyr::mutate(paper_discipline = stringr::str_to_title(paper_discipline)) |> 
  # Exclude missing values
  calculate_tolerance_region(
    grouping_var = paper_discipline,
    drop_missing = TRUE
    # weight = 0.2
    )

effect_region_discipline_plot <-
  plot_percentage(
    data = effect_region_discipline_data,
    grouping_var = paper_discipline,
    categorization_var = is_within_region,
    y_lab = "Percentage of re-analysis results",
    x_lab = "Disciplines",
    with_labels = TRUE,
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    axis.title.y = element_text(hjust = 1)
  )

ggplot2::ggsave(here::here("figures/effect_region_discipline_plot.jpg"), effect_region_discipline_plot, dpi = 300)

effect_region_discipline_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-robustness-discipline
#| fig-cap: "This raincloud figure shows for each major discipline the distributions of effect size estimate ranges (lowest to highest) calculated per study."

effect_robustness_discipline_data <-
  processed |> 
  dplyr::filter(paper_discipline %in% c("psychology", "economics", "political science")) |> 
  dplyr::mutate(paper_discipline = stringr::str_to_title(paper_discipline)) |> 
  calculate_estimate_range(grouping_var = paper_discipline)

effect_robustness_discipline_plot <-
  plot_rain(data = effect_robustness_discipline_data,
            grouping_var = paper_discipline,
            response_var = estimate_range,
            x_lab = "Disciplines",
            y_lab = "Effect size estimate range in Cohen's d",
            trans = "log10",
            breaks = c(0.01, 0.1, 0.5, 1, 5, 10, 30, 60)) +
  ggplot2::theme(
    axis.title = element_text(size = 9)
  )

ggplot2::ggsave(here::here("figures/effect_robustness_discipline_plot.jpg"), effect_robustness_discipline_plot, dpi = 300)

effect_robustness_discipline_plot 
```

##### Estimate robustness by study design (observational, experimental)

Here (@fig-effect-region-studytype and @fig-effect-robustness-studytype), we were interested to see whether these results show a different pattern when separating them by study type.

```{r include=FALSE}
processed |> 
  dplyr::distinct(paper_id, .keep_all = T) |> 
  dplyr::count(experimental_or_observational) |> 
  dplyr::arrange(n)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-studytype
#| fig-cap: "The figure shows the percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by study type. The figure displays the count of re-analyses next to each discipline name."

effect_region_studytype_data <- 
  processed |> 
  dplyr::mutate(experimental_or_observational = stringr::str_to_title(experimental_or_observational)) |> 
  calculate_tolerance_region(grouping_var = experimental_or_observational, drop_missing = TRUE)

effect_region_studytype_plot <-
  plot_percentage(
    data = effect_region_studytype_data,
    grouping_var = experimental_or_observational,
    categorization_var = is_within_region,
    y_lab = "Percentage of re-analysis results",
    x_lab = "Study type",
    with_labels = TRUE,
    rev_limits = FALSE
  ) +
  ggplot2::theme(
    axis.title.y = element_text(hjust = 1)
  )

ggplot2::ggsave(here::here("figures/effect_region_studytype_plot.jpg"), effect_region_studytype_plot, dpi = 300)

effect_region_studytype_plot
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-robustness-studytype
#| fig-cap: "This raincloud figure shows for each study type the distribution of effect size estimate ranges (lowest to highest) calculated per study."

effect_robustness_studytype_data <-
  processed |> 
  dplyr::mutate(experimental_or_observational = stringr::str_to_title(experimental_or_observational)) |> 
  calculate_estimate_range(grouping_var = experimental_or_observational)

effect_robustness_studytype_plot <- plot_rain(
  data = effect_robustness_studytype_data,
  grouping_var = experimental_or_observational,
  response_var = estimate_range,
  x_lab = "Study type",
  y_lab = "Effect size estimate range in Cohen's d",
  trans = "log10",
  breaks = c(0.01, 0.1, 0.5, 1, 5, 10, 30, 60)
) +
  # ggplot2::coord_flip() +
  ggplot2::theme(
    axis.title = element_text(size = 9),
    axis.text = element_text(size = 7)
  )

ggplot2::ggsave(here::here("figures/effect_robustness_studytype_plot.jpg"), effect_robustness_studytype_plot, dpi = 300)

effect_robustness_studytype_plot 
```

##### Estimate robustness by expertise

Here (@fig-effect-region-expertise), we were interested to see whether these results show a different pattern when inspecting them along the reported expertise of the co-analysts.

```{r include=FALSE}
processed |> 
  dplyr::distinct(expertise_self_rating)
```

```{r echo=FALSE, message=FALSE}
#| label: fig-effect-region-expertise
#| fig-cap: "The figure shows the percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by self-rated expertise (on a scale of 1 (Beginner) to 10 (Expert))."

effect_region_expertise_data <- 
  processed |> 
  dplyr::mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(expertise_self_rating, levels = c(
      "1\n(Beginner)",
      as.character(2:9),
      "10\n(Expert)")
    )
  ) |> 
  calculate_tolerance_region(grouping_var = expertise_self_rating, drop_missing = TRUE)

effect_region_expertise_plot <-
  plot_height(
    data = effect_region_expertise_data,
    grouping_var = expertise_self_rating,
    categorization_var = is_within_region,
    y_lab = "Number of re-analyses",
    x_lab = "Expertise rating",
    with_labels = TRUE,
    rev_limits = FALSE,
    with_sum = FALSE
  )

ggplot2::ggsave(here::here("figures/effect_region_expertise_plot.jpg"), effect_region_expertise_plot, dpi = 300,
                # width = 8, height = 6
                )

effect_region_expertise_plot
```

##### Estimate robustness by prior familiarity with the dataset

Here, we were interested to see whether these results show a different pattern when inspecting them along their prior familiarity with the dataset. @fig-effect-region-familiarity shows that for these results.

```{r echo=FALSE, message=FALSE, message=FALSE}
#| label: fig-effect-region-familiarity
#| fig-cap: "The figure shows the percentage of re-analysis results falling within or outside of the tolerance region of the original results of the studies by declared familiarity with the study."

effect_region_familiarity_data <- calculate_tolerance_region(data = processed, grouping_var = familiar_with_paper, drop_missing = TRUE)

effect_region_familiarity_plot <-
  plot_percentage(
    data = effect_region_familiarity_data,
    grouping_var = familiar_with_paper,
    categorization_var = is_within_region,
    y_lab = "Percentage of re-analysis\nresults",
    x_lab = "Familiar with the paper",
    with_labels = TRUE
  ) +
  ggplot2::theme(plot.margin = margin(
    t = 20,
    r = 10,
    b = 10,
    l = 10,
    unit = "pt"
  ))

ggplot2::ggsave(here::here("figures/effect_region_familiarity_plot.jpg"), effect_region_familiarity_plot, dpi = 300)

effect_region_familiarity_plot
```

##### Estimate robustness by the level of confidence with the suitability of the analysis

Here (Table 4), we were interested to see whether these results show a different pattern when inspecting them along their level of confidence with the suitability of the analysis.

```{r include=FALSE}
processed |> 
  dplyr::distinct(confidence_in_approach)
```

**Table 4. Estimate Robustness by the Level of Confidence with the Suitability of the Analysis**

```{r echo=FALSE, message=FALSE}
effect_region_suitability_data <- calculate_tolerance_region(data = processed, grouping_var = confidence_in_approach, drop_missing = TRUE) |> 
  dplyr::mutate(
        confidence_in_approach = dplyr::case_when(
      confidence_in_approach == 1 ~ "1\nNot confident at all",
      confidence_in_approach == 5 ~ "5\nVery confident",
      TRUE ~ as.character(confidence_in_approach)
      ),
    confidence_in_approach = as.factor(confidence_in_approach),
     is_within_region = dplyr::case_when(
       is_within_region == "Within tolerance region" ~ "Yes",
       is_within_region == "Outside of tolerance region" ~ "No",
       TRUE ~ NA_character_
     )
  )

effect_region_suitability_data |>
  dplyr::mutate(Count = paste(n, "/", N),
                percentage = paste0(round(percentage, 2), "%")) |>
  dplyr::select(
    `Confidence rating` = confidence_in_approach,
    `Is the estimate within the tolerance region?` = is_within_region,
    Count,
    Percentage = percentage
  ) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

##### Estimate robustness by the sample size

Here, we were interested to see whether these results show a different pattern when considering sample size. The following @fig-samplesize-region shows no remarkable differences between the two categories.

```{r echo=FALSE, message=FALSE}
#| label: fig-samplesize-region
#| fig-cap: "The figure shows the distribution of sample sizes separately for re-analysis effect sizes falling within or outside of the tolerance region of the original results. In this figure, we could not include those studies where the original effect sizes were missing, and cases where the re-analysis effect size or sample size were missing."

samplesize_region_data <-
  processed |>
  dplyr::select(
    paper_id,
    analyst_id,
    reanalysis_model_sample_size,
    original_cohens_d,
    reanalysis_cohens_d
  ) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    is_within_region = dplyr::case_when(
      reanalysis_cohens_d <= tolarence_region_lower |
        reanalysis_cohens_d >= tolarence_region_upper ~ "No",
      reanalysis_cohens_d >= tolarence_region_lower |
        reanalysis_cohens_d <= tolarence_region_upper ~ "Yes",
      is.na(reanalysis_cohens_d) ~ "Missing"
    ),
    is_within_region = factor(is_within_region, levels = c("Yes", "No"))
  ) |> 
  dplyr::filter(
    !is.na(original_cohens_d),
    !is.na(reanalysis_model_sample_size),
    is_within_region != "Missing")


samplesize_region_plot <-
  plot_rain(
    data = samplesize_region_data,
    grouping_var = is_within_region,
    response_var = reanalysis_model_sample_size,
    y_lab = "Sample size",
    x_lab = "Is the re-analysis effect size\nwithin the tolerance region?",
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000)
  ) +
  ggplot2::coord_flip() +
  ggplot2::theme(
    axis.title.x = element_text(size = 9)
  )

ggplot2::ggsave(here::here("figures/effect_region_samplesize_plot.jpg"), samplesize_region_plot, dpi = 300)

samplesize_region_plot
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
df_region_data <-
  processed |>
  dplyr::select(
    paper_id,
    analyst_id,
    reanalysis_degrees_of_freedom_1,
    reanalysis_degrees_of_freedom_2,
    original_cohens_d,
    reanalysis_cohens_d
  ) |>
  dplyr::mutate(
    reanalysis_degrees_of_freedom_1 = dplyr::case_when(
      !is.na(reanalysis_degrees_of_freedom_2) ~ reanalysis_degrees_of_freedom_2,
      is.na(reanalysis_degrees_of_freedom_2) ~ reanalysis_degrees_of_freedom_1
    )
  ) |>
  dplyr::select(-reanalysis_degrees_of_freedom_2) |>
  dplyr::filter(
    !is.na(original_cohens_d),!is.na(reanalysis_cohens_d),!is.na(reanalysis_degrees_of_freedom_1)
  ) |>
  dplyr::mutate(
    tolarence_region_lower = original_cohens_d - 0.05,
    tolarence_region_upper = original_cohens_d + 0.05,
    is_within_region = dplyr::case_when(
      reanalysis_cohens_d <= tolarence_region_lower |
        reanalysis_cohens_d >= tolarence_region_upper ~ "No",
      reanalysis_cohens_d >= tolarence_region_lower |
        reanalysis_cohens_d <= tolarence_region_upper ~ "Yes"
    ),
    is_within_region = factor(is_within_region, levels = c("Yes", "No"))
  )


df_region_plot <-
  plot_rain(
    data = df_region_data,
    grouping_var = is_within_region,
    response_var = reanalysis_degrees_of_freedom_1,
    y_lab = "Log10 degrees of freedom",
    x_lab = "Is the re-analysis effect size\nwithin the tolerance region?",
    trans = "log10",
    breaks = c(10, 100, 1000, 10000, 100000)
  ) +
  ggplot2::coord_flip() +
  ggplot2::theme(
    axis.title = element_text(size = 9)
  )

ggplot2::ggsave(here::here("figures/effect_region_df_plot.jpg"), df_region_plot, dpi = 300)

df_region_plot
```

#### Additional analyses

While Cohen's d has the advantage of being easily interpretable and comparable across different analyses, it was designed to compare the means of two groups and its calculation relies on assumptions that can be compromised in more complex designs. Following the conduct of the present project, Kümpel & Hoffmann (2022) proposed a formal definition of generalized marginal effects (gMEs) which measure is comparable across different statistical models. When standardized, the value of gMEs is equal to the value of Cohen’s d where the latter effect size measure is strictly applicable. Since we had not originally planned to calculate standardized gMEs, we did not collect all required analysis outputs to compute them. As a result, we calculated gMEs only for a sub-sample of the 100 studies. See @fig-gme for the results of the gME calculation for our sub-sample of studies.

```{r echo=FALSE}
#| label: fig-gme
#| fig-cap: "For each original and re-analysis of papers 22, 40, 63, and 75, this figure shows a forest-density plot of non-standardized gME values as defined by Kümpel & Hoffmann (2022). Specifically, the black dots give the point estimates of the average change in target expectation attributed to the regressor of interest by each analysis, while the thicker and thinner lines visualize the 0.66 and 0.95 quantiles of the corresponding densities. Study numbers correspond to studies listed in Table S1."

knitr::include_graphics(here::here("generalized Marginal Effects/gMEs.png"))
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
# Panel figures for Nature
## Inferential robustness
inferential_panel <- (
  conclusion_main_plot +
  (
    conclusions_discipline_robustness_plot +
    conclusions_discipline_plot +
    conclusions_studytype_robustness_plot +
    conclusions_studytype_plot +
    conclusions_expertise_plot +
    peer_eval_subset_task1_plot +
    conclusions_familiarity_plot +
    conclusions_samplesize_plot
  )
) +
  plot_layout(widths = c(1, 3)) + 
  plot_annotation(tag_levels = 'a')


inferential_panel

ggsave(here::here("figures/inferential_panel.png"), inferential_panel, width = 22, height = 14, dpi = 300)

# Estimate robustness
# effec_size_corr_plot_gg <- ggplotify::as.ggplot(effec_size_corr_plot) +
# ggplot2::theme(plot.margin = margin(t = 0, r = 0, b = -2, l = 0, "cm"))
# effec_size_corr_plot_gg
main_section <- 
  effect_main_plot +
  effect_region_all_plot +
  plot_layout(widths = c(2, 1))

# Define the remaining plots section
small_plot_section <-
  (
    effect_region_discipline_plot |
      effect_robustness_discipline_plot 
  ) /
  ( effect_region_studytype_plot | effect_robustness_studytype_plot) /
  (effect_region_expertise_plot |
     effect_region_familiarity_plot | samplesize_region_plot) +
  plot_layout(ncol = 1, nrow = 3)

small_plot_section
# Combine the main section and the remaining plots section
estimate_panel <-
  (main_section | small_plot_section) + plot_annotation(tag_levels = 'a')

estimate_panel

ggsave(here::here("figures/estimate_panel.png"), estimate_panel, width = 30, height = 14, dpi = 300)
```
