---
title: "Supplementary information: Investigating the analytical robustness of the social and behavioural sciences"
format: docx
editor: source
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
# Load packages
library(tidyverse)
library(ggrain)
library(maps)
library(mapdata)
library(countrycode)
library(raster)
library(gt)
library(grid)
library(patchwork)
library(purrr)
library(ggdist)

# Load custom functions
script_files <- list.files(here::here("R"), pattern = "\\.R$", full.names = TRUE)
walk(script_files, source)

# Read processed data files
processed <- readr::read_csv(here::here("data/processed/multi100_processed_data.csv"))

demographic_information <- readRDS(here::here("data/processed/multi100_demographic-information_data.rds"))

peer_eval <- readr::read_csv(here::here("data/processed/multi100_peer-eval_processed_data.csv"))

peer_eval_not_reviewed <- readr::read_csv(here::here("data/processed/multi100_peer-eval-not-reviewed_processed_data.csv"))

peer_eval_review <- readr::read_csv(here::here("data/processed/multi100_peer-eval-review_processed_data.csv"))

## Transform datafiles for analysis 
# Add number of evaluations per analysis
peer_eval <-
  peer_eval |>
  dplyr::group_by(paper_id, analyst_id) |>
  dplyr::mutate(n_peer_evals = dplyr::n()) |>
  dplyr::ungroup()

all_people <- readr::read_csv(here::here("data/processed/multi100_all-people_processed_data.csv"))

# Transforming the timestamp to date type from character
processed <-
  processed |>
  dplyr::mutate(
    task1_timestamp = lubridate::ymd_hms(task1_timestamp))

# Table of long paper ids and simplified paper ids
processed |> 
  dplyr::distinct(simplified_paper_id, paper_id) |> 
  dplyr::arrange(simplified_paper_id)
```

# Additional Results
## General descriptives

```{r include=FALSE}
analyst_signed_up <-
  all_people |>
  dplyr::mutate(first_name = tolower(first_name),
                last_name = tolower(last_name)) |>
  dplyr::distinct(first_name, last_name, .keep_all = T) |>
  dplyr::filter(disclosure_agreement == "I agree")

analyst_submitted <-
  processed |>
  dplyr::distinct(analyst_id) |>
  nrow()
```

Responding to our recruitment call, `r nrow(analyst_signed_up)` researchers signed up to participate in our study. Of these volunteers, `r analyst_submitted` signed up to analyse at least one dataset and submitted their work by the deadline or an extended deadline.

```{r include=FALSE}
n_analysis <-
  processed |> 
  nrow()
```

Throughout the project, `r n_analysis` re-analyses have been submitted. This number is higher than the number of re-analysts as some re-analysts volunteered to analyse more than one dataset.

```{r include=FALSE}
n_failed_peer <-
  processed |>
  dplyr::filter(!peer_eval_pass | !incomplete_response_pass) |>
  nrow()
```

Out of the submitted analyses, `r nrow(dplyr::filter(processed, !peer_eval_pass))` was omitted from the summary analysis as its analysis failed the peer evaluation, and an additional `r nrow(dplyr::filter(processed, !incomplete_response_pass))` analyses were excluded due to incomplete responses.

```{r include=FALSE}
# Excluding analyst who failed the peer evaluation from the rest of the analysis
processed <-
  processed |> 
  dplyr::filter(peer_eval_pass & incomplete_response_pass)
```

```{r include=FALSE}
final_n_analyst <-
  processed |> 
  dplyr::distinct(analyst_id) |> 
  nrow()
```

As a result, we ended up with `r nrow(processed)` re-analyses submitted by `r final_n_analyst` re-analysts.

Although we invited more than 5 re-analysts to work on each of the 100 studies, due to drop-outs and peer evaluation exclusions, the final number of completed analyses ranged between `r min(dplyr::count(processed, paper_id)$n)` and `r max(dplyr::count(processed, paper_id)$n)` Table S1 shows the distribution of the number of analyses for individual studies.

__Table S1. \| The Distribution of the Number of Analyses for Studies__

```{r echo=FALSE, message=FALSE}
processed |>
  dplyr::count(paper_id) |> 
  dplyr::count(n, name = "N") |>
  dplyr::rename(`Number of Completed Analyses` = n,
                `Number of Studies` = N) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

## Basic demographics of the re-analysts

```{r include=FALSE}
# Checking if analysts consistently reported their current position
check_diff_response(processed, analyst_id, current_position_grouped)

position <-
  processed |>
  dplyr::select(analyst_id,
                paper_id,
                current_position_grouped,
                task1_timestamp) |>
  # Keeping only the first response per analyst
  keep_first_response(analyst_id, task1_timestamp) |>
  count(current_position_grouped) |>
  rename(position = current_position_grouped) |>
  mutate(position = factor(
    position,
    levels = c(
      "Professor",
      "Associate Professor",
      "Assistant Professor",
      "Post-Doc Researcher",
      "Doctoral Student",
      "Other academic/research position"
    )
  ))
```

Out of all the re-analysts who submitted their work by the deadline, there were `r dplyr::filter(position, position == "Professor") |> dplyr::pull(n)` professors, `r dplyr::filter(position, position == "Associate Professor") |> dplyr::pull(n)` associate professors, `r filter(position, position == "Assistant Professor") |> dplyr::pull(n)` assistant professors, `r dplyr::filter(position, position == "Post-Doc Researcher") |> dplyr::pull(n)` post-doctoral researchers, `r dplyr::filter(position, position == "Doctoral Student") |> dplyr::pull(n)` doctoral students, `r dplyr::filter(position, position == "Other academic/research position") |> dplyr::pull(n)` from other academic/research positions. 

```{r include=FALSE}
gender <-
  demographic_information |>
  mutate(gender = purrr::map(gender, ~ .x[1]),
         unlist(gender)) |>
  dplyr::select(anon_id, gender) |> 
  dplyr::count(gender)
```

The gender distribution of the re-analysts is as follows: `r dplyr::filter(gender, gender == "Female") |> dplyr::pull(n)` female, `r dplyr::filter(gender, gender == "Male") |> dplyr::pull(n)` male, `r dplyr::filter(gender, gender == "Non-binary") |> dplyr::pull(n)` other, and `r dplyr::filter(gender, gender == "Prefer not to say") |> dplyr::pull(n)` didn't want to respond to this question.

```{r include=FALSE}
age <-
  demographic_information |>
  mutate(age = purrr::map(age, ~ .x[1]),
         unlist(age)) |> 
  dplyr::select(anon_id, age) |> 
  # Filter erroneous data
  dplyr::filter(age != "00") |> 
  dplyr::mutate(age = as.numeric(age))
  
age_group <-
  age |> 
  dplyr::mutate(
    age_group = dplyr::case_when(
      age <= 39 ~ "young",
      age >= 40 | age <= 59 ~ "middle",
      age >= 60 ~ "old",
      TRUE ~ NA_character_
    ),
    age_group = factor(age_group, levels = c("young", "middle", "old"))
    ) |> 
  dplyr::count(age_group) |> 
  tidyr::complete(age_group, fill = list(n = 0))
```

The age distribution of the re-analysts is depicted in Fig. S1: `r dplyr::filter(age_group, age_group == "young") |> dplyr::pull(n)` young adults (-39 years); `r dplyr::filter(age_group, age_group == "middle") |> dplyr::pull(n)` middle-aged adults (40-59 years); and no older adults (60- years).

```{r, echo=FALSE, warning=FALSE, message=FALSE}
age_plot <- 
  age |>
  ggplot2::ggplot() +
  ggplot2::aes(x = age) +
  ggplot2::geom_histogram(binwidth = 1) +
  ggplot2::scale_y_continuous(expand = c(0, 0), limits = c(0, 40)) +
  ggplot2::scale_x_continuous(
    # limits = c(20, 55),
    # breaks = c(20, 30, 40, 50, 60),
    # labels = c("20", "30", "40", "50", "60")
  ) +
  ggplot2::labs(x = "Age (years)",
       y = "Number of co-analysts") +
  ggplot2::theme(
    panel.grid = ggplot2::element_blank(),
    panel.background = ggplot2::element_blank(),
    axis.line = ggplot2::element_line()
  )

ggplot2::ggsave(here::here("figures/supplement/demographic_age_plot.jpg"), age_plot, dpi = 300)

age_plot
```

__Fig. S1 \| The distribution of the analysts’ age.__
When an analyst submitted additional re-analyses with a higher reported age, we kept only their age at the time of their first submission. Moreover, one analyst was excluded because they did not disclose their age.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = education_level)

education <-
  processed |>
  dplyr::select(analyst_id, paper_id, education_level, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(education_level) |> 
  dplyr::rename(education = education_level)
```

Regarding the highest level of education, `r dplyr::filter(education, education == "High-school diploma or equivalent") |> dplyr::pull(n)` re-analyst reported a high school diploma or equivalent, `r dplyr::filter(education, education == "Bachelor's degree or equivalent") |> dplyr::pull(n)` re-analysts had a Bachelor’s degree or equivalent, `r dplyr::filter(education, education == "Master's degree or equivalent") |> dplyr::pull(n)` had a Master’s degree or equivalent, and `r dplyr::filter(education, education == "Doctoral degree or equivalent") |> dplyr::pull(n)` had a Doctoral degree or equivalent. In case the analysts completed more than one re-analysis and advanced in their studies by the time of their second analysis, we kept only their first response for this comparison.

```{r include=FALSE}
country_data <- raster::ccodes()

country <- 
  demographic_information |>
  mutate(country_of_residence = as.character(purrr::map(country_of_residence, ~ .x[1])),
         unlist(country_of_residence)) |> 
  dplyr::select(anon_id, country_of_residence) |> 
  dplyr::count(country_of_residence) |> 
  dplyr::rename(region = country_of_residence) |> 
  # Modify country names to fit the worldmap data
  dplyr::mutate(
    subregion = dplyr::case_when(
      region == "Hong Kong (China)" ~ "Hong Kong",
      TRUE ~ NA_character_
    ),
    region = dplyr::case_when(
      region == "Hong Kong (China)" ~ "China",
      region == "United States" ~ "USA",
      region == "United Kingdom" ~ "UK",
      TRUE ~ region
    ),
    continent = countrycode::countrycode(region, "country.name", "continent"),
    iso3_code = countrycode::countrycode(region, "country.name", "iso3c")
  ) |> 
  dplyr::left_join(dplyr::select(country_data, ISO3, UNREGION1), by = c("iso3_code" = "ISO3"))

continent <-
  country |> 
  dplyr::group_by(continent) |> 
  dplyr::summarise(N = sum(n))

region <-
  country |> 
  dplyr::group_by(UNREGION1) |> 
  dplyr::summarise(N = sum(n))
```

The country of residence of the re-analysts is shown on the map in Fig. S2. Regarding the continents,  `r dplyr::filter(continent, continent == "Africa") |> dplyr::pull(N)` re-analyst was from Africa, `r dplyr::filter(continent, continent == "Asia") |> dplyr::pull(N)` were from Asia, `r dplyr::filter(continent, continent == "Oceania") |> dplyr::pull(N)` from Oceania, `r dplyr::filter(continent, continent == "Europe") |> dplyr::pull(N)` from Europe, `r dplyr::filter(region, UNREGION1 == "Northern America") |> dplyr::pull(N)` from North America, and `r dplyr::filter(region, UNREGION1 %in% c("Central America", "South America")) |> dplyr::summarise(sum(N)) |> dplyr::pull("sum(N)")` were from South America.

```{r echo=FALSE, warning=FALSE, message=FALSE}
world_map <- 
  ggplot2::map_data("world") |> 
  dplyr::mutate(
    subregion = dplyr::case_when(
      subregion == "Hong Kong" ~ subregion,
      TRUE ~ NA_character_
    )
  )

country_map <- dplyr::left_join(world_map, country, by = c("region", "subregion"))

country_map_plot <- 
  country_map |> 
  ggplot2::ggplot() +
  ggplot2::aes(x = long, y = lat, group = group, fill = n) +
  ggplot2::geom_polygon(color = "white", linewidth = 0.2) +
  ggplot2::scale_fill_gradient(low = "lightblue", high = "darkblue", name = "Number of\nanalyst") +
  ggplot2::theme_void() +
  ggplot2::theme(
    legend.title = element_text(size = 6),
    plot.margin = margin(t = 10, r = 20, b = 10, l = 10, unit = "pt")
  )

ggplot2::ggsave(here::here("figures/supplement/demographic_country_plot.jpg"), country_map_plot, dpi = 300)

country_map_plot
```

__Fig. S2 \| The analysts’ country of residence.__ 
When an analyst submitted more than one re-analysis, and they moved between the submissions, we only kept their first response.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = primary_discipline)

analyst_discipline <- 
  processed |>
  dplyr::select(analyst_id, paper_id, primary_discipline, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  calculate_percentage(response_var = primary_discipline) |>
  dplyr::rename(discipline = primary_discipline) |> 
  dplyr::arrange(dplyr::desc(dplyr::if_else(discipline == "Other", -Inf, percentage)))
```

We asked the re-analysts which discipline is the closest to their research area. The following Table S2 summarises the distribution of their disciplinary orientation. Re-analysts from `r dplyr::slice(analyst_discipline, 1) |> dplyr::pull(discipline)` and `r dplyr::slice(analyst_discipline, 2) |> dplyr::pull(discipline)` disciplines participated in the highest ratio in this study.

__Table 2. \| The Distribution of Co-analysts’ Disciplinary Orientation__

```{r, echo=FALSE}
analyst_discipline |>
  dplyr::select(discipline, n, percentage) |>
  dplyr::rename(Discipline = discipline,
                Count = n,
                Percentage = percentage) |>
  gt::gt() |>
  tab_style(style = gt::cell_text(weight = "bold"),
            locations = gt::cells_column_labels()) |>
  gt::tab_footnote(
    "Note: Whenever the respondents provided more than one field we only kept their first responses."
  )
```

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = years_of_experience)

analyst_experience_years_data <-
  processed |>
  dplyr::select(analyst_id, paper_id, years_of_experience, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  # Dropped because of faulty response
  dplyr::filter(analyst_id != "RTX71")
```

The distribution of the years of experience with data analysis is depicted in Fig. S3. The median duration of experience with data analysis was `r median(analyst_experience_years_data$years_of_experience)` years among our re-analysts.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = years_of_experience)

analyst_experience_years_data <-
  processed |>
  dplyr::select(analyst_id, paper_id, years_of_experience, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  # Dropped because of faulty response
  dplyr::filter(analyst_id != "RTX71")

analyst_experience_years_plot <-
  analyst_experience_years_data |> 
  ggplot2::ggplot() +
  ggplot2::aes(x = years_of_experience) +
  ggplot2::geom_histogram() +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(x = "Years of experience with data analysis",
                y = "Number of co-analysts") +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggplot2::ggsave(here::here("figures/supplement/demographic_experience_years_plot.jpg"), analyst_experience_years_plot, dpi = 300)

analyst_experience_years_plot
```

__Fig. S3 \| The analysts’ years of experience with data analysis.__
We only kept their first response when an analyst submitted additional re-analyses with a higher reported age.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = analysis_frequency)

analysis_frequency_count <-
  processed |> 
  dplyr::select(analyst_id, paper_id, analysis_frequency, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(analysis_frequency)
```

We asked our re-analysts how regularly they perform data analysis. Fig. S4 shows that the most frequent category was `r dplyr::filter(analysis_frequency_count, n == max(n)) |> dplyr::pull(analysis_frequency)`.

```{r include=FALSE}
check_diff_response(data = processed, id_var = analyst_id, response_var = analysis_frequency)
```

```{r echo=FALSE, message=FALSE}
analysis_frequency_count <-
  processed |> 
  dplyr::select(analyst_id, paper_id, analysis_frequency, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |> 
  dplyr::count(analysis_frequency)

# For this question we report the responses by analysis and not analyst 
analysis_frequency_plot <-
  analysis_frequency_count |>
  dplyr::mutate(
    analysis_frequency = dplyr::case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = forcats::fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) |>
  ggplot2::ggplot() +
  ggplot2::aes(x = analysis_frequency, y = n) +
  ggplot2::geom_bar(stat = "identity") +
  ggplot2::scale_y_continuous(expand = c(0, 0)) +
  ggplot2::labs(x = "Frequency of doing data analysis",
                y = "Number of co-analysts") +
  ggplot2::theme(
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(color = "black"),
    axis.text.x = ggplot2::element_text(size = 7)
  )

ggplot2::ggsave(here::here("figures/supplement/demographic_analysis_frequency_plot.jpg"), analysis_frequency_plot, dpi = 300)

analysis_frequency_plot
```

```{r include=FALSE}
expertise_self_rating_data <-
  processed |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = analyst_id, time_var = task1_timestamp) |>
  dplyr::count(expertise_self_rating)
```

We also asked them how they rated their level of expertise in data analysis between Beginner (1) and Expert (10). The distribution in Fig. S5 shows that the most prevalent answer was `r dplyr::filter(expertise_self_rating_data, n == max(n)) |> dplyr::pull(expertise_self_rating)`.

```{r echo=FALSE, message=FALSE}
expertise_self_rating_plot <-
  expertise_self_rating_data |>
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = as.factor(expertise_self_rating),
    expertise_self_rating = fct_relevel(
      expertise_self_rating,
      c("1\n(Beginner)",
        as.character(2:9),
        "10\n(Expert)")
    )
  ) |>
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-rated expertise of data analysis",
       y = "Number of co-analysts") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/supplement/demographic_expertise_self_rating_plot.jpg"), expertise_self_rating_plot, dpi = 300)

expertise_self_rating_plot
```

__Fig. S5 \| The analysts’ self-rated level of expertise in data analysis.__
When an analyst submitted more than one re-analysis, we only kept their first response.

```{r include=FALSE}
familiar_with_paper_data <-
  calculate_percentage(processed, familiar_with_paper)
```

In `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(percentage)` % (`r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(n)` out of `r filter(familiar_with_paper_data, familiar_with_paper == "Yes") |> pull(N)`) of the cases, the re-analysts were familiar with the paper that the provided dataset belongs to before beginning their work on the project.


```{r include=FALSE}
processed |> 
  count(communication_check)
```

All re-analysts reported that they had not communicated the details of their analysis with other re-analysts working with the same dataset.

```{r include=FALSE}
software_data <-
  processed |> 
  dplyr::reframe(
    software = c(task1_software, task2_software),
    software = tolower(software),
  ) |> 
  separate_rows(software, sep = ",\\s*") |> 
  mutate(
    software = case_when(
      software == "ms excel" ~ "excel",
      software == "r markdown" ~ "rmarkdown",
      software == "process v4.0 by hayes for r" ~ "process v4.0",
      software == "jamovi 1.6.23.0" ~ "jamovi",
      software == "jamovi 2.3.9" ~ "jamovi",
      software == "text editor to look at the stata code of the original paper" ~ "text editor",
      software == "text editor to read the stata code in the replication materials" ~ "text editor",
      TRUE ~ software
    )
  ) |> 
  calculate_percentage(software) |> 
  arrange(desc(n)) |> 
  mutate(
    software = case_when(
      software %in% c("r", "stata", "spss", "jasp") ~ toupper(software),
      TRUE ~ stringr::str_to_title(software)
    )
  )
```

We asked the re-analysts what programming language/software/tool they used in their data analysis during Task 1 and Task 2. The following figure indicates that `r slice(software_data, 1) |> pull(software)` (`r slice(software_data, 1) |> pull(percentage)`%), `r slice(software_data, 2) |> pull(software)` (`r slice(software_data, 2) |> pull(percentage)`%), and `r slice(software_data, 3) |> pull(software)` (`r slice(software_data, 3) |> pull(percentage)`%) were the most popular responses. Fig. S6 shows the distribution of these responses.

```{r include=FALSE}
software_data <-
  processed |> 
  dplyr::reframe(
    software = c(task1_software, task2_software),
    software = tolower(software),
  ) |> 
  separate_rows(software, sep = ",\\s*") |> 
  mutate(
    software = case_when(
      software == "ms excel" ~ "excel",
      software == "r markdown" ~ "rmarkdown",
      software == "process v4.0 by hayes for r" ~ "process v4.0",
      software == "jamovi 1.6.23.0" ~ "jamovi",
      software == "jamovi 2.3.9" ~ "jamovi",
      software == "text editor to look at the stata code of the original paper" ~ "text editor",
      software == "text editor to read the stata code in the replication materials" ~ "text editor",
      TRUE ~ software
    )
  ) |> 
  calculate_percentage(software) |> 
  arrange(desc(n)) |> 
  mutate(
    software = case_when(
      software %in% c("r", "stata", "spss", "jasp") ~ toupper(software),
      TRUE ~ stringr::str_to_title(software)
    )
  )
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
#| label: fig-software
#| fig-cap: "The figure shows which software the analysts used for their re-analysis tasks. In case an analyst completed multiple re-analyses or reported the use of multiple software we kept all their responses for this figure. The figure shows only software that was used by more than 1% of the analysts."

software_plot <-
  software_data |>
  dplyr::filter(percentage > 1) |>
  ggplot() +
  aes(x = reorder(software, -percentage),
      y = percentage) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0),
                     labels = scales::percent_format(scale = 1)) +
  labs(y = "Percentage of co-analysts",
       x = "Software") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/supplement/demographic_software_plot.jpg"), software_plot, dpi = 300)

software_plot
```

__Fig. S6 \| The software the analysts used for their re-analysis tasks.__ 
In case an analyst completed multiple re-analyses or reported using multiple software applications, we kept all their responses for this figure. The figure shows only software applications that were used by more than 1% of the analysts.

## Descriptives of the statistical analyses

A difference in Task 2 compared to Task 1 was that the re-analysts received some constraints for their analysis to focus on a single result in the original study (see Methods for more details).

```{r include=FALSE}
p_value_or_bayes_data <-
  calculate_percentage(processed, p_value_or_bayes)
```

In Task 2, when we asked the re-analysts to present one main statistical result, in `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(percentage)`% of the analyses (`r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(n)` out of `r filter(p_value_or_bayes_data, p_value_or_bayes == "p-value") |> pull(N)`), the conclusion was based on a p-value. A Bayes Factor was chosen in `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(percentage)`% of the cases (`r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(n)` out of `r filter(p_value_or_bayes_data, p_value_or_bayes == "Bayes factor") |> pull(N)`).

```{r include=FALSE}
additional_calculations_data <-
  calculate_percentage(processed, additional_calculations)
```

For `r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(percentage)`% (`r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "Yes") |> pull(N)`) of the analyses, the re-analysts reported having to make additional calculations in Task 2 compared to Task 1. In the remaining `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(percentage)`% (`r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(n)` out of `r filter(additional_calculations_data, additional_calculations == "No, I already had the neccessary calculations in Task 1") |> pull(N)`) of the cases, the re-analysts indicated that despite the requirements of the instructions, they could conduct the same analyses as in Task 1.

```{r include=FALSE}
direction_of_result_data <- 
  calculate_percentage(processed, direction_of_result)
```

In Task 2, `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(percentage)`% of the results (`r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(n)` out of `r filter(direction_of_result_data, direction_of_result == "Opposite as claimed by the original study") |> pull(N)`) were in the opposite direction as claimed by the original study, disregarding whether the effect was conclusive/significant.

```{r include=FALSE}
total_hours_data <-
  processed |> 
  filter(total_hours != 999)
```

The re-analysts were asked to estimate their time spent performing Task 1 and Task 2 together. The median value of their response is `r median(total_hours_data$total_hours)` hours (Fig. S7).

```{r echo=FALSE, warning=FALSE, message=FALSE}
total_hours_plot <-
  total_hours_data |>
  ggplot() +
  aes(x = total_hours) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Total hours spent on the analysis",
       y = "Number of co-analysts") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/supplement/demographic_total_hours_plot.jpg"), total_hours_plot, dpi = 300)

total_hours_plot
```

__Fig. S7 \| The reported total hours the analyst spent on Task 1 and Task 2 together.__
In case an analyst completed multiple re-analyses, we kept all their responses for this figure. One response was excluded due to being an outlier (999 hours).

## Peer evaluation
### Peer evaluators
#### Basic demographics of the peer evaluators

Fig. S8 shows that most peer evaluators have many years of experience with conducting statistical analysis.

```{r include=FALSE}
# Get peer evaluator demographic info from task1 and task2 survey results
peer_evaluator_data <-
  peer_eval |> 
  distinct(evaluator_id) |> 
  inner_join(processed, by = c("evaluator_id" = "analyst_id"))

# Check if an evaluator has more than one analysis submitted
peer_evaluator_data |> 
  count(evaluator_id) |> 
  arrange(desc(n))

# Check if a peer evaluator has more than one evaluation submitted
peer_eval |>
  count(evaluator_id) |>
  arrange(desc(n))
```

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, years_of_experience)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
peer_analyst_experience_years_data <-
  peer_evaluator_data |>
  dplyr::select(evaluator_id, years_of_experience, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(evaluator_id, task1_timestamp)

peer_analyst_experience_years_plot <-
  peer_analyst_experience_years_data |>
  ggplot() +
  aes(x = years_of_experience) +
  geom_histogram() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Years of experience",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/supplement/demographic_evaluators_experience_years_plot.jpg"), peer_analyst_experience_years_plot, dpi = 300)

peer_analyst_experience_years_plot
```

__Fig. S8 \| The peer evaluators’ years of experience with data analysis.__
When a peer evaluator submitted more than one evaluation and a year passed between the responses, we kept only their first response.

Fig. S9 indicates that peer evaluators regularly perform data analysis.

__Fig. S9 \| The regularity that the peer evaluators perform data analysis.__

```{r include=FALSE}
check_diff_response(peer_evaluator_data, evaluator_id, analysis_frequency)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
peer_analysis_frequency_count <-
  peer_evaluator_data |> 
  dplyr::select(evaluator_id, paper_id, analysis_frequency, task1_timestamp) |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = evaluator_id, time_var = task1_timestamp) |> 
  count(analysis_frequency)

# For this question we report the responses by analysis and not analyst 
peer_analysis_frequency_plot <-
  peer_analysis_frequency_count |>
  mutate(
    analysis_frequency = case_when(
      analysis_frequency == "2-3 times a week" ~ "2-3 times\na week",
      analysis_frequency == "Once every two weeks" ~ "Once every\ntwo weeks",
      analysis_frequency == "Less than once a month" ~ "Less than\nonce a month",
      TRUE ~ analysis_frequency
    ),
    analysis_frequency = as.factor(analysis_frequency),
    analysis_frequency = fct_relevel(
      analysis_frequency,
      c(
        "Daily",
        "2-3 times\na week",
        "Once a week",
        "Once every\ntwo weeks",
        "Once a month",
        "Less than\nonce a month"
      )
    )
  ) |>
  ggplot() +
  aes(x = analysis_frequency, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Frequency of doing data analysis",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    axis.text.x = element_text(size = 6)
  )

ggsave(here::here("figures/supplement/demographic_evaluators_analysis_frequency_plot.jpg"), peer_analysis_frequency_plot, dpi = 300)

peer_analysis_frequency_plot
```

Fig. S10 indicates that most peer evaluators rate themselves close to expert level in data analysis.

```{r echo=FALSE, warning=FALSE, message=FALSE}
peer_expertise_self_rating_data <-
  peer_evaluator_data |> 
  # Keeping only the first response per analyst
  keep_first_response(id_var = evaluator_id, time_var = task1_timestamp) |>
  count(expertise_self_rating) |> 
  complete(expertise_self_rating, fill = list(n = 0))

peer_expertise_self_rating_plot <-
  peer_expertise_self_rating_data |>
  mutate(
    expertise_self_rating = case_when(
      expertise_self_rating == 1 ~ "1\n(Beginner)",
      expertise_self_rating == 10 ~ "10\n(Expert)",
      TRUE ~ as.character(expertise_self_rating)
    ),
    expertise_self_rating = factor(
      expertise_self_rating,
      levels = c("1\n(Beginner)",
                 as.character(2:9),
                 "10\n(Expert)")
    )
  ) |>
  tidyr::complete(expertise_self_rating, fill = list(n = 0)) |> 
  ggplot() +
  aes(x = expertise_self_rating, y = n) +
  geom_bar(stat = "identity") +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "Self-reported expertise rating",
       y = "Number of peer evaluators") +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black")
  )

ggsave(here::here("figures/supplement/demographic_evaluators_expertise_self_rating_plot.jpg"), peer_expertise_self_rating_plot, dpi = 300)

peer_expertise_self_rating_plot
```

__Fig. S10 \| The peer evaluators’ self-rated level of expertise in data analysis.__
When a peer evaluator submitted more than one re-analysis, we kept only their first response.

## Peer evaluations
### Descriptives of peer evaluations

```{r include=FALSE}
# Number of peer evaluations
# One response was excluded in multi100_raw_processed because the evaluator did not provide the analyst_id
nrow(peer_eval_not_reviewed)
nrow(peer_eval)
```

In total, we received `r nrow(peer_eval_not_reviewed) + 1` peer evaluation reports. One peer evaluation was removed because the ID of the analyst was not provided, and as such, we could not verify with certainty which re-analysis was being evaluated, leaving us with a total of `r nrow(peer_eval_not_reviewed)` peer evaluation reports on `r nrow(distinct(peer_eval_not_reviewed, paper_id))` different papers. After the panel members had reviewed the peer evaluations (see ‘Peer Evaluation: Review and Decisions’ for all decisions and reasoning behind each case), the final result of the peer evaluation was as follows:

```{r include=FALSE}
dplyr::count(peer_eval, task1_pipeline_acceptable)
dplyr::count(peer_eval, task1_conclusion_follows_results)
dplyr::count(peer_eval, task2_pipeline_acceptable)
```

At the end of the peer evaluation process, one analysis was deemed to contain an unacceptable analysis pipeline. Therefore, we removed this single analysis from our results. For the remaining analyses, it was determined that all Task 1 and Task 2 analysis pipelines were acceptable. Furthermore, all remaining Task 1 conclusions were considered to follow from the results accurately, and the analysts’ self-categorization of the results was considered adequate.

```{r include=FALSE}
reproducibility_checks_data <-
  peer_eval |> 
  calculate_percentage(any_code_mismatches)

reproducibility_checks_n <-
  reproducibility_checks_data |> 
  filter(any_code_mismatches %in% c("(3) I executed it and I found no mismatches", "(4) I executed it and I found mismatches")) |> 
  summarise(n_reproducibility_checks = sum(n))
```

`r reproducibility_checks_n` analytical reproducibility checks were successfully conducted, and mismatches were identified in `r filter(reproducibility_checks_data, any_code_mismatches == "(4) I executed it and I found mismatches") |> pull(n)` analyses. In all of these cases, we verified that the mismatches did not have a meaningful impact on the reported conclusion, categorisation, or effect size.

## Robustness Analyses
### Inferential robustness by the level of confidence with the suitability of the analysis

Table S3 shows the percentage of same conclusion, no effect/inconclusive, and opposite effect of the re-analyses by the analyst’s level of confidence with the suitability of the analysis.

__Table S3. Inferential Robustness by the Level of Confidence with the Suitability of the Analysis__
```{r echo=FALSE, message=FALSE}
conclusions_suitability_data <- 
  processed |> 
  dplyr::mutate(
    confidence_in_approach = dplyr::case_when(
      confidence_in_approach == 1 ~ "1\nNot confident at all",
      confidence_in_approach == 5 ~ "5\nVery confident",
      TRUE ~ as.character(confidence_in_approach)
      ),
    confidence_in_approach = as.factor(confidence_in_approach)
    ) |> 
  calculate_conclusion(grouping_var = confidence_in_approach)

conclusions_suitability_data |>
  dplyr::mutate(Count = paste(n, "/", N),
                percentage = paste0(percentage, "%")) |>
  dplyr::select(
    `Confidence rating` = confidence_in_approach,
    `Direction of the conclusion` = categorisation,
    Count,
    Percentage = percentage
  ) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

### Estimate robustness by the level of confidence with the suitability of the analysis

Here (Table S4), we were interested to see whether these results show a different pattern when inspecting them as a function of the evaluators’ level of confidence in the suitability of the analysis.

__Table S4. Estimate Robustness by the Level of Confidence with the Suitability of the Analysis__
```{r echo=FALSE, message=FALSE}
effect_region_suitability_data <-
  summarize_tolerance_region(data = processed,
                             grouping_var = confidence_in_approach,
                             drop_missing = TRUE,
                             threshold = 0.05) |>
  dplyr::mutate(
    confidence_in_approach = dplyr::case_when(
      confidence_in_approach == 1 ~ "1\nNot confident at all",
      confidence_in_approach == 5 ~ "5\nVery confident",
      TRUE ~ as.character(confidence_in_approach)
    ),
    confidence_in_approach = as.factor(confidence_in_approach),
    is_within_region = dplyr::case_when(
      is_within_region == "Within tolerance region" ~ "Yes",
      is_within_region == "Outside of tolerance region" ~ "No",
      TRUE ~ NA_character_
    )
  )

effect_region_suitability_data |>
  dplyr::mutate(Count = paste(n, "/", N),
                percentage = paste0(round(percentage, 2), "%")) |>
  dplyr::select(
    `Confidence rating` = confidence_in_approach,
    `Is the estimate within the tolerance region?` = is_within_region,
    Count,
    Percentage = percentage
  ) |>
  gt::gt() |>
  gt::tab_style(
    style = gt::cell_text(weight = "bold"),
    locations = gt::cells_column_labels()
  )
```

### Estimate robustness with alternative tolerance regions

We learned that even with a 4 times broader tolerance region  (+/- 0.5 Cohen’s d), we would find that around 80% of the studies still show results outside of this region and around half of the individual reanalysis effect sizes are outside of this region (Fig. S11).

```{r include=FALSE, message=FALSE, warning=FALSE}
thresholds <- seq(0.01, 0.2, by = 0.001)

threshold_robustness_plot_data <- map_df(thresholds, ~ calculate_tolerance_region_proportions(processed, threshold = .x, weight = NULL))

threshold_robustness_plot_data <-
  threshold_robustness_plot_data |> 
  dplyr::mutate(
    tolerance_region = threshold * 2
  ) |> 
    pivot_longer(
    cols = c(analysis_percentage, paper_percentage),
    names_to = "type",
    values_to = "value",
    names_prefix = "_percentage"
  ) |> 
  dplyr::mutate(
    type =  stringr::str_replace(type, "_percentage", ""),
    type = dplyr::case_when(
      type == "analysis" ~ "Re-analysis effect sizes",
      type == "paper" ~ "Studies"
    )
  )
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
threshold_robustness_plot <-
  threshold_robustness_plot_data |> 
  ggplot() +
  aes(
    x = tolerance_region,
    y = value,
    linetype = type
  ) +
  geom_vline(xintercept = 0.1, color = "#440154FF", linetype = "dashed") +
  geom_line() +
  scale_x_continuous(breaks = c(0.01, seq(0.05, 0.2, by = 0.05)) * 2, expand = c(0.05, 0)) +
  scale_y_continuous(labels = scales::percent_format(scale = 1), breaks = seq(0, 100, 10), limits = c(0, 100)) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  labs(
    x = "Width of the Cohen's d tolerance region",
    y = "Percentage within the tolerance region",
    linetype = "Percentage of"
  ) +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    legend.position = c(0.8, 0.8)
  )

ggplot2::ggsave(here::here("figures/supplement/threshold_robustness_plot.jpg"), threshold_robustness_plot, height = 4, dpi = 300)

threshold_robustness_plot
```

__Fig. S 11 \| Estimate robustness with different widths (+/-[0.01-0.20] Cohen’s d) of tolerance region.__

Alternatively, we could define the tolerance region as a percentage of the given effect size. As an additional robustness test, we varied the tolerance region between +/-5% and +/-20%. Fig. S12 shows that there was barely any difference regarding the percentage of robust studies. 

```{r include=FALSE, message=FALSE, warning=FALSE}
weights <- seq(0.05, 0.2, by = 0.001)

threshold_weighted_robustness_plot_data <- map_df(weights, ~ calculate_tolerance_region_proportions(processed, threshold = NULL, weight = .x))

threshold_weighted_robustness_plot_data <-
  threshold_weighted_robustness_plot_data |> 
    pivot_longer(
    cols = c(analysis_percentage, paper_percentage),
    names_to = "type",
    values_to = "value",
    names_prefix = "_percentage"
  ) |> 
  dplyr::mutate(
    type =  stringr::str_replace(type, "_percentage", ""),
    type = dplyr::case_when(
      type == "analysis" ~ "Re-analysis effect sizes",
      type == "paper" ~ "Studies"
    )
  )
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
threshold_weighted_robustness_plot <-
  threshold_weighted_robustness_plot_data |> 
  ggplot() +
  aes(
    x = weight,
    y = value,
    linetype = type
  ) +
  geom_line() +
  scale_x_continuous(breaks = seq(0.05, 0.2, by = 0.05), labels = scales::percent_format(scale = 100), expand = c(0.05, 0)) +
  scale_y_continuous(labels = scales::percent_format(scale = 1), breaks = seq(0, 100, 10), limits = c(0, 100)) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  labs(
    x = "Percentage of original study's Cohen's d used\nto calculate tolerance region",
    y = "Percentage within the tolerance region",
    linetype = "Percentage of"
  ) +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    legend.position = c(0.8, 0.8)
  )

ggplot2::ggsave(here::here("figures/supplement/threshold_weighted_robustness_plot.jpg"), threshold_weighted_robustness_plot, height = 4, dpi = 300)

threshold_weighted_robustness_plot
```

__Fig. S 12 \| Estimate robustness with different percentages (5-20%) of Cohen’s d as tolerance region.__

## Additional analyses
While Cohen’s d has the advantage of being easy to compute and comparable across different analyses, Kümpel and Hoffmann26 recently proposed the concept of generalised marginal effects (gMEs), an effect size metric that is both formally applicable and comparable across different statistical models. When standardised, the value of gMEs is equal to the value of Cohen’s d, where the latter effect size measure is strictly applicable. Otherwise, gME-values are very intuitive to interpret and communicate, as they give the average expected change of the target variable. We had not originally planned to calculate standardised gMEs, and, accordingly, did not collect all required analysis outputs to compute them across the board. Still, we calculated gMEs for a sample of our studies (n = 4) to showcase their potential for future multi-analyst studies (Fig. S13).

```{r echo=FALSE}
knitr::include_graphics(here::here("generalized Marginal Effects/gMEs.png"))
```

__Fig. S13 \|__ 
For each original and re-analysis of papers 22, 40, 63, and 75, the figure shows a forest-density plot of non-standardized gME values, as defined by Kümpel and Hoffmann26. Specifically, the black dots give the point estimates of the average change in target expectation attributed to the regressor of interest by each analysis, whereas the thicker and thinner lines depict the 0.66 and 0.95 quantiles of the corresponding densities. Study numbers correspond to studies listed in Table S1.

# Additional Details of Methods
## Project contributors

_Lead team:_ The project was coordinated by a lead team (consisting of Balazs Aczel, Barnabas Szaszi, Harry Clelland, Livia Kosa, Zoltan Torma, Felix Holzmeister, Marton Kovacs, and Gustav Nilsonne). The lead team was responsible for the development of the research methodology, preregistration of the project, overall analysis of the results, and preparation of a manuscript for publication. Furthermore, the lead team provided the materials to re-analysts and peer evaluators; and communicated with the project management team, the expert panel, re-analysts and peer evaluators to ensure that the project proceeded as intended.
_Project management team:_ The project management team consisted of the SCORE team of the Center for Open Science and recruited research assistants. The project management team provided financial support, oversaw the legal and ethical aspects of the project, provided the infrastructure and support for data management, and supported the use of materials adopted from other SCORE projects.
_Expert panel:_ A group of experts who have previously conducted multi-analyst studies and/or are experts of relevant methodology were invited to participate in the project as members of an expert panel. The panel’s task was to oversee the research plan and remain available to comment on methodological questions throughout the project. The list of expert panel members is available here. 
_Re-analysts:_ Analysts who independently analysed the target datasets. 
_Peer evaluators:_ Peer evaluators were also re-analysts who were asked to evaluate the completed analyses.

# Materials
## Study and claim selection

__Fig. S14 \| The sampling process of scientific claims.__

After selecting 100 studies from our collection (Fig. S14), we selected one empirical claim from each (see Method). We provided the re-analysts with the claims to test on the original datasets, but we did not give them specific research questions. Instead, we used the selected claims to focus the analysts on an underlying research question. We decided to follow this approach because the original papers almost never contained a well-formulated research question, and we judged that any of our attempts to translate the extracted claims to research questions would carry the danger of influencing the analysts by our own understanding.

## Peer evaluations
### Peer evaluators

When volunteering to be a re-analyst in the project, researchers could indicate whether they would be willing to serve as peer evaluators as well. They were informed that peer evaluators could become co-authors of the resulting article and that they would be remunerated for their efforts. Peer evaluators were asked to deliver five evaluations by a predefined due date and will be paid a flat fee of $10 per evaluation as an incentive to comply with the agreement. 8 peer evaluators evaluated more than 5 ( 6-10) analyses and were paid accordingly.

We originally aimed to recruit at least two peer evaluations for each re-analysis. Therefore, our aim was to reach a total of 100 x 5 x 2 = 1,000 evaluations. In reality, this plan turned out to be overly ambitious due to the very labour-intensive coordination and assessment work. Therefore, after the completion of an initial 507 peer evaluations, we judged that our sample could provide us with a rough estimate of the potentially unacceptable analyses and since we found this value relatively low (see Results), we ceased to continue with further peer evaluations. 

### Peer Evaluation Procedure

The peer evaluation procedure described below follows our preregistered protocol. All minor deviations are listed in the ‘Deviations from Preregistration’ document.

In addition to completing a re-analysis, re-analysts who signed up to the Multi100 could opt-in to serve as a peer evaluator on the project. That is, the re-analyst who responded ‘Yes’ when asked “Are you interested in serving as an evaluator for this project?” was later approached to serve as peer evaluator. The role of a peer evaluator was to check the plausibility and legitimacy of an analysis based on a summary of the analysis submitted by the analyst. In order to successfully evaluate a given re-analysis, peer evaluators were provided with instructions and a summary of the re-analysts’ analysis (i.e., responses to their Task 1 and Task 2 post-analysis survey questions). A template of instructions is provided in the Figure below.

_Peer evaluation task template sent out to all evaluators. Square brackets indicate variable information that is specific to the re-analysis being evaluated._

## Main outputs of the peer evaluation

```{r include=FALSE}
count(peer_eval_not_reviewed, task1_pipeline_acceptable)
count(peer_eval_not_reviewed, task1_conclusion_follows_results)
count(peer_eval_not_reviewed, task1_categorisation_is_accurate)
count(peer_eval_not_reviewed, task2_pipeline_acceptable)
```

Re-analyses were evaluated on five key metrics.

First, peer evaluators judged whether the analysis pipeline of Task 1 was acceptable. That is, they judged whether it is within the variations that could be considered appropriate by the scientific community in addressing the underlying research question. Each re-analysis pipeline was rated as either (1) Unacceptable, (2) Acceptable but low quality, (3) Acceptable and medium quality, or (4) Acceptable and high quality. In cases where the analysis pipeline was deemed unacceptable, evaluators provided their reasoning via an open text box.

Second, peer evaluators judged whether the conclusion provided in Task 1 adequately followed from the results of the analysis. Each conclusion was rated as either (1) It adequately follows from the results of the analysis, or (2) It does not follow adequately from the results of the analysis. In cases where the conclusion was judged not to follow adequately from the results, evaluators provided their reasoning via an open text box.

Third, peer evaluators judged whether the analyst’s categorisation of the Task 1 result was adequate. For example, regarding an analyst who has claimed that the results of their analysis show evidence in favour of the original effect/relationship, the evaluator considered whether this judgment is adequate. Each categorisation was rated as either (1) Adequate, or (2) Inadequate. Given that the analyst’s categorisation of the results is tied to their conclusion, there was no open text box provided for inadequate ratings.

Fourth, peer evaluators judged whether the analysis pipeline of Task 2 was acceptable. That is, they judged whether it is within the variations that could be considered appropriate by the scientific community in addressing the underlying research question. Each re-analysis pipeline was rated as either (1) Unacceptable, (2) Acceptable but low quality, (3) Acceptable and medium quality, (4) Acceptable and high quality, or (5) Incomplete or missing analysis. In cases where the analysis pipeline was deemed unacceptable, evaluators provided their reasoning via an open text box.

Finally, peer evaluators could optionally complete a code reproducibility check. They were asked whether any mismatches were found between the executed code and the reported results. For each analysis, the evaluator indicated either (1) I didn’t try to execute it, (2) I tried but didn’t manage to execute it, (3) I executed it and I found no mismatches, or (4) I executed it and I found mismatches. In cases where mismatches were found, evaluators described the nature of these mismatches via an open text box.

Accordingly, the Task 1 analysis pipeline was rated as ‘Unacceptable’ in `r nrow(filter(peer_eval_not_reviewed, task1_pipeline_acceptable == "(1) Unacceptable"))` cases, the Task 1 conclusion was judged not to follow adequately from the results in `r nrow(filter(peer_eval_not_reviewed, task1_conclusion_follows_results == "(2) It does not follow adequately from the results of the analysis"))` cases, the Task 1 self-categorization of the result was rated as ‘inadequate’ in `r nrow(filter(peer_eval_not_reviewed, task1_categorisation_is_accurate == "(2) Inadequate"))` cases, the Task 2 analysis pipeline was rated as ‘unacceptable’ in `r nrow(filter(peer_eval_not_reviewed, task2_pipeline_acceptable == "(1) Unacceptable"))` cases, the Task 2 analysis pipeline was judged as ‘incomplete or missing’ in `r nrow(filter(peer_eval_not_reviewed, task2_pipeline_acceptable == "(5) Incomplete or missing analysis"))` cases, and the code reproducibility checks revealed `r filter(reproducibility_checks_data, any_code_mismatches == "(4) I executed it and I found mismatches") |> pull(n)` mismatches.

## Review of the peer evaluation reports

To identify potential errors or misunderstandings in the peer evaluations, each issue raised (above) by a peer evaluator was reviewed by a member of the expert panel who considered the information provided by the peer evaluator and, where necessary, contrasted it with the information provided by the re-analyst. For each issue, the panel member reviewed the evaluators’ initial categorisation and their reasoning. Note that our aim in the project was to explore the sensitivity of analytical results to the analytical choices of the re-analysts. Hence, during the process of peer evaluation, our goal was not to ensure that each analysis pipeline consisted of the most ideal steps from every possible perspective but to ensure that the steps of the analyses were within the variations that could be considered appropriate by the scientific community in addressing the given analytical task. For that reason, during the review of the peer evaluations, the expert panel member left the ratings of the peer evaluators ‘Unacceptable’ only if the analyst made one or more mistakes that could be objectively judged as incorrect. For all the other cases where the peer-evaluator categorised the analysis pipeline ‘Unacceptable’ based on non-objective reasoning (e.g., not adding control variables or controlling for another variable), the expert panel member adjusted the rating from ‘Unacceptable’ to ‘Acceptable but low quality’. We aimed to remove re-analyses from the dataset, which was judged as ‘Unacceptable’.

As a consequence of the full peer evaluation review, one analysis was rejected. What follows is a summary of revisions made to peer evaluator’s initial ratings as an outcome of the peer evaluation review.

```{r include=FALSE}
# peer eval data before changing the responses based on the review but after excluding the one analysis
peer_eval_after_exclusion <- 
  peer_eval_not_reviewed |>
  dplyr::filter(!(analyst_id == "NSDML" &
                  paper_id == "PALER_AmPoliSciRev_2013_Pxp7"))
# Number of task1_analysis_pipeline remaining unacceptable after the review of peer evaluations
nrow(filter(peer_eval, task1_pipeline_acceptable == "(1) Unacceptable"))
```

Following the Task 1 analysis pipeline review, ratings of ‘(1) Unacceptable’ (n = `r nrow(filter(peer_eval_after_exclusion, task1_pipeline_acceptable == "(1) Unacceptable"))`) were revised to ‘(2) Acceptable but low quality’. Following the Task 1 conclusion review, ratings of ‘(2) It does not follow adequately from the results of the analysis’ (n = `r nrow(filter(peer_eval_after_exclusion, task1_conclusion_follows_results == "(2) It does not follow adequately from the results of the analysis"))`) were revised to ‘(1) It follows adequately from the results of the analysis’.

Following the Task 1 categorization review, ratings of ‘(2) Inadequate’ (n = `r nrow(filter(peer_eval_after_exclusion, task1_categorisation_is_accurate == "(2) Inadequate"))`) were revised to ‘(1) Adequate’. In many cases, evaluators made their judgment of ‘inadequate’ on the basis of their Task 1 conclusion rating. Put simply, evaluators often considered the categorisation of results to be inadequate when they also judged that the conclusion does not follow from the results. It was often the case that verifying the legitimacy of the Task 1 conclusion also verified the legitimacy of the Task 1 categorisation. 

Following the Task 2 analysis pipeline review, all initial ratings of ‘(1) Unacceptable’ (n = `r nrow(filter(peer_eval_after_exclusion, task2_pipeline_acceptable == "(1) Unacceptable"))`) were revised to ‘(2) Acceptable but low quality’. Ratings of ‘(5) Incomplete or missing analysis’ (n = `r nrow(filter(peer_eval_after_exclusion, task2_pipeline_acceptable == "(5) Incomplete or missing analysis"))`) were also revised. Many of these ratings were made simply because the re-analysts’ Task 1 submission also satisfied the requirements of Task 2 (i.e., the paper-specific instructions given in Task 2 had already been adhered to in Task 1), and as a result, no further analysis was needed. For each case, the panel verified that the analyst had reported their test statistic appropriately in the Task 2 survey response and that their analysis files had been uploaded to the OSF as requested. 

Finally, no changes were made to initial ratings following the review of code mismatches. In the cases where evaluators reported ‘(4) I executed it and found mismatches’ (n = `r filter(reproducibility_checks_data, any_code_mismatches == "(4) I executed it and I found mismatches") |> pull(n)`), the panel verified that the mismatches did not have a meaningful impact on the re-analyst’s reported conclusion, categorisation, or effect size.
The issues raised by the peer evaluators and the decisions of the expert panel are documented in full in the ‘Peer Evaluation: Review and Decisions’ supplement. This document also contains the panel’s reasoning in each case.

### Resulting actions

Re-analyses: Rejected re-analyses were excluded from overall data analyses. Those re-analysts with no accepted analyses were not co-authors on the resulting publication unless they earned it by completing peer evaluations.

## Analysis methods
### Marginal Effect sizes

The generalised Marginal Effects (gMEs) were calculated as specified by definition 3 of Kümpel and Hoffmann (2022). Specifically, the calculation of means was based on the empirical distribution of the data each analyst used to fit a given model or compute a test statistic. The standardised gME values were obtained by dividing each gME point estimate by the standard deviation of the target variable. To facilitate this calculation, it was necessary to replicate the analysis code of each analyst and extract the data after preprocessing as well as draws from the posterior distribution or, alternatively, point estimates and variance-covariance matrices (for details see Kümpel & Hoffmann, 2022). Where applicable, precisely in a single instance, t-test analyses were redone by fitting a simple linear regression model, i.e., a linear regression with a single independent variable.

## Project timeline
The main milestones of the project were

* Start of the project Feb 10, 2021
* Recruitment of expert panel Feb 24, 2021
* Start of re-analyst recruitment Jan 21, 2022
* Start of re-analyses Nov 19, 2022
* Completion of the project Oct 22, 2024

---

## Additional analysis for revision

### Checking whether match between test statistic type between re-analysis and original studies influences the results by study type

We have to exclude re-analysis tests for which we could not convert the test statistic to Cohen's d.

```{r}
# Number of papers for which the original type of statistic is missing
processed |> 
  dplyr::select(
    simplified_paper_id,
    original_type_of_statistic,
  ) |>
  filter(is.na(original_type_of_statistic)) |> 
  distinct(simplified_paper_id) |> 
  count()

# Number of analysis for which the re-analysis type of statistic is missing
processed |> 
  dplyr::select(
    analyst_id,
    reanalysis_type_of_statistic,
  ) |>
  filter(is.na(reanalysis_type_of_statistic)) |> 
  distinct(analyst_id) |> 
  count()

same_test_family <-
  processed |>
  dplyr::select(
    simplified_paper_id,
    analyst_id,
    reanalysis_type_of_statistic,
    reanalysis_cohens_d,
    original_type_of_statistic,
    original_cohens_d,
    experimental_or_observational
  ) |>
  dplyr::filter(!is.na(reanalysis_type_of_statistic) &
                  !is.na(original_type_of_statistic)) |>
  dplyr::mutate(
    original_type_of_statistic = stringr::str_replace_all(original_type_of_statistic, "²", "2"), 
    original_type_of_statistic = tolower(original_type_of_statistic),
    reanalysis_type_of_statistic = tolower(reanalysis_type_of_statistic),
    match = dplyr::if_else(
      reanalysis_type_of_statistic == original_type_of_statistic, 
      "Match", 
      "No match"
    ))

nrow(same_test_family)

same_test_family |> 
  count(match) |> 
  add_percentage(n)

# Number and percentage of cases where the original and re-analysis test statistics match by study type
same_test_family |> 
  group_by(experimental_or_observational) |> 
  count(match) |> 
  add_percentage(n)

# Original and re-analysis test statistics match by study type and tolerance region
same_test_family |>
  calculate_tolerance_region(threshold = 0.05) |>
  count(experimental_or_observational, match, is_within_region) |>
  group_by(experimental_or_observational, match) |> 
  add_percentage(n)
```

### Results by analyst academic field and original paper field match

```{r}
# Missing paper discipline values
processed |> 
  dplyr::select(paper_discipline, primary_discipline) |> 
  dplyr::filter(is.na(paper_discipline)) |> 
  count()

# Missing analyst field values
processed |> 
  dplyr::select(paper_discipline, primary_discipline) |> 
  dplyr::filter(is.na(primary_discipline)) |> 
  count()

# Distinct paper disciplines and analyst disciplines
processed |> 
  distinct(simplified_paper_id, .keep_all = T) |> 
  count(paper_discipline)

count(processed, primary_discipline)

# We only keep cases where we can match the paper's discipline and the analysts background unambiguously
# Creating a variable that records whether the analyist's academic field matches the study discipline
academic_field_match_data <-
  processed |>
  filter(
    paper_discipline %in% c("economics", "political science", "psychology", "sociology"),
    primary_discipline %in% c("Economics", "Political Science", "Psychology", "Sociology")
  ) |> 
  dplyr::mutate(
    paper_discipline = tolower(paper_discipline),
    primary_discipline = tolower(primary_discipline)
  ) |>
  dplyr::mutate(field_match = dplyr::if_else(paper_discipline == primary_discipline,
                                       "Match",
                                       "No match"))

academic_field_match_data |> 
  nrow()
  
# Number and percentage of cases where the field of the analyst matches the field of the study
academic_field_match_data |> 
  count(field_match) |> 
  add_percentage(n)
```

```{r}
# academic_field_match_data |> 
#   dplyr::filter(field_match == "Match") |> 
#   calculate_abstract_res()
```

```{r}
# Calculating the frequency of results within tolerance region on the analysis level for matching and non-matching cases with 5% threshold
analysis_level_tolerance_region <-
  academic_field_match_data |>
  dplyr::select(paper_id,
                analyst_id,
                original_cohens_d,
                reanalysis_cohens_d,
                field_match) |>
  calculate_tolerance_region(threshold = 0.05) |>
  # Exclude missing re-analysis effect sizes
  dplyr::filter(is_within_region != "Missing") |>
  dplyr::count(field_match, is_within_region) |> 
  dplyr::group_by(field_match) |> 
  add_percentage(n)

analysis_level_tolerance_region
```

```{r}
conclusions_discipline_match_data <-
  academic_field_match_data |>
  calculate_conclusion(grouping_var = field_match)

conclusions_discipline_match_data
```

```{r}
conclusions_discipline_match_plot <-
  plot_percentage(
    data = conclusions_discipline_match_data,
    grouping_var = field_match,
    categorization_var = categorisation,
    with_labels = TRUE,
    y_lab = "Percentage of re-analyses",
    x_lab = "Disciplines",
    rev_limits = FALSE
  ) + 
  theme(
    plot.margin = margin(t = 20, r = 10, b = 10, l = 10, unit = "pt"),
    legend.text = element_text(size = 8),
    axis.title.y = element_text(hjust = 1)
    )

conclusions_discipline_match_plot
```

### Analysing the subset of original papers where the COS team did a reproducibility check

```{r}
processed |> distinct(simplified_paper_id, .keep_all = T) |> count(reproduction_result_osf)
```

```{r}
reproducibility_data <-
  processed |>
  dplyr::mutate(
    reproducibility = dplyr::case_when(
      reproduction_result_osf %in% c(
        "never attempted",
        "not evaluated -- analysis code not provided",
        "not evaluated -- code did not run",
        "not reproduced",
        "not evaluated -- dataset could not be opened",
        "not evaluated -- not all variables are publicly available",
        "not evaluated -- number of observations doesn't match paper and variables in code don't match dataset"
      ) ~ "not reproduced",
      TRUE ~ "reproduced"
    )
  )

# reproducibility_data |> 
#   dplyr::select(reproducibility, reproduction_result_osf)
```

```{r}
# Number of analysis for reproduced and not papers
reproducibility_data |> 
  dplyr::count(reproducibility)

# Number of papers that are reproduced and not reproduced
reproducibility_data |>
  dplyr::distinct(simplified_paper_id, .keep_all = T) |> 
  dplyr::count(reproducibility)
```

```{r}
# Calculate the main results from the abstract for reproduced and not reproduced papers
reproducibility_data |> 
  dplyr::filter(reproducibility == "reproduced") |> 
  calculate_abstract_res()

reproducibility_data |> 
  dplyr::filter(reproducibility == "not reproduced") |> 
  calculate_abstract_res()
```

### Main analysis for results without extreme outliers

```{r}
# processed |> 
#   dplyr::select(reanalysis_cohens_d)

# Excluding extreme outliers larger than 3 or smaller than -3 Cohen's d from the re-analysis effect sizes
outlier_data <- 
  processed |> 
  dplyr::filter(
    !is.na(reanalysis_cohens_d),
    reanalysis_cohens_d <= 3,
    reanalysis_cohens_d >= -3
  )

# Number of reanalysis remaining
nrow(outlier_data)
processed |> dplyr::filter(!is.na(reanalysis_cohens_d)) |> nrow()

# Calculate the main results from the abstract
## No conclusion result
calculate_abstract_res(outlier_data)
```

### Fig3b modifications

Fig 3b could have the re-analysis effect size dots in different form or colour when the conclusion was the same as the original.

```{r}
reanalysis_data <-
  processed |>
  dplyr::select(simplified_paper_id,
                analyst_id,
                reanalysis_cohens_d,
                original_cohens_d,
                direction_of_result) |>
  dplyr::group_by(simplified_paper_id) |>
  dplyr::rename(effect_size = reanalysis_cohens_d) |>
  dplyr::mutate(effect_size_type = paste0("re-analysis_0", row_number()),) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    simplified_paper_id = as.factor(simplified_paper_id),
    # Put missing values at the end
    original_cohens_d = dplyr::if_else(is.na(original_cohens_d), -Inf, original_cohens_d),
    simplified_paper_id = forcats::fct_reorder(.f = simplified_paper_id,
                                               .x = original_cohens_d,
                                               .fun = function(x) median(x),
                                               .na_rm = FALSE)
  )

effec_size_corr_plot_data <-
  reanalysis_data |>
  dplyr::filter(!is.na(effect_size)) |>
  dplyr::filter(effect_size <= 5 & effect_size >= -5) |>
  dplyr::filter(original_cohens_d != -Inf)

effec_size_corr_plot_data_zoomed <-
  effec_size_corr_plot_data |> 
  dplyr::filter(effect_size <= 1 & effect_size >= -1) |>
  dplyr::filter(original_cohens_d <= 1 & original_cohens_d >= -1)
```

```{r}
# Based on: https://github.com/CenterForOpenScience/rpp
colors <- c("Original" = "#440154FF",
            "Replication" = "#FDE725FF")

# Fit the linear model
lm_fit <-
  lm(effect_size ~ original_cohens_d, data = effec_size_corr_plot_data_zoomed)

# Extract slope
beta <- coef(lm_fit)[2]

# Calculate the breaks based on the supplied limits
x_breaks <- seq(from = 0, to = 1, by = 0.5)

y_breaks <- seq(from = -1, to = 1, by = 0.5)

# Create the scatter plot
scatter <-
  effec_size_corr_plot_data_zoomed |>
  ggplot(aes(
    x = original_cohens_d,
    y = effect_size,
    color = as.factor(colors)
  )) +
  geom_point(
    # TODO:  Is this the only place where I have to use direction_of_result and not task1_categorisation
    aes(fill = direction_of_result),
    shape = 21,
    color = "Grey30",
    alpha = 0.8
  ) +
  scale_fill_viridis_d(option = "D") +
  geom_smooth(
    method = "lm",
    se = FALSE,
    color = "black",
    alpha = 0.2
  ) +
  annotate(
    "text",
    x = 0.97,
    y =  0.8,
    label = bquote(italic(beta) == .(round(beta, 2))),
    size = 4,
    color = "black"
  ) +
  geom_rug(
    aes(color = "Original"),
    size = 1,
    sides = "b",
    alpha = .6,
    show.legend = FALSE
  ) +
  geom_rug(
    aes(color = "Replication"),
    size = 1,
    sides = "l",
    alpha = .6,
    show.legend = FALSE
  ) +
  scale_color_manual(values = colors) +
  geom_hline(aes(yintercept = 0), linetype = 2) +
  geom_abline(intercept = 0,
              slope = 1,
              color = "Grey60") +
  scale_x_continuous(limits = c(0, 1),
                     breaks = x_breaks) +
  scale_y_continuous(limits = c(-1, 1),
                     breaks = y_breaks) +
  labs(
    x = expression("Original effect size in Cohen's " * italic("d")),
    y = expression("Re-analysis effect size in Cohen's " * italic("d")),
    fill = "Conclusion"
  ) +
  theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.box = "horizontal",
    legend.title = element_blank(),
    axis.title = element_text(size = 22),
    axis.text = element_text(size = 20)
  )

# Add density plots on the axes
ggExtra::ggMarginal(
  scatter,
  type = "density",
  margins = "both",
  xparams = list(fill = colors["Original"], alpha = 0.5),
  yparams = list(fill = colors["Replication"], alpha = 0.5)
)
```

### Checking the main abstract results regarding the inference for  >=50% threshold

```{r}
summarize_conclusion_robustness(data = processed, threshold = 50, operator = ">")
```

### Calculating heterogeneity ratio

```{r}
processed |>
  dplyr::filter(
    !is.na(reanalysis_correlation_coef),!is.na(reanalysis_model_sample_size),!is.na(reanalysis_cohens_d)
  ) |>
  nrow()
```

```{r}
heterogeneity_data <-
  processed |>
  dplyr::select(
    simplified_paper_id,
    analyst_id,
    reanalysis_correlation_coef,
    reanalysis_model_sample_size,
    reanalysis_cohens_d,
    reanalysis_type_of_statistic,
    reanalysis_statistic_report,
    reanalysis_degrees_of_freedom_1,
    reanalysis_degrees_of_freedom_2
  ) |>
  # TODO: Might need to revise filters
  dplyr::filter(
    !is.na(reanalysis_correlation_coef),!is.na(reanalysis_model_sample_size),!is.na(reanalysis_cohens_d)
  ) |>
  heterogeneity_ratio(simplified_paper_id)

nrow(heterogeneity_data)

median(heterogeneity_data$heterogeneity_ratio)

quantile(heterogeneity_data$heterogeneity_ratio, probs = c(0.25, 0.75))

min(heterogeneity_data$heterogeneity_ratio)

max(heterogeneity_data$heterogeneity_ratio)

IQR(heterogeneity_data$heterogeneity_ratio)
```

```{r}
heterogeneity_distribution_plot <-
  heterogeneity_data |>
  ggplot2::ggplot() +
  ggplot2::aes(x = 1, y = heterogeneity_ratio) +
  ggrain::geom_rain(rain.side = 'r') +
  ggplot2::labs(y = "Heterogeneity Ratio (log10 transformed)",
                x = "") +
  ggplot2::scale_y_continuous(trans = "log10",
                              breaks = c(1, 10, 100, 1000),
                              labels = scales::comma) +
  ggplot2::theme(
    panel.background = ggplot2::element_blank(),
    panel.grid = ggplot2::element_blank(),
    axis.line = ggplot2::element_line(),
    axis.text.x = element_blank(),
    axis.ticks.x = ggplot2::element_blank(),
  )

ggplot2::ggsave(here::here("figures/supplement/heterogeneity_distribution_plot.jpg"), heterogeneity_distribution_plot, height = 4, dpi = 300)

heterogeneity_distribution_plot
```

### Open data availability

```{r}
open_data <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1GwDGeNqloliZqkaNMbgWGnzAHwWWDoXO-Egc0s9tR60/edit?usp=sharing", col_types = "c") |> 
  dplyr::mutate(available_data = as.logical(available_data))
```

```{r}
count(open_data, available_data)
```

```{r}
open_data_cleaned <- 
  open_data |> 
  dplyr::select(-original_materials) |> 
  dplyr::right_join(processed, by = "paper_id")
  # dplyr::filter(!is.na(available_data))

# Results in the abstract where the data are available
open_data_cleaned |> 
  dplyr::filter(available_data == TRUE) |> 
  calculate_abstract_res()

# Results in the abstract where the data are NOT available
open_data_cleaned |> 
  dplyr::filter(available_data == FALSE) |> 
  calculate_abstract_res()
```

### Estimate inferential robustness with alternative accordance thresholds

We learned that even with a 4 times broader tolerance region  (+/- 0.5 Cohen’s d), we would find that around 80% of the studies still show results outside of this region and around half of the individual reanalysis effect sizes are outside of this region (Fig. S11).

```{r include=FALSE, message=FALSE, warning=FALSE}
thresholds <- seq(10, 100, by = 1)

inferential_threshold_robustness_plot_data <-
  tibble(
    threshold = thresholds,
    robustness = purrr::map_dbl(
      threshold,
      ~ summarize_conclusion_robustness(
        processed,
        threshold = .x,
        operator = dplyr::if_else(.x == 100, "==", ">")
      ) |> 
        dplyr::filter(robust == "Inferentially robust") |> 
        dplyr::pull(percentage)
    )
  )
```

```{r}
inferential_threshold_robustness_plot <-
  inferential_threshold_robustness_plot_data |>
  ggplot() +
  aes(x = threshold,
      y = robustness) +
  geom_line() +
  scale_x_continuous(
    breaks = seq(10, 100, 10), 
    labels = scales::percent_format(scale = 1),
    limits = c(0, 100)) +
  scale_y_continuous(
    labels = scales::percent_format(scale = 1),
    breaks = seq(0, 100, 10),
    limits = c(0, 100)
  ) +
  labs(x = "Threshold of proportion of re-analyses arriving at the same conclusion as the original study",
       y = "Percentage of studies") +
  ggplot2::theme(
    panel.background = element_blank(),
    panel.grid = element_blank(),
    axis.line = element_line(color = "black"),
    legend.position = c(0.8, 0.8)
  )

ggplot2::ggsave(here::here("figures/supplement/inferential_threshold_robustness_plot.jpg"), inferential_threshold_robustness_plot, height = 4, dpi = 300)

inferential_threshold_robustness_plot
```

